{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface Sagemaker SDK\n",
    "\n",
    "\n",
    "## Scenarios:\n",
    "\n",
    "1. Basic data prep, training, deployment, inference\n",
    "1. Advanced training: spot, distributed\n",
    "1. Model Registry comparison between model trained with small training dataset size compared to larger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install --upgrade \"sagemaker>=2.31.0\" \"transformers==4.4.2\" \"datasets[s3]==1.5.0\"\n",
    "!conda install -c conda-forge ipywidgets -y\n",
    "\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)\n",
    "\n",
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name AmazonSageMaker-ExecutionRole-20201221T131849 to get Role path.\n",
      "Assuming role was created in SageMaker AWS console, as the name contains `AmazonSageMaker-ExecutionRole`. Defaulting to Role ARN with service-role in path. If this Role ARN is incorrect, please add IAM read permissions to your role or supply the Role Arn directly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker role arn: arn:aws:iam::061635907654:role/service-role/AmazonSageMaker-ExecutionRole-20201221T131849\n",
      "SageMaker bucket: sagemaker-us-east-1-061635907654\n",
      "SageMaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f\"SageMaker role arn: {role}\")\n",
    "print(f\"SageMaker bucket: {session.default_bucket()}\")\n",
    "print(f\"SageMaker session region: {session.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.11 s, sys: 175 ms, total: 1.29 s\n",
      "Wall time: 2.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pandas\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n"
     ]
    }
   ],
   "source": [
    "# train_dataset, test_dataset = datasets.load_dataset(\n",
    "#     \"imdb\",\n",
    "#     ignore_verifications = True,\n",
    "#     split = [\"train\", \"test\"]\n",
    "# )\n",
    "\n",
    "train_dataset, test_dataset = datasets.load_dataset(\n",
    "    \"imdb\", \n",
    "    ignore_verifications = True,\n",
    "    split = [\"train\", \"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411db716cf3a4bd18f4d3cb41931f1ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be79fa24dd64d869873a7fceb024e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[101, 1045, 1005, 2222, 2022, 14969, 1012, 104...</td>\n",
       "      <td>0</td>\n",
       "      <td>I'll be blunt. I'm not one for politically cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[101, 2023, 3185, 2003, 1037, 2200, 3532, 3535...</td>\n",
       "      <td>0</td>\n",
       "      <td>This movie is a very poor attempt to make mone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[101, 1998, 2043, 1045, 3422, 4532, 3165, 2386...</td>\n",
       "      <td>0</td>\n",
       "      <td>And when I watch Sarah Silverman, I get the sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[101, 2732, 5867, 22214, 1011, 1015, 4076, 199...</td>\n",
       "      <td>1</td>\n",
       "      <td>Stargate SG-1 follows and expands upon the Egy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[101, 2339, 2106, 2023, 3185, 8246, 11088, 102...</td>\n",
       "      <td>1</td>\n",
       "      <td>Why did this movie fail commercially? It's got...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      attention_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                           input_ids  labels  \\\n",
       "0  [101, 1045, 1005, 2222, 2022, 14969, 1012, 104...       0   \n",
       "1  [101, 2023, 3185, 2003, 1037, 2200, 3532, 3535...       0   \n",
       "2  [101, 1998, 2043, 1045, 3422, 4532, 3165, 2386...       0   \n",
       "3  [101, 2732, 5867, 22214, 1011, 1015, 4076, 199...       1   \n",
       "4  [101, 2339, 2106, 2023, 3185, 8246, 11088, 102...       1   \n",
       "\n",
       "                                                text  \n",
       "0  I'll be blunt. I'm not one for politically cor...  \n",
       "1  This movie is a very poor attempt to make mone...  \n",
       "2  And when I watch Sarah Silverman, I get the sa...  \n",
       "3  Stargate SG-1 follows and expands upon the Egy...  \n",
       "4  Why did this movie fail commercially? It's got...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize = lambda batch: tokenizer(\n",
    "    batch[\"text\"], \n",
    "    padding = \"max_length\", \n",
    "    truncation = True\n",
    ")\n",
    "# test_ds = test_dataset.shuffle().select(range(10000))\n",
    "\n",
    "train_ds = train_dataset.shuffle().map(tokenize)\n",
    "test_ds = test_dataset.shuffle().map(tokenize)\n",
    "\n",
    "try:\n",
    "    train_ds = train_ds.rename_column(\"label\", \"labels\")\n",
    "    test_ds = test_ds.rename_column(\"label\", \"labels\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "pandas.DataFrame(train_ds[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "train_ds.set_format(\"torch\", columns = columns)\n",
    "test_ds.set_format(\"torch\", columns = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data to `sagemaker_session_bucket`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "s3_prefix = \"datasets/imdb-binary-classification\"\n",
    "\n",
    "training_input_path = f\"s3://{bucket}/{s3_prefix}/train\"\n",
    "train_ds.save_to_disk(training_input_path, fs = s3)\n",
    "\n",
    "test_input_path = f\"s3://{bucket}/{s3_prefix}/test\"\n",
    "test_ds.save_to_disk(test_input_path, fs = s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "job_name = \"imdb-huggingface\"\n",
    "\n",
    "metric_definitions = [\n",
    "    {\n",
    "      \"Name\": \"loss\", \n",
    "      \"Regex\": \"'loss': ([0-9]+(.|e\\-)[0-9]+),?\"\n",
    "    },\n",
    "    {\n",
    "      \"Name\": \"learning_rate\", \n",
    "      \"Regex\": \"'learning_rate': ([0-9]+(.|e\\-)[0-9]+),?\"\n",
    "    },\n",
    "    {\n",
    "      \"Name\": \"eval_loss\", \n",
    "      \"Regex\": \"'eval_loss': ([0-9]+(.|e\\-)[0-9]+),?\"\n",
    "    },\n",
    "    {\n",
    "      \"Name\": \"eval_accuracy\", \n",
    "      \"Regex\": \"'eval_accuracy': ([0-9]+(.|e\\-)[0-9]+),?\"\n",
    "    },\n",
    "    {\n",
    "      \"Name\": \"eval_f1\", \n",
    "      \"Regex\": \"'eval_f1': ([0-9]+(.|e\\-)[0-9]+),?\"\n",
    "    },\n",
    "    {\n",
    "      \"Name\": \"eval_precision\", \n",
    "      \"Regex\": \"'eval_precision': ([0-9]+(.|e\\-)[0-9]+),?\"\n",
    "    },\n",
    "    {\n",
    "      \"Name\": \"eval_recall\", \n",
    "      \"Regex\": \"'eval_recall': ([0-9]+(.|e\\-)[0-9]+),?\"\n",
    "    },\n",
    "    {\n",
    "      \"Name\": \"eval_runtime\", \n",
    "      \"Regex\": \"'eval_runtime': ([0-9]+(.|e\\-)[0-9]+),?\"\n",
    "    },\n",
    "    {\n",
    "      \"Name\": \"eval_samples_per_second\", \n",
    "      \"Regex\": \"'eval_samples_per_second': ([0-9]+(.|e\\-)[0-9]+),?\"\n",
    "    },\n",
    "    {\n",
    "      \"Name\": \"epoch\", \n",
    "      \"Regex\": \"'epoch': ([0-9]+(.|e\\-)[0-9]+),?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "params = {\n",
    "    \"base_job_name\": job_name,\n",
    "    \"enable_sagemaker_metrics\": True,\n",
    "    \"entry_point\": \"train.py\",\n",
    "    \"instance_count\": 1,\n",
    "    \"instance_type\": \"ml.p3.16xlarge\",\n",
    "    \"py_version\": \"py36\",\n",
    "    \"pytorch_version\": \"1.6.0\",\n",
    "    \"role\": role,\n",
    "    \"source_dir\": \"./scripts\",\n",
    "    \"transformers_version\": \"4.4.2\"\n",
    "}\n",
    "\n",
    "spot_params = {\n",
    "    \"checkpoint_s3_uri\": f\"s3://{bucket}/{job_name}/checkpoints\",\n",
    "    \"use_spot_instances\": True,\n",
    "    \"max_wait\": 3600,\n",
    "    \"max_run\": 3600\n",
    "}\n",
    "\n",
    "dataparallel_params = {\n",
    "    \"instance_count\": 2,\n",
    "    \"distribution\": {\n",
    "        \"smdistributed\": {\n",
    "            \"dataparallel\": {\n",
    "                \"enabled\": True\n",
    "            }\n",
    "        },\n",
    "        \"mpi\": {\n",
    "            \"enabled\": True,\n",
    "            \"processes_per_host\" : 2\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "modelparallel_params = {\n",
    "    \"instance_count\": 2,\n",
    "    \"distribution\": {\n",
    "        \"smdistributed\": {\n",
    "            \"modelparallel\": {\n",
    "                \"enabled\": True,\n",
    "                \"parameters\": {\n",
    "                    \"microbatches\": 4,\n",
    "                    \"placement_strategy\": \"spread\",\n",
    "                    \"pipeline\": \"interleaved\",\n",
    "                    \"optimize\": \"speed\",\n",
    "                    \"partitions\": 4,\n",
    "                    \"ddp\": True\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"mpi\": {\n",
    "            \"enabled\": True,\n",
    "            \"processes_per_host\" : 2\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "hyperparams = {\n",
    "    \"epochs\": 6,\n",
    "    \"eval_batch_size\": 128,\n",
    "    \"model_name\": model_name,\n",
    "    \"train_batch_size\": 64\n",
    "}\n",
    "\n",
    "def use_standard_training():\n",
    "    return HuggingFace(\n",
    "        **params,\n",
    "        hyperparameters = hyperparams,\n",
    "        metric_definitions = metric_definitions\n",
    "    )\n",
    "\n",
    "def use_spot():\n",
    "    return HuggingFace(\n",
    "        **params,\n",
    "        **spot_params,\n",
    "        metric_definitions = metric_definitions,\n",
    "        hyperparameters = {\n",
    "            **hyperparams,\n",
    "            \"output_dir\": \"/opt/ml/checkpoints\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "def use_spot_distributed(distributed_params = dataparallel_params):\n",
    "    return HuggingFace(**{\n",
    "        **params,\n",
    "        **spot_params, \n",
    "        **distributed_params,\n",
    "        \"metric_definitions\": metric_definitions,\n",
    "        \"hyperparameters\": {\n",
    "            **hyperparams,\n",
    "            \"output_dir\": \"/opt/ml/checkpoints\"\n",
    "        }\n",
    "    })\n",
    "\n",
    "\n",
    "def use_distributed(distributed_params = dataparallel_params):\n",
    "    return HuggingFace(**{\n",
    "        **params,\n",
    "        **distributed_params,\n",
    "        \"metric_definitions\": metric_definitions\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-05 20:29:36 Starting - Starting the training job...\n",
      "2021-05-05 20:30:02 Starting - Launching requested ML instancesProfilerReport-1620246576: InProgress\n",
      "............\n",
      "2021-05-05 20:32:03 Starting - Preparing the instances for training.........\n",
      "2021-05-05 20:33:29 Downloading - Downloading input data\n",
      "2021-05-05 20:33:29 Training - Downloading the training image..................\n",
      "2021-05-05 20:36:28 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-05-05 20:36:28,589 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-05-05 20:36:28,666 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-05-05 20:36:34,906 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-05-05 20:36:35,347 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers==4.4.2 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (4.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 3)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 3)) (2021.3.17)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 3)) (3.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 3)) (0.0.43)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 3)) (0.10.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 3)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 3)) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 3)) (4.49.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 3)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 2)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 2)) (2021.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas->-r requirements.txt (line 2)) (1.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.4.2->-r requirements.txt (line 3)) (3.7.4.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.4.2->-r requirements.txt (line 3)) (3.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.4.2->-r requirements.txt (line 3)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.4.2->-r requirements.txt (line 3)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.4.2->-r requirements.txt (line 3)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.4.2->-r requirements.txt (line 3)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.4.2->-r requirements.txt (line 3)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.4.2->-r requirements.txt (line 3)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.4.2->-r requirements.txt (line 3)) (7.1.2)\u001b[0m\n",
      "\u001b[34m2021-05-05 20:36:37,901 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"train_batch_size\": 64,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"epochs\": 6,\n",
      "        \"eval_batch_size\": 128\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"imdb-huggingface-2021-05-05-20-29-36-251\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-061635907654/imdb-huggingface-2021-05-05-20-29-36-251/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":6,\"eval_batch_size\":128,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":64}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-061635907654/imdb-huggingface-2021-05-05-20-29-36-251/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":6,\"eval_batch_size\":128,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":64},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"imdb-huggingface-2021-05-05-20-29-36-251\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-061635907654/imdb-huggingface-2021-05-05-20-29-36-251/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"6\",\"--eval_batch_size\",\"128\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"64\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=6\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_BATCH_SIZE=128\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train.py --epochs 6 --eval_batch_size 128 --model_name distilbert-base-uncased --train_batch_size 64\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m2021-05-05 20:36:42,359 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m2021-05-05 20:36:42,359 - __main__ - INFO -  loaded test_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m2021-05-05 20:36:42,548 - filelock - INFO - Lock 140267964476384 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m2021-05-05 20:36:42,568 - filelock - INFO - Lock 140267964476384 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m2021-05-05 20:36:42,589 - filelock - INFO - Lock 140267964476384 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m2021-05-05 20:36:47,451 - filelock - INFO - Lock 140267964476384 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:54.902 algo-1:31 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:55.101 algo-1:31 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:55.101 algo-1:31 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:55.102 algo-1:31 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:55.207 algo-1:31 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:55.207 algo-1:31 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.059 algo-1:31 INFO hook.py:550] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.059 algo-1:31 INFO hook.py:550] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.059 algo-1:31 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.059 algo-1:31 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.060 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.060 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.060 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.060 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.060 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.060 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.060 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.060 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.060 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.060 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.060 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.060 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.060 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.060 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.061 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.061 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.061 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.061 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.061 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.061 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.061 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.061 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.061 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.061 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.061 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.061 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.061 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.061 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.062 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.062 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.062 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.062 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.062 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.062 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.062 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.062 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.062 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.062 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.062 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.062 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.062 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.062 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.063 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.063 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.063 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.063 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.063 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.063 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.063 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.063 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.063 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.063 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.063 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.063 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.063 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.063 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.064 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.064 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.064 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.064 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.064 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.064 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.064 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.064 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.064 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.064 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.064 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.064 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.064 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.064 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.065 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.065 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.065 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.065 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.065 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.065 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.065 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.065 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.065 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.065 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.065 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.065 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.065 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.065 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.065 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.066 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.066 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.066 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.066 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.066 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.066 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.066 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.066 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.066 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.066 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.066 algo-1:31 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.066 algo-1:31 INFO hook.py:550] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.066 algo-1:31 INFO hook.py:550] name:module.pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.066 algo-1:31 INFO hook.py:550] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.067 algo-1:31 INFO hook.py:550] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.067 algo-1:31 INFO hook.py:552] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.067 algo-1:31 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:36:56.070 algo-1:31 INFO hook.py:476] Hook is writing from the hook with pid: 31\n",
      "\u001b[0m\n",
      "\u001b[34mNCCL version 2.4.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:37:10.667 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:37:10.667 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:37:10.681 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:37:11.061 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:37:11.061 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:37:11.064 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:37:11.833 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:37:11.834 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:37:11.839 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:37:11.969 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:37:11.970 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:37:11.973 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:37:12.788 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:37:12.788 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:37:12.812 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:37:13.129 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:37:13.129 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:37:13.135 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:37:13.236 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:37:13.236 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:37:13.239 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:37:13.608 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:37:13.608 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:37:13.611 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:37:13.618 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:DataParallel SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'loss': 0.6961, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m{'loss': 0.6928, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m{'loss': 0.6889, 'learning_rate': 3e-06, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m{'loss': 0.6813, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m{'loss': 0.6653, 'learning_rate': 5e-06, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m{'loss': 0.6177, 'learning_rate': 6e-06, 'epoch': 0.61}\u001b[0m\n",
      "\u001b[34m{'loss': 0.5125, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.71}\u001b[0m\n",
      "\u001b[34m{'loss': 0.4056, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.82}\u001b[0m\n",
      "\u001b[34m{'loss': 0.3424, 'learning_rate': 9e-06, 'epoch': 0.92}\u001b[0m\n",
      "\u001b[34m'eval_accuracy': 0.88844\u001b[0m\n",
      "\u001b[34m'eval_f1': 0.8838835921562096\u001b[0m\n",
      "\u001b[34m'eval_precision': 0.9215209653615766\u001b[0m\n",
      "\u001b[34m'eval_recall': 0.8492\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.28888311982154846, 'eval_accuracy': 0.88844, 'eval_f1': 0.8838835921562096, 'eval_precision': 0.9215209653615766, 'eval_recall': 0.8492, 'eval_runtime': 66.1686, 'eval_samples_per_second': 377.822, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2987, 'learning_rate': 1e-05, 'epoch': 1.02}\u001b[0m\n",
      "\u001b[34m{'loss': 0.279, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.12}\u001b[0m\n",
      "\u001b[34m{'loss': 0.266, 'learning_rate': 1.2e-05, 'epoch': 1.22}\u001b[0m\n",
      "\u001b[34m{'loss': 0.267, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.33}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2573, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.43}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2556, 'learning_rate': 1.5e-05, 'epoch': 1.53}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2343, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.63}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2308, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.73}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2227, 'learning_rate': 1.8e-05, 'epoch': 1.84}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2242, 'learning_rate': 1.9e-05, 'epoch': 1.94}\u001b[0m\n",
      "\u001b[34m'eval_accuracy': 0.92184\u001b[0m\n",
      "\u001b[34m'eval_f1': 0.9225463770413825\u001b[0m\n",
      "\u001b[34m'eval_precision': 0.9142834695160277\u001b[0m\n",
      "\u001b[34m'eval_recall': 0.93096\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.20060744881629944, 'eval_accuracy': 0.92184, 'eval_f1': 0.9225463770413825, 'eval_precision': 0.9142834695160277, 'eval_recall': 0.93096, 'eval_runtime': 66.3447, 'eval_samples_per_second': 376.82, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2021, 'learning_rate': 2e-05, 'epoch': 2.04}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2059, 'learning_rate': 2.1e-05, 'epoch': 2.14}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1804, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.24}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1826, 'learning_rate': 2.3000000000000003e-05, 'epoch': 2.35}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1724, 'learning_rate': 2.4e-05, 'epoch': 2.45}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1808, 'learning_rate': 2.5e-05, 'epoch': 2.55}\u001b[0m\n",
      "\u001b[34m{'loss': 0.172, 'learning_rate': 2.6000000000000002e-05, 'epoch': 2.65}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2374, 'learning_rate': 2.7000000000000002e-05, 'epoch': 2.76}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2084, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.86}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1864, 'learning_rate': 2.9e-05, 'epoch': 2.96}\u001b[0m\n",
      "\u001b[34m'eval_accuracy': 0.92404\u001b[0m\n",
      "\u001b[34m'eval_f1': 0.9221753206835785\u001b[0m\n",
      "\u001b[34m'eval_precision': 0.9453827409461389\u001b[0m\n",
      "\u001b[34m'eval_recall': 0.90008\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2001716047525406, 'eval_accuracy': 0.92404, 'eval_f1': 0.9221753206835785, 'eval_precision': 0.9453827409461389, 'eval_recall': 0.90008, 'eval_runtime': 65.5075, 'eval_samples_per_second': 381.636, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1534, 'learning_rate': 3e-05, 'epoch': 3.06}\u001b[0m\n",
      "\u001b[34m{'loss': 0.144, 'learning_rate': 3.1e-05, 'epoch': 3.16}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1274, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.27}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1446, 'learning_rate': 3.3e-05, 'epoch': 3.37}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1407, 'learning_rate': 3.4000000000000007e-05, 'epoch': 3.47}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1285, 'learning_rate': 3.5e-05, 'epoch': 3.57}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1201, 'learning_rate': 3.6e-05, 'epoch': 3.67}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1844, 'learning_rate': 3.7e-05, 'epoch': 3.78}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1564, 'learning_rate': 3.8e-05, 'epoch': 3.88}\u001b[0m\n",
      "\u001b[34m{'loss': 0.179, 'learning_rate': 3.9000000000000006e-05, 'epoch': 3.98}\u001b[0m\n",
      "\u001b[34m'eval_accuracy': 0.9292\u001b[0m\n",
      "\u001b[34m'eval_f1': 0.9293018054002237\u001b[0m\n",
      "\u001b[34m'eval_precision': 0.9279674537332483\u001b[0m\n",
      "\u001b[34m'eval_recall': 0.93064\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.18453869223594666, 'eval_accuracy': 0.9292, 'eval_f1': 0.9293018054002237, 'eval_precision': 0.9279674537332483, 'eval_recall': 0.93064, 'eval_runtime': 66.3531, 'eval_samples_per_second': 376.772, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1057, 'learning_rate': 4e-05, 'epoch': 4.08}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0969, 'learning_rate': 4.1e-05, 'epoch': 4.18}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1092, 'learning_rate': 4.2e-05, 'epoch': 4.29}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0838, 'learning_rate': 4.3e-05, 'epoch': 4.39}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1069, 'learning_rate': 4.4000000000000006e-05, 'epoch': 4.49}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0924, 'learning_rate': 4.5e-05, 'epoch': 4.59}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1052, 'learning_rate': 4.600000000000001e-05, 'epoch': 4.69}\u001b[0m\n",
      "\u001b[34m{'loss': 0.09, 'learning_rate': 4.7e-05, 'epoch': 4.8}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1472, 'learning_rate': 4.8e-05, 'epoch': 4.9}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0903, 'learning_rate': 4.9e-05, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m'eval_accuracy': 0.90828\u001b[0m\n",
      "\u001b[34m'eval_f1': 0.9023049720932215\u001b[0m\n",
      "\u001b[34m'eval_precision': 0.9651809315468052\u001b[0m\n",
      "\u001b[34m'eval_recall': 0.84712\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2826699912548065, 'eval_accuracy': 0.90828, 'eval_f1': 0.9023049720932215, 'eval_precision': 0.9651809315468052, 'eval_recall': 0.84712, 'eval_runtime': 66.2863, 'eval_samples_per_second': 377.152, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'loss': 0.099, 'learning_rate': 5e-05, 'epoch': 5.1}\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:52:15.333 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:52:15.333 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:52:15.335 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:52:15.336 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:52:15.338 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:52:15.339 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:52:15.339 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:52:15.340 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:52:15.345 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:52:15.346 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:52:15.347 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:52:15.347 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:52:15.347 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:52:15.349 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:52:15.349 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:52:15.351 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:52:15.351 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:52:15.352 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:52:15.352 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:52:15.352 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:52:15.353 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:52:15.354 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:52:15.355 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:52:15.355 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[2021-05-05 20:52:15.359 algo-1:31 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:DataParallel SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'loss': 0.0566, 'learning_rate': 4.431818181818182e-05, 'epoch': 5.2}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0515, 'learning_rate': 3.8636363636363636e-05, 'epoch': 5.31}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0656, 'learning_rate': 3.295454545454545e-05, 'epoch': 5.41}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0617, 'learning_rate': 2.7272727272727273e-05, 'epoch': 5.51}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0553, 'learning_rate': 2.1590909090909093e-05, 'epoch': 5.61}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0545, 'learning_rate': 1.590909090909091e-05, 'epoch': 5.71}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0413, 'learning_rate': 1.0227272727272729e-05, 'epoch': 5.82}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0411, 'learning_rate': 4.5454545454545455e-06, 'epoch': 5.92}\u001b[0m\n",
      "\u001b[34m'eval_accuracy': 0.92868\u001b[0m\n",
      "\u001b[34m'eval_f1': 0.9297007451799867\u001b[0m\n",
      "\u001b[34m'eval_precision': 0.9165824457747026\u001b[0m\n",
      "\u001b[34m'eval_recall': 0.9432\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2510910630226135, 'eval_accuracy': 0.92868, 'eval_f1': 0.9297007451799867, 'eval_precision': 0.9165824457747026, 'eval_recall': 0.9432, 'eval_runtime': 65.5065, 'eval_samples_per_second': 381.642, 'epoch': 6.0}\u001b[0m\n",
      "\u001b[34m{'train_runtime': 1084.7515, 'train_samples_per_second': 0.542, 'epoch': 6.0}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "inputs = { \n",
    "    \"train\": training_input_path, \n",
    "    \"test\": test_input_path \n",
    "}\n",
    "\n",
    "estimators = [it() for it in (use_standard_training,)]\n",
    "for it in estimators:\n",
    "    it.fit(inputs, wait = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: No metrics called eval_loss found\n",
      "Warning: No metrics called eval_accuracy found\n",
      "Warning: No metrics called eval_f1 found\n",
      "Warning: No metrics called eval_precision found\n",
      "Warning: No metrics called eval_recall found\n",
      "Warning: No metrics called eval_runtime found\n",
      "Warning: No metrics called eval_samples_per_second found\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric_name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <td>0.895000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rate</th>\n",
       "      <td>2.500005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>0.689975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  value\n",
       "metric_name            \n",
       "epoch          0.895000\n",
       "learning_rate  2.500005\n",
       "loss           0.689975"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sagemaker import TrainingJobAnalytics\n",
    "\n",
    "for it in estimators:\n",
    "    df = TrainingJobAnalytics(training_job_name = it.latest_training_job.name).dataframe()\n",
    "    display(df[[\"metric_name\", \"value\"]].groupby(\"metric_name\").max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (404) when calling the HeadObject operation: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-90c011af1a13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mendpoint_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mwait\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     ) for name, model in zip(names, models)\n\u001b[0m\u001b[1;32m     43\u001b[0m ]\n",
      "\u001b[0;32m<ipython-input-11-90c011af1a13>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mendpoint_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mwait\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     ) for name, model in zip(names, models)\n\u001b[0m\u001b[1;32m     43\u001b[0m ]\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, **kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"-\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompiled_model_suffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_sagemaker_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccelerator_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m         production_variant = sagemaker.production_variant(\n\u001b[1;32m    765\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_instance_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccelerator_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccelerator_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36m_create_sagemaker_model\u001b[0;34m(self, instance_type, accelerator_type, tags)\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0;34m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlatest\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mreference\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mservices\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;31m#SageMaker.Client.add_tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0mcontainer_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_container_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccelerator_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccelerator_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_base_name_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontainer_def\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/pytorch/model.py\u001b[0m in \u001b[0;36mprepare_container_def\u001b[0;34m(self, instance_type, accelerator_type)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mdeploy_key_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_code_key_prefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeploy_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_upload_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeploy_key_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_mms_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0mdeploy_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mdeploy_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_framework_env_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36m_upload_code\u001b[0;34m(self, key_prefix, repack)\u001b[0m\n\u001b[1;32m   1136\u001b[0m                 \u001b[0mrepacked_model_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepacked_model_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m                 \u001b[0msagemaker_session\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m                 \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_kms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1139\u001b[0m             )\n\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/utils.py\u001b[0m in \u001b[0;36mrepack_model\u001b[0;34m(inference_script, source_directory, dependencies, model_uri, repacked_model_uri, sagemaker_session, kms_key)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_tmpdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         _create_or_update_code_dir(\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/utils.py\u001b[0m in \u001b[0;36m_extract_model\u001b[0;34m(model_uri, sagemaker_session, tmp)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3://\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0mlocal_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tar_file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mdownload_file_from_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msagemaker_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0mlocal_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file://\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/utils.py\u001b[0m in \u001b[0;36mdownload_file_from_url\u001b[0;34m(url, dst, sagemaker_session)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m     \u001b[0mdownload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msagemaker_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/utils.py\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(bucket_name, path, target, sagemaker_session)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0ms3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboto_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregion_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboto_region_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0mbucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m     \u001b[0mbucket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/boto3/s3/inject.py\u001b[0m in \u001b[0;36mbucket_download_file\u001b[0;34m(self, Key, Filename, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    244\u001b[0m     return self.meta.client.download_file(\n\u001b[1;32m    245\u001b[0m         \u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         ExtraArgs=ExtraArgs, Callback=Callback, Config=Config)\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/boto3/s3/inject.py\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(self, Bucket, Key, Filename, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    170\u001b[0m         return transfer.download_file(\n\u001b[1;32m    171\u001b[0m             \u001b[0mbucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             extra_args=ExtraArgs, callback=Callback)\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/boto3/s3/transfer.py\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(self, bucket, key, filename, extra_args, callback)\u001b[0m\n\u001b[1;32m    305\u001b[0m             bucket, key, filename, extra_args, subscribers)\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0;31m# This is for backwards compatibility where when retries are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;31m# exceeded we need to throw the same error from boto3 instead of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;31m# however if a KeyboardInterrupt is raised we want want to exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# out of this and propogate the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;31m# final result.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/s3transfer/tasks.py\u001b[0m in \u001b[0;36m_main\u001b[0;34m(self, transfer_future, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;31m# Call the submit method to start submitting tasks to execute the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;31m# transfer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_submit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;31m# If there was an exception raised during the submission of task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/s3transfer/download.py\u001b[0m in \u001b[0;36m_submit\u001b[0;34m(self, client, config, osutil, request_executor, io_executor, transfer_future, bandwidth_limiter)\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                 \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m             )\n\u001b[1;32m    345\u001b[0m             transfer_future.meta.provide_transfer_size(\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (404) when calling the HeadObject operation: Not Found"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "class SentimentAnalysis(Predictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super().__init__(\n",
    "            endpoint_name, \n",
    "            sagemaker_session = sagemaker_session, \n",
    "            serializer = JSONSerializer(), \n",
    "            deserializer = JSONDeserializer()\n",
    "        )\n",
    "\n",
    "        \n",
    "names = []\n",
    "for _ in estimators:\n",
    "    names.append(name_from_base(\"imdb-huggingface\"))\n",
    "    time.sleep(1)\n",
    "\n",
    "models = [\n",
    "    PyTorchModel(\n",
    "        name = name,\n",
    "        role = role, \n",
    "        model_data = estimator.model_data,\n",
    "        source_dir = \"./scripts\",\n",
    "        entry_point = \"torchserve-predictor.py\",\n",
    "        framework_version = \"1.6.0\",\n",
    "        py_version = \"py36\",\n",
    "        predictor_cls = SentimentAnalysis\n",
    "    ) for name, estimator in zip(names, estimators)\n",
    "]\n",
    "\n",
    "predictors = [\n",
    "    model.deploy(\n",
    "        initial_instance_count = 1, \n",
    "        instance_type = \"ml.m5.large\",\n",
    "        endpoint_name = name,\n",
    "        wait = False\n",
    "    ) for name, model in zip(names, models)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb-huggingface-2021-05-05-14-06-19-051\n",
      "    POSITIVE: Willow is the greatest movie that has ever lived.\n",
      "    NEGATIVE: The Notebook is ironically depressing.\n",
      "    NEGATIVE: My cat's breath smells like cat food.\n",
      "imdb-huggingface-2021-05-05-14-06-20-052\n",
      "    POSITIVE: Willow is the greatest movie that has ever lived.\n",
      "    NEGATIVE: The Notebook is ironically depressing.\n",
      "    NEGATIVE: My cat's breath smells like cat food.\n"
     ]
    }
   ],
   "source": [
    "inputs = [\n",
    "    \"Willow is the greatest movie that has ever lived.\",\n",
    "    \"The Notebook is ironically depressing.\",\n",
    "    \"My cat's breath smells like cat food.\"\n",
    "]\n",
    "\n",
    "for predictor in predictors:\n",
    "    print(predictor.endpoint_name)\n",
    "    for it in inputs:\n",
    "        prediction = predictor.predict({\"text\": it})\n",
    "        print(f'    {prediction}: {it}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for i in range(len(models)):\n",
    "        predictors[i].delete_endpoint()\n",
    "        models[i].delete_model()\n",
    "except:\n",
    "    display(\"Already deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.6-cpu-py36-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
