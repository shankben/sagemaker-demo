{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Sentiment Classifier\n",
    "### Using Hugging Face with the SageMaker SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What We're Going To Do:\n",
    "\n",
    "#### Installation\n",
    "1. Install the SageMaker SDK and the Hugging Face libraries\n",
    "1. Start a SageMaker session, including the default IAM role and S3 bucket\n",
    "    \n",
    "#### Data Preparation\n",
    "1. Tokenization: Download and prepare our IMDB dataset for NLP model training\n",
    "1. Upload our tokenized and split dataset to S3\n",
    "\n",
    "#### Model Training\n",
    "1. Setup an Estimator\n",
    "1. Train a model\n",
    "\n",
    "#### Realtime Inference\n",
    "1. Prepare the model for deployment\n",
    "1. Deploy the model and create a Predictor\n",
    "1. Make inferences using a Predictor\n",
    "\n",
    "#### Clean Up"
   ]
  },
  {
   "attachments": {
    "what-is-ml.svg": {
     "image/svg+xml": [
      "<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" width="336px" height="425px" viewBox="-0.5 -0.5 336 425" content="&lt;mxfile host=&quot;Electron&quot; modified=&quot;2021-05-17T13:25:46.785Z&quot; agent=&quot;5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) draw.io/14.6.13 Chrome/89.0.4389.128 Electron/12.0.7 Safari/537.36&quot; etag=&quot;crTZht76QfDrV0jaZEQi&quot; version=&quot;14.6.13&quot; type=&quot;device&quot;&gt;&lt;diagram id=&quot;-nsVYrA13-EjBoEKal0-&quot; name=&quot;Page-1&quot;&gt;5Vpbc9o4FP41zLQP2bHlC+YxQNqd2XTaSbbT9lHYwlZXWF5ZDrC/fiVbxhfZQAIOtEkesI6ko6PvfN+RuIys2WrzkcEk+kQDREbACDYjaz4CwLTHhniRlq2yOMArLCHDgbJVhkf8H1JGNTHMcIDSxkBOKeE4aRp9GsfI5w0bZIyum8OWlDRXTWCINMOjD4lu/YYDHimraxhVx58Ih1G5NCh7VrAcrQxpBAO6rpmsu5E1Y5Ty4mm1mSEi4SuBKeZ96OndRcZQzI+ZgL8+/D3fMvD5y4fwPqBPxl8P8EZ5eYIkUzueQw5VwHxbwiA8CcRFY7qOMEePCfRlz1pkXdgiviKiZYpHmCZFGpZ4g8TC0yUmZEYJZbkjK3CQF9jCnnJG/0G1Hg8sLNeVM2jMFRFM2VYRIsbRpnfr5g5QwUVEV4izrRiiJthlUhQNd0laVzn1lCmqZbO0QcWicOe5wlk8KKifATvQYP+c8STjwwG/XCLX97uAD8aThcBjEOBd+8qAd3WEAyF41aSMRzSkMSR3lXXKaBYHEtO5RKkac09potD/iTjfKuxgxmkzN2iD+ffa8w/p6g9HteYb5TlvbFWjnQwBONt+L0fKRs2LbFZu8lbpp9iv3OT+FIq6ClmI+CHS6qlmiECOn5r+uxKnpn6hWKxcUaStTbuV+5RmzEdqVr2+tR2Nm44ss+Wo2KDmKOfRbj8vp5ajaVrnWhzcykNJtHwC0xT7pxPlpcToVvsxRCkScvhQOUgo0+1mVK04OB3FobSdSLwxaPLFM15IPAccYPDAxDM7iHadRe0KuTo+kquTcxQ/IX64rQ1IJCXSforakyazbMvYy0RtvGe0CFdEcFb6jd/kFdKaXPgmY1q/iuoHVK83yJWkXdDtdhJ7Cvqp8nbsK5S3p8n793yr0ha4c+m3KqapQ3ydAr/CY31y7BXU6CbF0RXkpBRPNG19gn6EYySM9wiyGMehRoJT1eXI/y51ufnfMOrSSqp9aXUZGvYPGUHpgPcWiLxlZ1lzfQ8tlq/zCczlgbc14EfAJVxtvwG++29Gy46bNAfmVgwAdrKpOsVTqF5zL4u2Qbn1d4BXM8sro+ZsF5PYYzG/9Gkc41ZltCPG6WmO++K1DrsVPRU0uds2XEdFoM5gLQKnN4KO5fuGtrQnaM+bImtqJ6YxaglNmSDBYSyavpAJEvapFBH2IblVHSscBKRP1c1TtKZKYJ/psmE6DVWarq5KMOmQJRhMlu7bqIfahzUXv+bp75/PUw/fvaye9C23VmDIBRfya7Rn1Mx6HKVtdtTo4aMWr3AlqRsv0mRfbXrdsPoL+m8EZprA+DnLFV9olNuQdhYu3gFZTI0cBMN0qmfj/f5Tqli9sL+/XEh7MKtHWDuyn3c8v1o2B02yaXgVojaonidmJ6a9ML7ZO4bVccewwHnuGKJZ/ZCh+OSp+kGIdfc/&lt;/diagram&gt;&lt;/mxfile&gt;" style="background-color: rgb(255, 255, 255);"><defs/><g><ellipse cx="40" cy="84" rx="40" ry="40" fill="#d5e8d4" stroke="#82b366" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 78px; height: 1px; padding-top: 84px; margin-left: 1px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; "><div style="display: inline-block; font-size: 16px; font-family: Helvetica; color: #000000; line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">Data</div></div></div></foreignObject><text x="40" y="89" fill="#000000" font-family="Helvetica" font-size="16px" text-anchor="middle">Data</text></switch></g><ellipse cx="280" cy="84" rx="40" ry="40" fill="#ffe6cc" stroke="#d79b00" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 78px; height: 1px; padding-top: 84px; margin-left: 241px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; "><div style="display: inline-block; font-size: 16px; font-family: Helvetica; color: #000000; line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">Output</div></div></div></foreignObject><text x="280" y="89" fill="#000000" font-family="Helvetica" font-size="16px" text-anchor="middle">Output</text></switch></g><path d="M 200 84 L 233.63 84" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 238.88 84 L 231.88 87.5 L 233.63 84 L 231.88 80.5 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="all"/><path d="M 80 84 L 113.63 84" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 118.88 84 L 111.88 87.5 L 113.63 84 L 111.88 80.5 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="all"/><path d="M 80 274 L 90 274 L 90 324 L 113.63 324" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 118.88 324 L 111.88 327.5 L 113.63 324 L 111.88 320.5 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="all"/><ellipse cx="40" cy="274" rx="40" ry="40" fill="#d5e8d4" stroke="#82b366" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 78px; height: 1px; padding-top: 274px; margin-left: 1px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; "><div style="display: inline-block; font-size: 16px; font-family: Helvetica; color: #000000; line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">Data</div></div></div></foreignObject><text x="40" y="279" fill="#000000" font-family="Helvetica" font-size="16px" text-anchor="middle">Data</text></switch></g><path d="M 80 384 L 90 384 L 90 324 L 113.63 324" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 118.88 324 L 111.88 327.5 L 113.63 324 L 111.88 320.5 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="all"/><ellipse cx="40" cy="384" rx="40" ry="40" fill="#ffe6cc" stroke="#d79b00" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 78px; height: 1px; padding-top: 384px; margin-left: 1px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; "><div style="display: inline-block; font-size: 16px; font-family: Helvetica; color: #000000; line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">Output</div></div></div></foreignObject><text x="40" y="389" fill="#000000" font-family="Helvetica" font-size="16px" text-anchor="middle">Output</text></switch></g><path d="M 200 324 L 233.63 324" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 238.88 324 L 231.88 327.5 L 233.63 324 L 231.88 320.5 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="all"/><rect x="120" y="284" width="80" height="80" fill="#f5f5f5" stroke="#666666" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 78px; height: 1px; padding-top: 324px; margin-left: 121px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; "><div style="display: inline-block; font-size: 16px; font-family: Helvetica; color: #000000; line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">Machine Learning</div></div></div></foreignObject><text x="160" y="329" fill="#000000" font-family="Helvetica" font-size="16px" text-anchor="middle">Machine Le...</text></switch></g><ellipse cx="280" cy="324" rx="40" ry="40" fill="#dae8fc" stroke="#6c8ebf" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 78px; height: 1px; padding-top: 324px; margin-left: 241px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; "><div style="display: inline-block; font-size: 16px; font-family: Helvetica; color: #000000; line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">Rules</div></div></div></foreignObject><text x="280" y="329" fill="#000000" font-family="Helvetica" font-size="16px" text-anchor="middle">Rules</text></switch></g><rect x="15" y="4" width="290" height="20" fill="none" stroke="none" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 288px; height: 1px; padding-top: 14px; margin-left: 16px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; "><div style="display: inline-block; font-size: 24px; font-family: Helvetica; color: #000000; line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; "><font style="font-size: 24px"><b><font color="#82b366">2</font> <font color="#6c8ebf">+</font> <font color="#82b366">3</font> </b>=<b> <font color="#d79b00">5</font></b></font></div></div></div></foreignObject><text x="160" y="21" fill="#000000" font-family="Helvetica" font-size="24px" text-anchor="middle">2 + 3 = 5</text></switch></g><ellipse cx="160" cy="84" rx="40" ry="40" fill="#dae8fc" stroke="#6c8ebf" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 78px; height: 1px; padding-top: 84px; margin-left: 121px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; "><div style="display: inline-block; font-size: 16px; font-family: Helvetica; color: #000000; line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">Rules</div></div></div></foreignObject><text x="160" y="89" fill="#000000" font-family="Helvetica" font-size="16px" text-anchor="middle">Rules</text></switch></g><rect x="15" y="204" width="320" height="20" fill="none" stroke="none" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 318px; height: 1px; padding-top: 214px; margin-left: 16px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; "><div style="display: inline-block; font-size: 24px; font-family: Helvetica; color: #000000; line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; "><font style="font-size: 24px">(<font color="#82b366" style="font-weight: bold">2</font><font>,</font><font color="#82b366" style="font-weight: bold"> </font><font color="#82b366" style="font-weight: bold">3</font><font>,</font><font color="#82b366" style="font-weight: bold"> </font><span style="font-weight: bold ; color: rgb(215 , 155 , 0)">5</span>)<span style="font-weight: bold ; color: rgb(215 , 155 , 0)"> </span>=<font color="#d79b00" style="font-weight: bold"> </font></font><span style="font-weight: bold ; color: rgb(108 , 142 , 191)">+</span></div></div></div></foreignObject><text x="175" y="221" fill="#000000" font-family="Helvetica" font-size="24px" text-anchor="middle">(2, 3, 5) = +</text></switch></g></g><switch><g requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"/><a transform="translate(0,-5)" xlink:href="https://www.diagrams.net/doc/faq/svg-export-text-problems" target="_blank"><text text-anchor="middle" font-size="10px" x="50%" y="100%">Viewer does not support full SVG 1.1</text></a></switch></svg>"
     ]
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# But What _Is_ Machine Learning?\n",
    "\n",
    "For our purposes, we can think of machine learning as a method of using computers to learn the rules of computation. \n",
    "\n",
    "For example, in a traditional computation like adding two integers, we supply the input data, the integers 2 and 3, and wish to apply a rule, addition, to compute the output, 5. Computers are convenient for these types of operations for obvious historical reasons.\n",
    "\n",
    "However, with machine learning, we supply the input and output data, but are interested in computing the unknown rules that generated our output from the input. This process is not magic. Behind the scenes, machine learning relies on statistical techniques and often complex framing of the problem as one of optimizing the fit of rules that minimize the error between the input and output data. Both how this optimization problem is framed and what particular mechanisms are employed to use computers to fit optimized rules to the data is at the frontier of machine learning research. \n",
    "\n",
    "Due to the increasingly convenient and economical benefits of cloud computing of the past decade, machine learning has become more accessible and democratized. However, to perform machine learning in a cloud environment, one is still responsible for the data preparation, training, and inference infrastructure. This is where Amazon SageMaker is beneficial. It's a machine learning service that you can use to build, train, and deploy machine learning models for virtually any use case.\n",
    "\n",
    "![diagram](assets/what-is-ml.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Installation\n",
    "##### ⏰ About 30 seconds\n",
    "\n",
    "This section has nothing to do with machine learning, but sets up our development environment with the requisite SDKs and AWS constructs we'll need to perform machine learning. In particular, we'll fix specific versions of the SageMaker and Hugging Face SDKs, as well as direct our SageMaker Studio session to use a particular S3 bucket for staging our input and output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.2 ms, sys: 47.4 ms, total: 81.6 ms\n",
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture\n",
    "\n",
    "import os\n",
    "\n",
    "DATASETS_VERSION = \"1.6.2\"\n",
    "TRANSFORMERS_VERSION = \"4.5.0\"\n",
    "SAGEMAKER_VERSION = \"2.40.0\"\n",
    "\n",
    "requirements_txt = f\"\"\"numpy\n",
    "pandas\n",
    "transformers=={TRANSFORMERS_VERSION}\n",
    "datasets=={DATASETS_VERSION}\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(os.getcwd(), \"scripts\", \"requirements.txt\"), \"w\") as f:\n",
    "    f.write(requirements_txt)\n",
    "\n",
    "!pip install --upgrade \"sagemaker==$SAGEMAKER_VERSION\" \"transformers==$TRANSFORMERS_VERSION\" \"datasets[s3]==$DATASETS_VERSION\"\n",
    "!conda install -c conda-forge ipywidgets -y\n",
    "\n",
    "# import IPython\n",
    "# IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "import sagemaker.huggingface\n",
    "\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker role arn: arn:aws:iam::934284400219:role/service-role/AmazonSageMaker-ExecutionRole-20210510T080103\n",
      "SageMaker bucket: sagemaker-us-east-1-934284400219\n",
      "SageMaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "print(f\"SageMaker role arn: {role}\")\n",
    "print(f\"SageMaker bucket: {session.default_bucket()}\")\n",
    "print(f\"SageMaker session region: {session.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Split the Dataset\n",
    "\n",
    "Machine learning datasets are often a mixture of labeled and unlabeled data. For this example, we'll only be using labeled from the IMDB movie reviews. \n",
    "\n",
    "When a model is trained, the process feeds labeled examples from our dataset into the training algorithm, which evaluates its performance against other labeled examples in the dataset. If the model is doing well, then the error between its predictions and the test data will be low. But we need to first decide how much of our dataset will be used for training and how much will be used for evaluating the model as it is trained. For our example, we'll simply split the labeled dataset in half and use one half for training and the other for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/4ea52f2e58a08dbc12c2bd52d0d92b30b88c00230b4522801b3636782f625c5b)\n"
     ]
    }
   ],
   "source": [
    "%% time\n",
    "\n",
    "import importlib\n",
    "import pandas\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from sagemaker_demo_helper import SageMakerDemoHelper\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_dataset, test_dataset = datasets.load_dataset(\n",
    "    \"imdb\", \n",
    "    ignore_verifications = True,\n",
    "    split = [\"train\", \"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "##### ⏰ About 1 minute\n",
    "\n",
    "NLP models are not trained directly against the natural languages they form predictions over. Generally speaking, machine learning models are trained with numerical inputs. Tokenization is the data preparation process by which we take our natural English language movie reviews and transform them into numbers the model training algorithm understands. There are many different ways to tokenize natural language data. In our case we will select the tokenizer that was originally used for training the pretrained [DiltilBERT model Hugging Face provides](https://huggingface.co/distilbert-base-uncased)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937727af4ac84225b3b637378149fbbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be83a8030c1b48a5aa5668ba118c11d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1min 5s, sys: 584 ms, total: 1min 6s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenize = lambda batch: tokenizer(\n",
    "    batch[\"text\"], \n",
    "    padding = \"max_length\", \n",
    "    truncation = \"longest_first\"\n",
    ")\n",
    "\n",
    "train_ds = train_dataset.shuffle().map(tokenize)\n",
    "test_ds = test_dataset.shuffle().map(tokenize)\n",
    "\n",
    "try:\n",
    "    train_ds = train_ds.rename_column(\"label\", \"labels\")\n",
    "    test_ds = test_ds.rename_column(\"label\", \"labels\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "columns = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "train_ds.set_format(\"torch\", columns = columns)\n",
    "test_ds.set_format(\"torch\", columns = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So What Does a Tokenized Natural Language Dataset Look Like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last November, I had a chance to see this film...</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2197, 2281, 1010, 1045, 2018, 1037, 3382...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was great to see some of my favorite stars ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 2009, 2001, 2307, 2000, 2156, 2070, 1997...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Well, this might be one of the funniest movies...</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2092, 1010, 2023, 2453, 2022, 2028, 1997...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Right from the start you see that \"Anchors Awe...</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2157, 2013, 1996, 2707, 2017, 2156, 2008...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I watched this film in shire joy.&lt;br /&gt;&lt;br /&gt;T...</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 1045, 3427, 2023, 2143, 1999, 13182, 656...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>The creativeness of this movie was lost from t...</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 1996, 5541, 2791, 1997, 2023, 3185, 2001...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>This was an exteremely good historical drama. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2023, 2001, 2019, 4654, 3334, 21382, 213...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>I was expecting a lot better from the Battlest...</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 1045, 2001, 8074, 1037, 2843, 2488, 2013...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Creative use of modern and mystical elements: ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 5541, 2224, 1997, 2715, 1998, 17529, 378...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>I really liked this film when it was released,...</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 1045, 2428, 4669, 2023, 2143, 2043, 2009...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  labels  \\\n",
       "0   Last November, I had a chance to see this film...       1   \n",
       "1   It was great to see some of my favorite stars ...       0   \n",
       "2   Well, this might be one of the funniest movies...       1   \n",
       "3   Right from the start you see that \"Anchors Awe...       1   \n",
       "4   I watched this film in shire joy.<br /><br />T...       1   \n",
       "..                                                ...     ...   \n",
       "95  The creativeness of this movie was lost from t...       0   \n",
       "96  This was an exteremely good historical drama. ...       1   \n",
       "97  I was expecting a lot better from the Battlest...       0   \n",
       "98  Creative use of modern and mystical elements: ...       1   \n",
       "99  I really liked this film when it was released,...       1   \n",
       "\n",
       "                                            input_ids  \\\n",
       "0   [101, 2197, 2281, 1010, 1045, 2018, 1037, 3382...   \n",
       "1   [101, 2009, 2001, 2307, 2000, 2156, 2070, 1997...   \n",
       "2   [101, 2092, 1010, 2023, 2453, 2022, 2028, 1997...   \n",
       "3   [101, 2157, 2013, 1996, 2707, 2017, 2156, 2008...   \n",
       "4   [101, 1045, 3427, 2023, 2143, 1999, 13182, 656...   \n",
       "..                                                ...   \n",
       "95  [101, 1996, 5541, 2791, 1997, 2023, 3185, 2001...   \n",
       "96  [101, 2023, 2001, 2019, 4654, 3334, 21382, 213...   \n",
       "97  [101, 1045, 2001, 8074, 1037, 2843, 2488, 2013...   \n",
       "98  [101, 5541, 2224, 1997, 2715, 1998, 17529, 378...   \n",
       "99  [101, 1045, 2428, 4669, 2023, 2143, 2043, 2009...   \n",
       "\n",
       "                                       attention_mask  \n",
       "0   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "..                                                ...  \n",
       "95  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "96  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "97  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "98  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "99  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.to_pandas().head(100)[[\"text\", \"labels\", \"input_ids\", \"attention_mask\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WTF?\n",
    "\n",
    "- `text` contains the raw English IMDB movie reviews \n",
    "- `labels` are the sentiment values for each review where `1` is positive and `0` is negative\n",
    "- `input_ids` are the tokens, referred to here as IDs. Hugging Face associates the token IDs with the raw numerical token values that are fed into the model training loop.\n",
    "- `attention_mask` refers to which elements of the `input_ids` vector are actually processed in the training loop. Because each original `text` is a different length, we've chosen to pad the data to the same length. The attention mask makes sure the empty padding values are not used in the training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the Dataset to S3\n",
    "##### ⏰ About 5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 889 ms, sys: 475 ms, total: 1.36 s\n",
      "Wall time: 6.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "s3_prefix = \"datasets/imdb-binary-classification\"\n",
    "training_input_path = f\"s3://{bucket}/{s3_prefix}/train\"\n",
    "test_input_path = f\"s3://{bucket}/{s3_prefix}/test\"\n",
    "\n",
    "train_ds.save_to_disk(training_input_path, fs = s3)\n",
    "test_ds.save_to_disk(test_input_path, fs = s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 16px;\"><a href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-934284400219?region=us-east-1&prefix=datasets/imdb-binary-classification/&showversions=false\">Prove it Landed in S3</a></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup an Estimator\n",
    "\n",
    "Estimators are part of the SageMaker SDK and represent at a high-level the model training job, data access, and managed infrastructure required to produce the trained model artifact. Using the latest version of the SageMaker SDK, we can leverage its [Hugging Face integration](https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face) to simplify the training process.\n",
    "\n",
    "How do we evaluate the model training performance as its running? When we train a model using SageMaker, we can monitor several metrics in real time in AWS using Amazon CloudWatch. In particular, we'll look at two varieties of metrics: the EC2 training instance metrics and the training algorithm metrics. The EC2 training instance metrics will be supplied by SageMaker without needing to configure anything. But to capture the specific Hugging Face model training metrics, we need to tell the `HuggingFace` estimator that we're interested in specific ones, which we do by specifiying in the `metric_definitions` list below. There are many more detailed metrics we can subscribe to, but for this example we will only pay attention to two: the epoch and the loss. \n",
    "\n",
    "Loosely speaking, when we train a machine learning model over a dataset, one complete run through the dataset is called an _epoch_. Usually models are trained for more than one epoch, and in our case we will train for three epochs. The _loss_ is a generalized notion of the error associated with the model's performance against the test dataset we split from the training set at the beginning of this notebook. The lower the loss is, the better our model is at predicting correct sentiment labels on the test dataset, which it has never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "job_name = \"imdb-huggingface\"\n",
    "\n",
    "metric_definitions = [\n",
    "    { \"Name\": \"epoch\", \"Regex\": \"'epoch': ([0-9]+(.|e\\-)[0-9]+),?\" },\n",
    "    { \"Name\": \"loss\", \"Regex\": \"'loss': ([0-9]+(.|e\\-)[0-9]+),?\" },\n",
    "#     { \"Name\": \"learning_rate\", \"Regex\": \"'learning_rate': ([0-9]+(.|e\\-)[0-9]+),?\" },\n",
    "#     { \"Name\": \"eval_loss\", \"Regex\": \"'eval_loss': ([0-9]+(.|e\\-)[0-9]+),?\" },\n",
    "#     { \"Name\": \"eval_accuracy\", \"Regex\": \"'eval_accuracy': ([0-9]+(.|e\\-)[0-9]+),?\" },\n",
    "#     { \"Name\": \"eval_f1\", \"Regex\": \"'eval_f1': ([0-9]+(.|e\\-)[0-9]+),?\" },\n",
    "#     { \"Name\": \"eval_precision\", \"Regex\": \"'eval_precision': ([0-9]+(.|e\\-)[0-9]+),?\" },\n",
    "#     { \"Name\": \"eval_recall\", \"Regex\": \"'eval_recall': ([0-9]+(.|e\\-)[0-9]+),?\" },\n",
    "#     { \"Name\": \"eval_runtime\", \"Regex\": \"'eval_runtime': ([0-9]+(.|e\\-)[0-9]+),?\" },\n",
    "#     { \"Name\": \"eval_samples_per_second\", \"Regex\": \"'eval_samples_per_second': ([0-9]+(.|e\\-)[0-9]+),?\" },\n",
    "]\n",
    "\n",
    "estimator = HuggingFace(\n",
    "    base_job_name = job_name,\n",
    "    role = role,\n",
    "    py_version = \"py36\",\n",
    "    pytorch_version = \"1.6.0\",\n",
    "    transformers_version = TRANSFORMERS_VERSION,\n",
    "    entry_point = \"trainer.py\",\n",
    "    instance_count = 1,\n",
    "    instance_type = \"ml.p3.16xlarge\",\n",
    "    source_dir = \"./scripts\",\n",
    "    enable_sagemaker_metrics = True,\n",
    "    metric_definitions = metric_definitions,\n",
    "    hyperparameters = {\n",
    "        \"epochs\": 3,\n",
    "        \"eval_batch_size\": 128,\n",
    "        \"model_name\": model_name,\n",
    "        \"train_batch_size\": 64\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 16px;\"><a href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs/\">See Training Jobs in the SageMaker Console</a></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Model using the Estimator\n",
    "##### ⏰ About 15 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-13 21:37:42 Starting - Starting the training job...\n",
      "2021-05-13 21:38:06 Starting - Launching requested ML instancesProfilerReport-1620941862: InProgress\n",
      "............\n",
      "2021-05-13 21:40:06 Starting - Preparing the instances for training.........\n",
      "2021-05-13 21:41:27 Downloading - Downloading input data...\n",
      "2021-05-13 21:42:07 Training - Downloading the training image..............\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-05-13 21:44:19,641 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-05-13 21:44:19,720 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-05-13 21:44:19,728 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-05-13 21:44:20,117 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers==4.5.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (4.5.0)\u001b[0m\n",
      "\u001b[34mCollecting datasets==1.6.2\n",
      "  Downloading datasets-1.6.2-py3-none-any.whl (221 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from datasets==1.6.2->-r requirements.txt (line 4)) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.6/site-packages (from datasets==1.6.2->-r requirements.txt (line 4)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.6/site-packages (from datasets==1.6.2->-r requirements.txt (line 4)) (2.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=1.0.0<4.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets==1.6.2->-r requirements.txt (line 4)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from datasets==1.6.2->-r requirements.txt (line 4)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets==1.6.2->-r requirements.txt (line 4)) (0.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets==1.6.2->-r requirements.txt (line 4)) (0.70.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.6/site-packages (from datasets==1.6.2->-r requirements.txt (line 4)) (0.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from datasets==1.6.2->-r requirements.txt (line 4)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<0.1.0 in /opt/conda/lib/python3.6/site-packages (from datasets==1.6.2->-r requirements.txt (line 4)) (0.0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.6/site-packages (from datasets==1.6.2->-r requirements.txt (line 4)) (4.49.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers==4.5.0->-r requirements.txt (line 3)) (0.0.45)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.5.0->-r requirements.txt (line 3)) (2021.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers==4.5.0->-r requirements.txt (line 3)) (3.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.5.0->-r requirements.txt (line 3)) (0.10.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets==1.6.2->-r requirements.txt (line 4)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets==1.6.2->-r requirements.txt (line 4)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets==1.6.2->-r requirements.txt (line 4)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets==1.6.2->-r requirements.txt (line 4)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 2)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 2)) (2021.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas->-r requirements.txt (line 2)) (1.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->datasets==1.6.2->-r requirements.txt (line 4)) (3.7.4.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->datasets==1.6.2->-r requirements.txt (line 4)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->datasets==1.6.2->-r requirements.txt (line 4)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.5.0->-r requirements.txt (line 3)) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.5.0->-r requirements.txt (line 3)) (1.0.1)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: datasets\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 1.5.0\n",
      "    Uninstalling datasets-1.5.0:\n",
      "      Successfully uninstalled datasets-1.5.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed datasets-1.6.2\u001b[0m\n",
      "\u001b[34m2021-05-13 21:44:23,528 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"train_batch_size\": 64,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"epochs\": 3,\n",
      "        \"eval_batch_size\": 128\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"imdb-huggingface-2021-05-13-21-37-42-367\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-934284400219/imdb-huggingface-2021-05-13-21-37-42-367/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"trainer\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"trainer.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":3,\"eval_batch_size\":128,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":64}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=trainer.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=trainer\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-934284400219/imdb-huggingface-2021-05-13-21-37-42-367/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":3,\"eval_batch_size\":128,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":64},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"imdb-huggingface-2021-05-13-21-37-42-367\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-934284400219/imdb-huggingface-2021-05-13-21-37-42-367/source/sourcedir.tar.gz\",\"module_name\":\"trainer\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"trainer.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"3\",\"--eval_batch_size\",\"128\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"64\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_BATCH_SIZE=128\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 trainer.py --epochs 3 --eval_batch_size 128 --model_name distilbert-base-uncased --train_batch_size 64\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m2021-05-13 21:44:28,284 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m2021-05-13 21:44:28,285 - __main__ - INFO -  loaded test_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m2021-05-13 21:44:28,310 - filelock - INFO - Lock 140139416021704 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m2021-05-13 21:44:28,329 - filelock - INFO - Lock 140139416021704 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m2021-05-13 21:44:28,349 - filelock - INFO - Lock 140132189024720 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\n",
      "2021-05-13 21:44:27 Training - Training image download completed. Training in progress.\u001b[34m2021-05-13 21:44:33,736 - filelock - INFO - Lock 140132189024720 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.028 algo-1:31 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.171 algo-1:31 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.172 algo-1:31 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.172 algo-1:31 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.173 algo-1:31 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.173 algo-1:31 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.495 algo-1:31 INFO hook.py:584] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.496 algo-1:31 INFO hook.py:584] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.496 algo-1:31 INFO hook.py:584] name:module.distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.496 algo-1:31 INFO hook.py:584] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.496 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.496 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.496 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.496 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.496 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.496 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.496 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.496 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.496 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.497 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.497 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.497 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.497 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.497 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.497 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.497 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.497 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.497 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.497 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.497 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.497 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.497 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.497 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.498 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.498 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.498 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.498 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.498 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.498 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.498 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.498 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.498 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.498 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.498 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.498 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.499 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.499 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.499 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.499 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.499 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.499 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.499 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.499 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.499 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.499 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.499 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.499 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.499 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.500 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.500 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.500 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.500 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.500 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.500 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.500 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.500 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.500 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.500 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.500 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.500 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.500 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.500 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.500 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.500 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.500 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.500 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.501 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.501 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.501 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.501 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.501 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.501 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.501 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.501 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.501 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.501 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.501 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.501 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.501 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.501 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.501 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.501 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.501 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.501 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.502 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.502 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.502 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.502 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.502 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.502 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.502 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.502 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.502 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.502 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.502 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.502 algo-1:31 INFO hook.py:584] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.502 algo-1:31 INFO hook.py:584] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.502 algo-1:31 INFO hook.py:584] name:module.pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.502 algo-1:31 INFO hook.py:584] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.502 algo-1:31 INFO hook.py:584] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.503 algo-1:31 INFO hook.py:586] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.503 algo-1:31 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-05-13 21:44:41.505 algo-1:31 INFO hook.py:476] Hook is writing from the hook with pid: 31\n",
      "\u001b[0m\n",
      "\u001b[34mNCCL version 2.4.8+cuda11.0\u001b[0m\n",
      "\u001b[34m{'loss': 0.6898, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m{'loss': 0.688, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m{'loss': 0.6836, 'learning_rate': 3e-06, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m{'loss': 0.6741, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m{'loss': 0.6524, 'learning_rate': 5e-06, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m{'loss': 0.5851, 'learning_rate': 6e-06, 'epoch': 0.61}\u001b[0m\n",
      "\u001b[34m{'loss': 0.4646, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.71}\u001b[0m\n",
      "\u001b[34m{'loss': 0.3662, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.82}\u001b[0m\n",
      "\u001b[34m{'loss': 0.3204, 'learning_rate': 9e-06, 'epoch': 0.92}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2681870460510254, 'eval_accuracy': 0.89712, 'eval_f1': 0.8970458730285806, 'eval_precision': 0.8976926774555359, 'eval_recall': 0.8964, 'eval_runtime': 20.6685, 'eval_samples_per_second': 1209.568, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2931, 'learning_rate': 1e-05, 'epoch': 1.02}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2582, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.12}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2633, 'learning_rate': 1.2e-05, 'epoch': 1.22}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2367, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.33}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2294, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.43}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2226, 'learning_rate': 1.5e-05, 'epoch': 1.53}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2591, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.63}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2302, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.73}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2387, 'learning_rate': 1.8e-05, 'epoch': 1.84}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2305, 'learning_rate': 1.9e-05, 'epoch': 1.94}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.20483413338661194, 'eval_accuracy': 0.91984, 'eval_f1': 0.9205833399381786, 'eval_precision': 0.9121250196324799, 'eval_recall': 0.9292, 'eval_runtime': 20.2945, 'eval_samples_per_second': 1231.858, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1908, 'learning_rate': 2e-05, 'epoch': 2.04}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1793, 'learning_rate': 2.1e-05, 'epoch': 2.14}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1766, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.24}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1821, 'learning_rate': 2.3000000000000003e-05, 'epoch': 2.35}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1886, 'learning_rate': 2.4e-05, 'epoch': 2.45}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1825, 'learning_rate': 2.5e-05, 'epoch': 2.55}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2076, 'learning_rate': 2.6000000000000002e-05, 'epoch': 2.65}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2051, 'learning_rate': 2.7000000000000002e-05, 'epoch': 2.76}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2043, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.86}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2015, 'learning_rate': 2.9e-05, 'epoch': 2.96}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.19652530550956726, 'eval_accuracy': 0.92448, 'eval_f1': 0.9246548008620001, 'eval_precision': 0.9225195094760312, 'eval_recall': 0.9268, 'eval_runtime': 20.4451, 'eval_samples_per_second': 1222.785, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m{'train_runtime': 298.7791, 'train_samples_per_second': 0.984, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m***** Eval results *****\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/442 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 442/442 [00:00<00:00, 524kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]#015Downloading:   1%|▏         | 3.97M/268M [00:00<00:06, 39.7MB/s]#015Downloading:   3%|▎         | 8.10M/268M [00:00<00:06, 40.2MB/s]#015Downloading:   5%|▍         | 12.8M/268M [00:00<00:06, 42.0MB/s]#015Downloading:   7%|▋         | 17.6M/268M [00:00<00:05, 43.5MB/s]#015Downloading:   8%|▊         | 22.6M/268M [00:00<00:05, 45.4MB/s]#015Downloading:  10%|█         | 27.7M/268M [00:00<00:05, 47.0MB/s]#015Downloading:  12%|█▏        | 32.8M/268M [00:00<00:04, 48.1MB/s]#015Downloading:  14%|█▍        | 37.9M/268M [00:00<00:04, 49.0MB/s]#015Downloading:  16%|█▌        | 43.1M/268M [00:00<00:04, 49.8MB/s]#015Downloading:  18%|█▊        | 48.3M/268M [00:01<00:04, 50.5MB/s]#015Downloading:  20%|█▉        | 53.6M/268M [00:01<00:04, 51.0MB/s]#015Downloading:  22%|██▏       | 58.8M/268M [00:01<00:04, 51.3MB/s]#015Downloading:  24%|██▍       | 64.0M/268M [00:01<00:03, 51.7MB/s]#015Downloading:  26%|██▌       | 69.3M/268M [00:01<00:03, 52.1MB/s]#015Downloading:  28%|██▊       | 74.7M/268M [00:01<00:03, 52.5MB/s]#015Downloading:  30%|██▉       | 79.9M/268M [00:01<00:03, 52.5MB/s]#015Downloading:  32%|███▏      | 85.2M/268M [00:01<00:03, 52.5MB/s]#015Downloading:  34%|███▍      | 90.4M/268M [00:01<00:03, 52.5MB/s]#015Downloading:  36%|███▌      | 95.7M/268M [00:01<00:03, 52.4MB/s]#015Downloading:  38%|███▊      | 101M/268M [00:02<00:03, 52.4MB/s] #015Downloading:  40%|███▉      | 106M/268M [00:02<00:03, 52.4MB/s]#015Downloading:  42%|████▏     | 111M/268M [00:02<00:03, 51.6MB/s]#015Downloading:  44%|████▎     | 117M/268M [00:02<00:02, 51.8MB/s]#015Downloading:  45%|████▌     | 122M/268M [00:02<00:02, 52.0MB/s]#015Downloading:  47%|████▋     | 127M/268M [00:02<00:02, 52.2MB/s]#015Downloading:  49%|████▉     | 132M/268M [00:02<00:02, 52.0MB/s]#015Downloading:  51%|█████▏    | 138M/268M [00:02<00:02, 52.2MB/s]#015Downloading:  53%|█████▎    | 143M/268M [00:02<00:02, 52.1MB/s]#015Downloading:  55%|█████▌    | 148M/268M [00:02<00:02, 52.2MB/s]#015Downloading:  57%|█████▋    | 153M/268M [00:03<00:02, 52.3MB/s]#015Downloading:  59%|█████▉    | 159M/268M [00:03<00:02, 52.4MB/s]#015Downloading:  61%|██████    | 164M/268M [00:03<00:01, 52.7MB/s]#015Downloading:  63%|██████▎   | 169M/268M [00:03<00:01, 52.7MB/s]#015Downloading:  65%|██████▌   | 174M/268M [00:03<00:01, 52.6MB/s]#015Downloading:  67%|██████▋   | 180M/268M [00:03<00:01, 52.4MB/s]#015Downloading:  69%|██████▉   | 185M/268M [00:03<00:01, 52.4MB/s]#015Downloading:  71%|███████   | 190M/268M [00:03<00:01, 52.5MB/s]#015Downloading:  73%|███████▎  | 196M/268M [00:03<00:01, 52.6MB/s]#015Downloading:  75%|███████▍  | 201M/268M [00:03<00:01, 52.7MB/s]#015Downloading:  77%|███████▋  | 206M/268M [00:04<00:01, 52.9MB/s]#015Downloading:  79%|███████▉  | 211M/268M [00:04<00:01, 53.0MB/s]#015Downloading:  81%|████████  | 217M/268M [00:04<00:00, 52.9MB/s]#015Downloading:  83%|████████▎ | 222M/268M [00:04<00:00, 52.9MB/s]#015Downloading:  85%|████████▍ | 227M/268M [00:04<00:00, 53.0MB/s]#015Downloading:  87%|████████▋ | 233M/268M [00:04<00:00, 52.9MB/s]#015Downloading:  89%|████████▉ | 238M/268M [00:04<00:00, 52.7MB/s]#015Downloading:  91%|█████████ | 243M/268M [00:04<00:00, 52.7MB/s]#015Downloading:  93%|█████████▎| 249M/268M [00:04<00:00, 54.2MB/s]#015Downloading:  95%|█████████▌| 255M/268M [00:04<00:00, 55.4MB/s]#015Downloading:  97%|█████████▋| 261M/268M [00:05<00:00, 56.4MB/s]#015Downloading: 100%|█████████▉| 267M/268M [00:05<00:00, 57.1MB/s]#015Downloading: 100%|██████████| 268M/268M [00:05<00:00, 52.2MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/294 [00:00<?, ?it/s]/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m#015  0%|          | 1/294 [00:18<1:31:35, 18.75s/it]#015  1%|          | 2/294 [00:19<1:04:58, 13.35s/it]#015  1%|          | 3/294 [00:20<46:23,  9.57s/it]  #015  1%|▏         | 4/294 [00:20<33:26,  6.92s/it]#015  2%|▏         | 5/294 [00:21<24:22,  5.06s/it]#015  2%|▏         | 6/294 [00:22<18:02,  3.76s/it]#015  2%|▏         | 7/294 [00:23<13:37,  2.85s/it]#015  3%|▎         | 8/294 [00:23<10:31,  2.21s/it]#015  3%|▎         | 9/294 [00:24<08:21,  1.76s/it]#015  3%|▎         | 10/294 [00:25<06:54,  1.46s/it]#015                                                #015#015  3%|▎         | 10/294 [00:25<06:54,  1.46s/it]#015  4%|▎         | 11/294 [00:26<05:50,  1.24s/it]#015  4%|▍         | 12/294 [00:26<05:04,  1.08s/it]#015  4%|▍         | 13/294 [00:27<04:33,  1.03it/s]#015  5%|▍         | 14/294 [00:28<04:12,  1.11it/s]#015  5%|▌         | 15/294 [00:28<03:57,  1.18it/s]#015  5%|▌         | 16/294 [00:29<03:48,  1.21it/s]#015  6%|▌         | 17/294 [00:30<03:40,  1.26it/s]#015  6%|▌         | 18/294 [00:31<03:33,  1.29it/s]#015  6%|▋         | 19/294 [00:31<03:30,  1.31it/s]#015  7%|▋         | 20/294 [00:32<03:26,  1.33it/s]#015                                                #015#015  7%|▋         | 20/294 [00:32<03:26,  1.33it/s]#015  7%|▋         | 21/294 [00:33<03:23,  1.34it/s]#015  7%|▋         | 22/294 [00:34<03:20,  1.35it/s]#015  8%|▊         | 23/294 [00:34<03:19,  1.36it/s]#015  8%|▊         | 24/294 [00:35<03:22,  1.33it/s]#015  9%|▊         | 25/294 [00:36<03:19,  1.35it/s]#015  9%|▉         | 26/294 [00:37<03:17,  1.35it/s]#015  9%|▉         | 27/294 [00:37<03:15,  1.36it/s]#015 10%|▉         | 28/294 [00:38<03:14,  1.36it/s]#015 10%|▉         | 29/294 [00:39<03:13,  1.37it/s]#015 10%|█         | 30/294 [00:39<03:12,  1.37it/s]#015                                                #015#015 10%|█         | 30/294 [00:39<03:12,  1.37it/s]#015 11%|█         | 31/294 [00:40<03:12,  1.37it/s]#015 11%|█         | 32/294 [00:41<03:11,  1.37it/s]#015 11%|█         | 33/294 [00:42<03:11,  1.37it/s]#015 12%|█▏        | 34/294 [00:42<03:10,  1.37it/s]#015 12%|█▏        | 35/294 [00:43<03:09,  1.36it/s]#015 12%|█▏        | 36/294 [00:44<03:08,  1.37it/s]#015 13%|█▎        | 37/294 [00:45<03:13,  1.33it/s]#015 13%|█▎        | 38/294 [00:45<03:11,  1.34it/s]#015 13%|█▎        | 39/294 [00:46<03:09,  1.35it/s]#015 14%|█▎        | 40/294 [00:47<03:07,  1.36it/s]#015                                                #015#015 14%|█▎        | 40/294 [00:47<03:07,  1.36it/s]#015 14%|█▍        | 41/294 [00:48<03:06,  1.36it/s]#015 14%|█▍        | 42/294 [00:48<03:05,  1.36it/s]#015 15%|█▍        | 43/294 [00:49<03:04,  1.36it/s]#015 15%|█▍        | 44/294 [00:50<03:03,  1.36it/s]#015 15%|█▌        | 45/294 [00:51<03:03,  1.36it/s]#015 16%|█▌        | 46/294 [00:51<03:02,  1.36it/s]#015 16%|█▌        | 47/294 [00:52<03:01,  1.36it/s]#015 16%|█▋        | 48/294 [00:53<03:00,  1.36it/s]#015 17%|█▋        | 49/294 [00:53<02:59,  1.37it/s]#015 17%|█▋        | 50/294 [00:54<03:04,  1.32it/s]#015                                                #015#015 17%|█▋        | 50/294 [00:54<03:04,  1.32it/s]#015 17%|█▋        | 51/294 [00:55<03:01,  1.34it/s]#015 18%|█▊        | 52/294 [00:56<02:59,  1.35it/s]#015 18%|█▊        | 53/294 [00:56<02:57,  1.36it/s]#015 18%|█▊        | 54/294 [00:57<02:56,  1.36it/s]#015 19%|█▊        | 55/294 [00:58<02:56,  1.36it/s]#015 19%|█▉        | 56/294 [00:59<02:55,  1.35it/s]#015 19%|█▉        | 57/294 [00:59<02:55,  1.35it/s]#015 20%|█▉        | 58/294 [01:00<02:54,  1.35it/s]#015 20%|██        | 59/294 [01:01<02:54,  1.35it/s]#015 20%|██        | 60/294 [01:02<02:54,  1.34it/s]#015                                                #015#015 20%|██        | 60/294 [01:02<02:54,  1.34it/s]#015 21%|██        | 61/294 [01:02<02:53,  1.34it/s]#015 21%|██        | 62/294 [01:03<02:52,  1.34it/s]#015 21%|██▏       | 63/294 [01:04<02:56,  1.31it/s]#015 22%|██▏       | 64/294 [01:05<02:54,  1.32it/s]#015 22%|██▏       | 65/294 [01:05<02:52,  1.33it/s]#015 22%|██▏       | 66/294 [01:06<02:50,  1.34it/s]#015 23%|██▎       | 67/294 [01:07<02:49,  1.34it/s]#015 23%|██▎       | 68/294 [01:08<02:47,  1.35it/s]#015 23%|██▎       | 69/294 [01:08<02:46,  1.35it/s]#015 24%|██▍       | 70/294 [01:09<02:46,  1.34it/s]#015                                                #015#015 24%|██▍       | 70/294 [01:09<02:46,  1.34it/s]#015 24%|██▍       | 71/294 [01:10<02:46,  1.34it/s]#015 24%|██▍       | 72/294 [01:11<02:45,  1.34it/s]#015 25%|██▍       | 73/294 [01:11<02:44,  1.34it/s]#015 25%|██▌       | 74/294 [01:12<02:43,  1.34it/s]#015 26%|██▌       | 75/294 [01:13<02:42,  1.35it/s]#015 26%|██▌       | 76/294 [01:14<02:46,  1.31it/s]#015 26%|██▌       | 77/294 [01:14<02:45,  1.31it/s]#015 27%|██▋       | 78/294 [01:15<02:43,  1.32it/s]#015 27%|██▋       | 79/294 [01:16<02:42,  1.33it/s]#015 27%|██▋       | 80/294 [01:17<02:40,  1.33it/s]#015                                                #015#015 27%|██▋       | 80/294 [01:17<02:40,  1.33it/s]#015 28%|██▊       | 81/294 [01:17<02:39,  1.34it/s]#015 28%|██▊       | 82/294 [01:18<02:38,  1.34it/s]#015 28%|██▊       | 83/294 [01:19<02:37,  1.34it/s]#015 29%|██▊       | 84/294 [01:20<02:36,  1.34it/s]#015 29%|██▉       | 85/294 [01:20<02:35,  1.34it/s]#015 29%|██▉       | 86/294 [01:21<02:35,  1.34it/s]#015 30%|██▉       | 87/294 [01:22<02:34,  1.34it/s]#015 30%|██▉       | 88/294 [01:23<02:33,  1.35it/s]#015 30%|███       | 89/294 [01:23<02:36,  1.31it/s]#015 31%|███       | 90/294 [01:24<02:34,  1.32it/s]#015                                                #015#015 31%|███       | 90/294 [01:24<02:34,  1.32it/s]#015 31%|███       | 91/294 [01:25<02:33,  1.32it/s]#015 31%|███▏      | 92/294 [01:26<02:33,  1.31it/s]#015 32%|███▏      | 93/294 [01:26<02:32,  1.32it/s]#015 32%|███▏      | 94/294 [01:27<02:30,  1.33it/s]#015 32%|███▏      | 95/294 [01:28<02:29,  1.33it/s]#015 33%|███▎      | 96/294 [01:29<02:29,  1.33it/s]#015 33%|███▎      | 97/294 [01:29<02:27,  1.33it/s]#015 33%|███▎      | 98/294 [01:30<02:15,  1.45it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/49 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  4%|▍         | 2/49 [00:00<00:09,  4.88it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  6%|▌         | 3/49 [00:00<00:12,  3.74it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  8%|▊         | 4/49 [00:01<00:13,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 10%|█         | 5/49 [00:01<00:15,  2.83it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 12%|█▏        | 6/49 [00:02<00:15,  2.71it/s]#033[A\u001b[0m\n",
      "\u001b[34m2021-05-13 21:50:05,731 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m#015 14%|█▍        | 7/49 [00:02<00:15,  2.64it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 16%|█▋        | 8/49 [00:02<00:15,  2.58it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 18%|█▊        | 9/49 [00:03<00:15,  2.56it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 20%|██        | 10/49 [00:03<00:15,  2.53it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 22%|██▏       | 11/49 [00:04<00:15,  2.51it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 24%|██▍       | 12/49 [00:04<00:14,  2.50it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 27%|██▋       | 13/49 [00:04<00:14,  2.49it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 29%|██▊       | 14/49 [00:05<00:14,  2.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 31%|███       | 15/49 [00:05<00:13,  2.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 33%|███▎      | 16/49 [00:06<00:13,  2.38it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 35%|███▍      | 17/49 [00:06<00:13,  2.40it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 37%|███▋      | 18/49 [00:07<00:12,  2.42it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 39%|███▉      | 19/49 [00:07<00:12,  2.43it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 41%|████      | 20/49 [00:07<00:11,  2.44it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 43%|████▎     | 21/49 [00:08<00:11,  2.44it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 45%|████▍     | 22/49 [00:08<00:11,  2.44it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 47%|████▋     | 23/49 [00:09<00:10,  2.44it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 49%|████▉     | 24/49 [00:09<00:10,  2.45it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 51%|█████     | 25/49 [00:09<00:09,  2.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 53%|█████▎    | 26/49 [00:10<00:09,  2.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 55%|█████▌    | 27/49 [00:10<00:09,  2.36it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 57%|█████▋    | 28/49 [00:11<00:08,  2.40it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 59%|█████▉    | 29/49 [00:11<00:08,  2.42it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 61%|██████    | 30/49 [00:11<00:07,  2.44it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 63%|██████▎   | 31/49 [00:12<00:07,  2.45it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 65%|██████▌   | 32/49 [00:12<00:06,  2.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 67%|██████▋   | 33/49 [00:13<00:06,  2.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 69%|██████▉   | 34/49 [00:13<00:06,  2.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 71%|███████▏  | 35/49 [00:13<00:05,  2.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 73%|███████▎  | 36/49 [00:14<00:05,  2.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 76%|███████▌  | 37/49 [00:14<00:04,  2.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 78%|███████▊  | 38/49 [00:15<00:04,  2.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 80%|███████▉  | 39/49 [00:15<00:04,  2.37it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 82%|████████▏ | 40/49 [00:16<00:03,  2.40it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 84%|████████▎ | 41/49 [00:16<00:03,  2.43it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 86%|████████▌ | 42/49 [00:16<00:02,  2.44it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 88%|████████▊ | 43/49 [00:17<00:02,  2.40it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 90%|████████▉ | 44/49 [00:17<00:02,  2.39it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 92%|█████████▏| 45/49 [00:18<00:01,  2.38it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 94%|█████████▍| 46/49 [00:18<00:01,  2.38it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 96%|█████████▌| 47/49 [00:18<00:00,  2.36it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 98%|█████████▊| 48/49 [00:19<00:00,  2.36it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 49/49 [00:19<00:00,  2.45it/s]#033[A#015                                                #015\u001b[0m\n",
      "\u001b[34m#015                                               #015#033[A#015 33%|███▎      | 98/294 [01:51<02:15,  1.45it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 49/49 [00:19<00:00,  2.45it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015                                               #033[A/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m#015 34%|███▎      | 99/294 [01:52<22:51,  7.03s/it]#015 34%|███▍      | 100/294 [01:53<16:40,  5.15s/it]#015                                                 #015#015 34%|███▍      | 100/294 [01:53<16:40,  5.15s/it]#015 34%|███▍      | 101/294 [01:53<12:18,  3.83s/it]#015 35%|███▍      | 102/294 [01:54<09:16,  2.90s/it]#015 35%|███▌      | 103/294 [01:55<07:09,  2.25s/it]#015 35%|███▌      | 104/294 [01:55<05:41,  1.80s/it]#015 36%|███▌      | 105/294 [01:56<04:39,  1.48s/it]#015 36%|███▌      | 106/294 [01:57<03:55,  1.25s/it]#015 36%|███▋      | 107/294 [01:58<03:25,  1.10s/it]#015 37%|███▋      | 108/294 [01:58<03:04,  1.01it/s]#015 37%|███▋      | 109/294 [01:59<02:48,  1.10it/s]#015 37%|███▋      | 110/294 [02:00<02:38,  1.16it/s]#015                                                 #015#015 37%|███▋      | 110/294 [02:00<02:38,  1.16it/s]#015 38%|███▊      | 111/294 [02:01<02:30,  1.21it/s]#015 38%|███▊      | 112/294 [02:01<02:25,  1.25it/s]#015 38%|███▊      | 113/294 [02:02<02:20,  1.29it/s]#015 39%|███▉      | 114/294 [02:03<02:17,  1.31it/s]#015 39%|███▉      | 115/294 [02:04<02:15,  1.32it/s]#015 39%|███▉      | 116/294 [02:04<02:13,  1.34it/s]#015 40%|███▉      | 117/294 [02:05<02:14,  1.32it/s]#015 40%|████      | 118/294 [02:06<02:12,  1.33it/s]#015 40%|████      | 119/294 [02:07<02:10,  1.34it/s]#015 41%|████      | 120/294 [02:07<02:09,  1.34it/s]#015                                                 #015#015 41%|████      | 120/294 [02:07<02:09,  1.34it/s]#015 41%|████      | 121/294 [02:08<02:09,  1.34it/s]#015 41%|████▏     | 122/294 [02:09<02:08,  1.34it/s]#015 42%|████▏     | 123/294 [02:10<02:07,  1.34it/s]#015 42%|████▏     | 124/294 [02:10<02:06,  1.35it/s]#015 43%|████▎     | 125/294 [02:11<02:05,  1.35it/s]#015 43%|████▎     | 126/294 [02:12<02:04,  1.35it/s]#015 43%|████▎     | 127/294 [02:13<02:04,  1.34it/s]#015 44%|████▎     | 128/294 [02:13<02:03,  1.34it/s]#015 44%|████▍     | 129/294 [02:14<02:02,  1.35it/s]#015 44%|████▍     | 130/294 [02:15<02:01,  1.35it/s]#015                                                 #015#015 44%|████▍     | 130/294 [02:15<02:01,  1.35it/s]#015 45%|████▍     | 131/294 [02:16<02:03,  1.32it/s]#015 45%|████▍     | 132/294 [02:16<02:02,  1.33it/s]#015 45%|████▌     | 133/294 [02:17<02:01,  1.33it/s]#015 46%|████▌     | 134/294 [02:18<01:59,  1.34it/s]#015 46%|████▌     | 135/294 [02:19<01:58,  1.34it/s]#015 46%|████▋     | 136/294 [02:19<01:58,  1.34it/s]#015 47%|████▋     | 137/294 [02:20<01:57,  1.34it/s]#015 47%|████▋     | 138/294 [02:21<01:56,  1.34it/s]#015 47%|████▋     | 139/294 [02:22<01:55,  1.34it/s]#015 48%|████▊     | 140/294 [02:22<01:55,  1.34it/s]#015                                                 #015#015 48%|████▊     | 140/294 [02:22<01:55,  1.34it/s]#015 48%|████▊     | 141/294 [02:23<01:54,  1.34it/s]#015 48%|████▊     | 142/294 [02:24<01:53,  1.34it/s]#015 49%|████▊     | 143/294 [02:24<01:53,  1.33it/s]#015 49%|████▉     | 144/294 [02:25<01:53,  1.33it/s]#015 49%|████▉     | 145/294 [02:26<01:54,  1.30it/s]#015 50%|████▉     | 146/294 [02:27<01:53,  1.31it/s]#015 50%|█████     | 147/294 [02:28<01:51,  1.32it/s]#015 50%|█████     | 148/294 [02:28<01:50,  1.32it/s]#015 51%|█████     | 149/294 [02:29<01:49,  1.32it/s]#015 51%|█████     | 150/294 [02:30<01:49,  1.32it/s]#015                                                 #015#015 51%|█████     | 150/294 [02:30<01:49,  1.32it/s]#015 51%|█████▏    | 151/294 [02:31<01:47,  1.33it/s]#015 52%|█████▏    | 152/294 [02:31<01:46,  1.34it/s]#015 52%|█████▏    | 153/294 [02:32<01:45,  1.34it/s]#015 52%|█████▏    | 154/294 [02:33<01:43,  1.35it/s]#015 53%|█████▎    | 155/294 [02:34<01:43,  1.35it/s]#015 53%|█████▎    | 156/294 [02:34<01:42,  1.35it/s]#015 53%|█████▎    | 157/294 [02:35<01:41,  1.35it/s]#015 54%|█████▎    | 158/294 [02:36<01:41,  1.34it/s]#015 54%|█████▍    | 159/294 [02:36<01:40,  1.35it/s]#015 54%|█████▍    | 160/294 [02:37<01:39,  1.35it/s]#015                                                 #015#015 54%|█████▍    | 160/294 [02:37<01:39,  1.35it/s]#015 55%|█████▍    | 161/294 [02:38<01:39,  1.34it/s]#015 55%|█████▌    | 162/294 [02:39<01:38,  1.35it/s]#015 55%|█████▌    | 163/294 [02:39<01:36,  1.35it/s]#015 56%|█████▌    | 164/294 [02:40<01:36,  1.35it/s]#015 56%|█████▌    | 165/294 [02:41<01:35,  1.35it/s]#015 56%|█████▋    | 166/294 [02:42<01:34,  1.35it/s]#015 57%|█████▋    | 167/294 [02:42<01:33,  1.35it/s]#015 57%|█████▋    | 168/294 [02:43<01:32,  1.36it/s]#015 57%|█████▋    | 169/294 [02:44<01:32,  1.35it/s]#015 58%|█████▊    | 170/294 [02:45<01:31,  1.35it/s]#015                                                 #015#015 58%|█████▊    | 170/294 [02:45<01:31,  1.35it/s]#015 58%|█████▊    | 171/294 [02:45<01:30,  1.35it/s]#015 59%|█████▊    | 172/294 [02:46<01:30,  1.35it/s]#015 59%|█████▉    | 173/294 [02:47<01:29,  1.35it/s]#015 59%|█████▉    | 174/294 [02:48<01:28,  1.35it/s]#015 60%|█████▉    | 175/294 [02:48<01:29,  1.33it/s]#015 60%|█████▉    | 176/294 [02:49<01:28,  1.34it/s]#015 60%|██████    | 177/294 [02:50<01:27,  1.34it/s]#015 61%|██████    | 178/294 [02:51<01:26,  1.34it/s]#015 61%|██████    | 179/294 [02:51<01:26,  1.33it/s]#015 61%|██████    | 180/294 [02:52<01:25,  1.33it/s]#015                                                 #015#015 61%|██████    | 180/294 [02:52<01:25,  1.33it/s]#015 62%|██████▏   | 181/294 [02:53<01:24,  1.33it/s]#015 62%|██████▏   | 182/294 [02:54<01:23,  1.33it/s]#015 62%|██████▏   | 183/294 [02:54<01:23,  1.34it/s]#015 63%|██████▎   | 184/294 [02:55<01:22,  1.34it/s]#015 63%|██████▎   | 185/294 [02:56<01:21,  1.34it/s]#015 63%|██████▎   | 186/294 [02:57<01:20,  1.34it/s]#015 64%|██████▎   | 187/294 [02:57<01:20,  1.34it/s]#015 64%|██████▍   | 188/294 [02:58<01:19,  1.34it/s]#015 64%|██████▍   | 189/294 [02:59<01:20,  1.31it/s]#015 65%|██████▍   | 190/294 [03:00<01:19,  1.31it/s]#015                                                 #015#015 65%|██████▍   | 190/294 [03:00<01:19,  1.31it/s]#015 65%|██████▍   | 191/294 [03:00<01:18,  1.31it/s]#015 65%|██████▌   | 192/294 [03:01<01:17,  1.32it/s]#015 66%|██████▌   | 193/294 [03:02<01:16,  1.33it/s]#015 66%|██████▌   | 194/294 [03:03<01:15,  1.33it/s]#015 66%|██████▋   | 195/294 [03:03<01:14,  1.33it/s]#015 67%|██████▋   | 196/294 [03:04<01:08,  1.44it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/49 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  4%|▍         | 2/49 [00:00<00:09,  4.97it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  6%|▌         | 3/49 [00:00<00:12,  3.83it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  8%|▊         | 4/49 [00:01<00:13,  3.30it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 10%|█         | 5/49 [00:01<00:14,  3.00it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 12%|█▏        | 6/49 [00:02<00:15,  2.83it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 14%|█▍        | 7/49 [00:02<00:15,  2.72it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 16%|█▋        | 8/49 [00:02<00:16,  2.52it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 18%|█▊        | 9/49 [00:03<00:15,  2.51it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 20%|██        | 10/49 [00:03<00:15,  2.50it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 22%|██▏       | 11/49 [00:04<00:15,  2.50it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 24%|██▍       | 12/49 [00:04<00:14,  2.49it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 27%|██▋       | 13/49 [00:04<00:14,  2.49it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 29%|██▊       | 14/49 [00:05<00:14,  2.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 31%|███       | 15/49 [00:05<00:13,  2.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 33%|███▎      | 16/49 [00:06<00:13,  2.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 35%|███▍      | 17/49 [00:06<00:12,  2.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 37%|███▋      | 18/49 [00:06<00:12,  2.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 39%|███▉      | 19/49 [00:07<00:12,  2.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 41%|████      | 20/49 [00:07<00:11,  2.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 43%|████▎     | 21/49 [00:08<00:11,  2.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 45%|████▍     | 22/49 [00:08<00:10,  2.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 47%|████▋     | 23/49 [00:08<00:10,  2.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 49%|████▉     | 24/49 [00:09<00:10,  2.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 51%|█████     | 25/49 [00:09<00:09,  2.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 53%|█████▎    | 26/49 [00:10<00:09,  2.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 55%|█████▌    | 27/49 [00:10<00:08,  2.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 57%|█████▋    | 28/49 [00:10<00:08,  2.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 59%|█████▉    | 29/49 [00:11<00:08,  2.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 61%|██████    | 30/49 [00:11<00:08,  2.37it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 63%|██████▎   | 31/49 [00:12<00:07,  2.40it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 65%|██████▌   | 32/49 [00:12<00:07,  2.42it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 67%|██████▋   | 33/49 [00:13<00:06,  2.43it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 69%|██████▉   | 34/49 [00:13<00:06,  2.43it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 71%|███████▏  | 35/49 [00:13<00:05,  2.45it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 73%|███████▎  | 36/49 [00:14<00:05,  2.45it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 76%|███████▌  | 37/49 [00:14<00:04,  2.43it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 78%|███████▊  | 38/49 [00:15<00:04,  2.40it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 80%|███████▉  | 39/49 [00:15<00:04,  2.37it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 82%|████████▏ | 40/49 [00:15<00:03,  2.39it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 84%|████████▎ | 41/49 [00:16<00:03,  2.41it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 86%|████████▌ | 42/49 [00:16<00:02,  2.42it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 88%|████████▊ | 43/49 [00:17<00:02,  2.37it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 90%|████████▉ | 44/49 [00:17<00:02,  2.34it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 92%|█████████▏| 45/49 [00:18<00:01,  2.37it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 94%|█████████▍| 46/49 [00:18<00:01,  2.36it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 96%|█████████▌| 47/49 [00:18<00:00,  2.36it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 98%|█████████▊| 48/49 [00:19<00:00,  2.35it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 49/49 [00:19<00:00,  2.41it/s]#033[A#015                                                 #015\u001b[0m\n",
      "\u001b[34m#015                                               #015#033[A#015 67%|██████▋   | 196/294 [03:24<01:08,  1.44it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 49/49 [00:19<00:00,  2.41it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015                                               #033[A/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m#015 67%|██████▋   | 197/294 [03:25<11:03,  6.84s/it]#015 67%|██████▋   | 198/294 [03:26<08:00,  5.01s/it]#015 68%|██████▊   | 199/294 [03:27<05:54,  3.73s/it]#015 68%|██████▊   | 200/294 [03:27<04:26,  2.83s/it]#015                                                 #015#015 68%|██████▊   | 200/294 [03:27<04:26,  2.83s/it]#015 68%|██████▊   | 201/294 [03:28<03:26,  2.22s/it]#015 69%|██████▊   | 202/294 [03:29<02:43,  1.78s/it]#015 69%|██████▉   | 203/294 [03:30<02:14,  1.47s/it]#015 69%|██████▉   | 204/294 [03:30<01:53,  1.26s/it]#015 70%|██████▉   | 205/294 [03:31<01:38,  1.11s/it]#015 70%|███████   | 206/294 [03:32<01:27,  1.00it/s]#015 70%|███████   | 207/294 [03:33<01:20,  1.08it/s]#015 71%|███████   | 208/294 [03:33<01:14,  1.15it/s]#015 71%|███████   | 209/294 [03:34<01:10,  1.20it/s]#015 71%|███████▏  | 210/294 [03:35<01:08,  1.23it/s]#015                                                 #015#015 71%|███████▏  | 210/294 [03:35<01:08,  1.23it/s]#015 72%|███████▏  | 211/294 [03:36<01:05,  1.27it/s]#015 72%|███████▏  | 212/294 [03:36<01:03,  1.29it/s]#015 72%|███████▏  | 213/294 [03:37<01:01,  1.31it/s]#015 73%|███████▎  | 214/294 [03:38<01:00,  1.32it/s]#015 73%|███████▎  | 215/294 [03:39<01:00,  1.30it/s]#015 73%|███████▎  | 216/294 [03:39<00:59,  1.31it/s]#015 74%|███████▍  | 217/294 [03:40<00:58,  1.32it/s]#015 74%|███████▍  | 218/294 [03:41<00:57,  1.33it/s]#015 74%|███████▍  | 219/294 [03:42<00:56,  1.34it/s]#015 75%|███████▍  | 220/294 [03:42<00:55,  1.34it/s]#015                                                 #015#015 75%|███████▍  | 220/294 [03:42<00:55,  1.34it/s]#015 75%|███████▌  | 221/294 [03:43<00:54,  1.34it/s]#015 76%|███████▌  | 222/294 [03:44<00:53,  1.34it/s]#015 76%|███████▌  | 223/294 [03:45<00:52,  1.34it/s]#015 76%|███████▌  | 224/294 [03:45<00:52,  1.34it/s]#015 77%|███████▋  | 225/294 [03:46<00:51,  1.35it/s]#015 77%|███████▋  | 226/294 [03:47<00:50,  1.35it/s]#015 77%|███████▋  | 227/294 [03:48<00:49,  1.34it/s]#015 78%|███████▊  | 228/294 [03:48<00:50,  1.31it/s]#015 78%|███████▊  | 229/294 [03:49<00:49,  1.32it/s]#015 78%|███████▊  | 230/294 [03:50<00:48,  1.32it/s]#015                                                 #015#015 78%|███████▊  | 230/294 [03:50<00:48,  1.32it/s]#015 79%|███████▊  | 231/294 [03:51<00:47,  1.32it/s]#015 79%|███████▉  | 232/294 [03:51<00:46,  1.33it/s]#015 79%|███████▉  | 233/294 [03:52<00:45,  1.33it/s]#015 80%|███████▉  | 234/294 [03:53<00:44,  1.34it/s]#015 80%|███████▉  | 235/294 [03:54<00:44,  1.34it/s]#015 80%|████████  | 236/294 [03:54<00:43,  1.34it/s]#015 81%|████████  | 237/294 [03:55<00:42,  1.34it/s]#015 81%|████████  | 238/294 [03:56<00:41,  1.34it/s]#015 81%|████████▏ | 239/294 [03:57<00:40,  1.34it/s]#015 82%|████████▏ | 240/294 [03:57<00:40,  1.34it/s]#015                                                 #015#015 82%|████████▏ | 240/294 [03:57<00:40,  1.34it/s]#015 82%|████████▏ | 241/294 [03:58<00:40,  1.31it/s]#015 82%|████████▏ | 242/294 [03:59<00:39,  1.31it/s]#015 83%|████████▎ | 243/294 [04:00<00:38,  1.32it/s]#015 83%|████████▎ | 244/294 [04:00<00:37,  1.32it/s]#015 83%|████████▎ | 245/294 [04:01<00:36,  1.33it/s]#015 84%|████████▎ | 246/294 [04:02<00:36,  1.33it/s]#015 84%|████████▍ | 247/294 [04:03<00:35,  1.33it/s]#015 84%|████████▍ | 248/294 [04:03<00:34,  1.33it/s]#015 85%|████████▍ | 249/294 [04:04<00:33,  1.33it/s]#015 85%|████████▌ | 250/294 [04:05<00:32,  1.34it/s]#015                                                 #015#015 85%|████████▌ | 250/294 [04:05<00:32,  1.34it/s]#015 85%|████████▌ | 251/294 [04:06<00:32,  1.33it/s]#015 86%|████████▌ | 252/294 [04:06<00:31,  1.34it/s]#015 86%|████████▌ | 253/294 [04:07<00:30,  1.34it/s]#015 86%|████████▋ | 254/294 [04:08<00:30,  1.30it/s]#015 87%|████████▋ | 255/294 [04:09<00:29,  1.31it/s]#015 87%|████████▋ | 256/294 [04:09<00:28,  1.32it/s]#015 87%|████████▋ | 257/294 [04:10<00:27,  1.33it/s]#015 88%|████████▊ | 258/294 [04:11<00:27,  1.33it/s]#015 88%|████████▊ | 259/294 [04:12<00:26,  1.34it/s]#015 88%|████████▊ | 260/294 [04:12<00:25,  1.34it/s]#015                                                 #015#015 88%|████████▊ | 260/294 [04:12<00:25,  1.34it/s]#015 89%|████████▉ | 261/294 [04:13<00:24,  1.34it/s]#015 89%|████████▉ | 262/294 [04:14<00:23,  1.34it/s]#015 89%|████████▉ | 263/294 [04:15<00:23,  1.35it/s]#015 90%|████████▉ | 264/294 [04:15<00:22,  1.35it/s]#015 90%|█████████ | 265/294 [04:16<00:21,  1.35it/s]#015 90%|█████████ | 266/294 [04:17<00:20,  1.35it/s]#015 91%|█████████ | 267/294 [04:18<00:20,  1.31it/s]#015 91%|█████████ | 268/294 [04:18<00:19,  1.32it/s]#015 91%|█████████▏| 269/294 [04:19<00:18,  1.33it/s]#015 92%|█████████▏| 270/294 [04:20<00:17,  1.33it/s]#015                                                 #015#015 92%|█████████▏| 270/294 [04:20<00:17,  1.33it/s]#015 92%|█████████▏| 271/294 [04:21<00:17,  1.34it/s]#015 93%|█████████▎| 272/294 [04:21<00:16,  1.34it/s]#015 93%|█████████▎| 273/294 [04:22<00:15,  1.34it/s]#015 93%|█████████▎| 274/294 [04:23<00:14,  1.34it/s]#015 94%|█████████▎| 275/294 [04:24<00:14,  1.33it/s]#015 94%|█████████▍| 276/294 [04:24<00:13,  1.33it/s]#015 94%|█████████▍| 277/294 [04:25<00:12,  1.32it/s]#015 95%|█████████▍| 278/294 [04:26<00:12,  1.32it/s]#015 95%|█████████▍| 279/294 [04:27<00:11,  1.33it/s]#015 95%|█████████▌| 280/294 [04:27<00:10,  1.31it/s]#015                                                 #015#015 95%|█████████▌| 280/294 [04:27<00:10,  1.31it/s]#015 96%|█████████▌| 281/294 [04:28<00:09,  1.32it/s]#015 96%|█████████▌| 282/294 [04:29<00:09,  1.33it/s]#015 96%|█████████▋| 283/294 [04:30<00:08,  1.33it/s]#015 97%|█████████▋| 284/294 [04:30<00:07,  1.34it/s]#015 97%|█████████▋| 285/294 [04:31<00:06,  1.33it/s]#015 97%|█████████▋| 286/294 [04:32<00:06,  1.33it/s]#015 98%|█████████▊| 287/294 [04:33<00:05,  1.34it/s]#015 98%|█████████▊| 288/294 [04:33<00:04,  1.34it/s]#015 98%|█████████▊| 289/294 [04:34<00:03,  1.34it/s]#015 99%|█████████▊| 290/294 [04:35<00:02,  1.34it/s]#015                                                 #015#015 99%|█████████▊| 290/294 [04:35<00:02,  1.34it/s]#015 99%|█████████▉| 291/294 [04:36<00:02,  1.34it/s]#015 99%|█████████▉| 292/294 [04:36<00:01,  1.34it/s]#015100%|█████████▉| 293/294 [04:37<00:00,  1.30it/s]#015100%|██████████| 294/294 [04:38<00:00,  1.41it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/49 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  4%|▍         | 2/49 [00:00<00:09,  4.91it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  6%|▌         | 3/49 [00:00<00:12,  3.77it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  8%|▊         | 4/49 [00:01<00:13,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 10%|█         | 5/49 [00:01<00:14,  2.95it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 12%|█▏        | 6/49 [00:02<00:15,  2.78it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 14%|█▍        | 7/49 [00:02<00:15,  2.65it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 16%|█▋        | 8/49 [00:02<00:15,  2.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 18%|█▊        | 9/49 [00:03<00:15,  2.53it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 20%|██        | 10/49 [00:03<00:15,  2.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 22%|██▏       | 11/49 [00:04<00:16,  2.35it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 24%|██▍       | 12/49 [00:04<00:15,  2.36it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 27%|██▋       | 13/49 [00:05<00:15,  2.37it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 29%|██▊       | 14/49 [00:05<00:14,  2.39it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 31%|███       | 15/49 [00:05<00:14,  2.39it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 33%|███▎      | 16/49 [00:06<00:13,  2.41it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 35%|███▍      | 17/49 [00:06<00:13,  2.44it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 37%|███▋      | 18/49 [00:07<00:12,  2.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 39%|███▉      | 19/49 [00:07<00:12,  2.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 41%|████      | 20/49 [00:07<00:11,  2.49it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 43%|████▎     | 21/49 [00:08<00:11,  2.49it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 45%|████▍     | 22/49 [00:08<00:11,  2.39it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 47%|████▋     | 23/49 [00:09<00:10,  2.42it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 49%|████▉     | 24/49 [00:09<00:10,  2.45it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 51%|█████     | 25/49 [00:09<00:09,  2.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 53%|█████▎    | 26/49 [00:10<00:09,  2.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 55%|█████▌    | 27/49 [00:10<00:08,  2.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 57%|█████▋    | 28/49 [00:11<00:08,  2.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 59%|█████▉    | 29/49 [00:11<00:08,  2.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 61%|██████    | 30/49 [00:11<00:07,  2.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 63%|██████▎   | 31/49 [00:12<00:07,  2.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 65%|██████▌   | 32/49 [00:12<00:06,  2.49it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 67%|██████▋   | 33/49 [00:13<00:06,  2.38it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 69%|██████▉   | 34/49 [00:13<00:06,  2.41it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 71%|███████▏  | 35/49 [00:13<00:05,  2.44it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 73%|███████▎  | 36/49 [00:14<00:05,  2.45it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 76%|███████▌  | 37/49 [00:14<00:04,  2.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 78%|███████▊  | 38/49 [00:15<00:04,  2.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 80%|███████▉  | 39/49 [00:15<00:04,  2.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 82%|████████▏ | 40/49 [00:16<00:03,  2.43it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 84%|████████▎ | 41/49 [00:16<00:03,  2.42it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 86%|████████▌ | 42/49 [00:16<00:02,  2.42it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 88%|████████▊ | 43/49 [00:17<00:02,  2.40it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 90%|████████▉ | 44/49 [00:17<00:02,  2.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 92%|█████████▏| 45/49 [00:18<00:01,  2.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 94%|█████████▍| 46/49 [00:18<00:01,  2.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 96%|█████████▌| 47/49 [00:19<00:00,  2.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 98%|█████████▊| 48/49 [00:19<00:00,  2.34it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 49/49 [00:19<00:00,  2.44it/s]#033[A#015                                                 #015\u001b[0m\n",
      "\u001b[34m#015                                               #015#033[A#015100%|██████████| 294/294 [04:58<00:00,  1.41it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 49/49 [00:20<00:00,  2.44it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015                                               #033[A#015                                                 #015#015100%|██████████| 294/294 [04:58<00:00,  1.41it/s]#015100%|██████████| 294/294 [04:58<00:00,  1.02s/it]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/49 [00:00<?, ?it/s]#015  4%|▍         | 2/49 [00:00<00:09,  4.90it/s]#015  6%|▌         | 3/49 [00:00<00:12,  3.78it/s]#015  8%|▊         | 4/49 [00:01<00:13,  3.26it/s]#015 10%|█         | 5/49 [00:01<00:14,  2.97it/s]#015 12%|█▏        | 6/49 [00:02<00:15,  2.79it/s]#015 14%|█▍        | 7/49 [00:02<00:15,  2.68it/s]#015 16%|█▋        | 8/49 [00:02<00:15,  2.60it/s]#015 18%|█▊        | 9/49 [00:03<00:15,  2.55it/s]#015 20%|██        | 10/49 [00:03<00:15,  2.51it/s]#015 22%|██▏       | 11/49 [00:04<00:16,  2.32it/s]#015 24%|██▍       | 12/49 [00:04<00:15,  2.35it/s]#015 27%|██▋       | 13/49 [00:05<00:15,  2.38it/s]#015 29%|██▊       | 14/49 [00:05<00:14,  2.37it/s]#015 31%|███       | 15/49 [00:05<00:14,  2.39it/s]#015 33%|███▎      | 16/49 [00:06<00:13,  2.37it/s]#015 35%|███▍      | 17/49 [00:06<00:13,  2.39it/s]#015 37%|███▋      | 18/49 [00:07<00:12,  2.41it/s]#015 39%|███▉      | 19/49 [00:07<00:12,  2.41it/s]#015 41%|████      | 20/49 [00:07<00:11,  2.42it/s]#015 43%|████▎     | 21/49 [00:08<00:11,  2.42it/s]#015 45%|████▍     | 22/49 [00:08<00:11,  2.42it/s]#015 47%|████▋     | 23/49 [00:09<00:11,  2.27it/s]#015 49%|████▉     | 24/49 [00:09<00:10,  2.31it/s]#015 51%|█████     | 25/49 [00:10<00:10,  2.33it/s]#015 53%|█████▎    | 26/49 [00:10<00:09,  2.34it/s]#015 55%|█████▌    | 27/49 [00:10<00:09,  2.35it/s]#015 57%|█████▋    | 28/49 [00:11<00:08,  2.33it/s]#015 59%|█████▉    | 29/49 [00:11<00:08,  2.36it/s]#015 61%|██████    | 30/49 [00:12<00:08,  2.36it/s]#015 63%|██████▎   | 31/49 [00:12<00:07,  2.34it/s]#015 65%|██████▌   | 32/49 [00:13<00:07,  2.36it/s]#015 67%|██████▋   | 33/49 [00:13<00:06,  2.34it/s]#015 69%|██████▉   | 34/49 [00:14<00:06,  2.16it/s]#015 71%|███████▏  | 35/49 [00:14<00:06,  2.22it/s]#015 73%|███████▎  | 36/49 [00:14<00:05,  2.25it/s]#015 76%|███████▌  | 37/49 [00:15<00:05,  2.29it/s]#015 78%|███████▊  | 38/49 [00:15<00:04,  2.32it/s]#015 80%|███████▉  | 39/49 [00:16<00:04,  2.32it/s]#015 82%|████████▏ | 40/49 [00:16<00:03,  2.34it/s]#015 84%|████████▎ | 41/49 [00:16<00:03,  2.33it/s]#015 86%|████████▌ | 42/49 [00:17<00:03,  2.32it/s]#015 88%|████████▊ | 43/49 [00:17<00:02,  2.30it/s]#015 90%|████████▉ | 44/49 [00:18<00:02,  2.28it/s]#015 92%|█████████▏| 45/49 [00:18<00:01,  2.10it/s]#015 94%|█████████▍| 46/49 [00:19<00:01,  2.17it/s]#015 96%|█████████▌| 47/49 [00:19<00:00,  2.18it/s]#015 98%|█████████▊| 48/49 [00:20<00:00,  2.23it/s]#015100%|██████████| 49/49 [00:20<00:00,  2.33it/s]#015100%|██████████| 49/49 [00:20<00:00,  2.36it/s]\n",
      "\u001b[0m\n",
      "\n",
      "2021-05-13 21:50:10 Uploading - Uploading generated training model\n",
      "2021-05-13 21:51:09 Completed - Training job completed\n",
      "ProfilerReport-1620941862: NoIssuesFound\n",
      "Training seconds: 574\n",
      "Billable seconds: 574\n",
      "CPU times: user 1.71 s, sys: 170 ms, total: 1.88 s\n",
      "Wall time: 13min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "inputs = {\n",
    "    \"train\": training_input_path, \n",
    "    \"test\": test_input_path\n",
    "}\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How'd It Go?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import TrainingJobAnalytics\n",
    "df = TrainingJobAnalytics(training_job_name = estimator.latest_training_job.name).dataframe()\n",
    "df = df[[\"metric_name\", \"value\"]]\n",
    "\n",
    "summary = df.groupby(\"metric_name\").describe()\n",
    "summary.columns = summary.columns.droplevel(0)\n",
    "summary = summary.reset_index().rename(columns = { \n",
    "    \"metric_name\": \"Metric\",\n",
    "    \"min\": \"Min\", \n",
    "    \"max\": \"Max\", \n",
    "    \"mean\": \"Average\" \n",
    "}).set_index(\"Metric\")\n",
    "summary = summary.drop([\"std\", \"count\", \"25%\", \"50%\", \"75%\"], axis = 1).drop([\"epoch\"])\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Model for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "class SentimentAnalysis(Predictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super().__init__(\n",
    "            endpoint_name, \n",
    "            sagemaker_session = sagemaker_session, \n",
    "            serializer = JSONSerializer(), \n",
    "            deserializer = JSONDeserializer()\n",
    "        )\n",
    "\n",
    "name = name_from_base(job_name)\n",
    "\n",
    "model = PyTorchModel(\n",
    "    name = name,\n",
    "    role = role, \n",
    "    model_data = estimator.model_data,\n",
    "    source_dir = \"./scripts\",\n",
    "    entry_point = \"predictor.py\",\n",
    "    framework_version = \"1.6.0\",\n",
    "    py_version = \"py36\",\n",
    "    predictor_cls = SentimentAnalysis\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = \"ml.m5.large\",\n",
    "    endpoint_name = name,\n",
    "    wait = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Inferences Using a SageMaker Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "inputs = [\n",
    "    \"Willow is the greatest movie that ever lived.\",\n",
    "    \"The Notebook is ironically depressing.\",\n",
    "    \"It's annoying that I had to Google the capitalization of \\\"Back to the Future\\\", but it is a gem of nostalgic wonder.\",\n",
    "    \"Yikes! Weird Science did not age well for 2021.\"\n",
    "]\n",
    "\n",
    "for it in inputs:\n",
    "    prediction = predictor.predict({\"text\": it})\n",
    "    print(f'    {prediction}: {it}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    predictor.delete_endpoint()\n",
    "    model.delete_model()\n",
    "except:\n",
    "    display(\"Already deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.6-cpu-py36-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
