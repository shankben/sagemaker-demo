{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Sentiment Classifier\n",
    "### Using Hugging Face with the SageMaker SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What We're Going To Do:\n",
    "\n",
    "#### Installation\n",
    "1. Install the SageMaker SDK and the Hugging Face libraries\n",
    "1. Start a SageMaker session, including the default IAM role and S3 bucket\n",
    "    \n",
    "#### Data Preparation\n",
    "1. Tokenization: Download and prepare our IMDB dataset for NLP model training\n",
    "1. Upload our tokenized and split dataset to S3\n",
    "\n",
    "#### Model Training\n",
    "1. Setup an Estimator\n",
    "1. Train a model\n",
    "\n",
    "#### Realtime Inference\n",
    "1. Prepare the model for deployment\n",
    "1. Deploy the model and create a Predictor\n",
    "1. Make inferences using a Predictor\n",
    "\n",
    "#### Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import os\n",
    "\n",
    "TRANSFORMERS_VERSION = \"4.5.0\"\n",
    "DATASETS_VERSION = \"1.6.2\"\n",
    "\n",
    "requirements_txt = \"\"\"numpy\n",
    "pandas\n",
    "transformers=={0}\n",
    "datasets=={1}\n",
    "\"\"\".format(TRANSFORMERS_VERSION, DATASETS_VERSION)\n",
    "\n",
    "with open(os.path.join(os.getcwd(), \"scripts\", \"requirements.txt\"), \"w\") as f:\n",
    "    f.write(requirements_txt)\n",
    "\n",
    "!pip install --upgrade \"sagemaker>=2.31.0\" \"transformers==$TRANSFORMERS_VERSION\" \"datasets[s3]==$DATASETS_VERSION\"\n",
    "!conda install -c conda-forge ipywidgets -y\n",
    "\n",
    "# import IPython\n",
    "# IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "import sagemaker.huggingface\n",
    "\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker role arn: arn:aws:iam::934284400219:role/service-role/AmazonSageMaker-ExecutionRole-20210510T080103\n",
      "SageMaker bucket: sagemaker-us-east-1-934284400219\n",
      "SageMaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "print(f\"SageMaker role arn: {role}\")\n",
    "print(f\"SageMaker bucket: {session.default_bucket()}\")\n",
    "print(f\"SageMaker session region: {session.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Split the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pandas\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from sagemaker_demo_helper import SageMakerDemoHelper\n",
    "\n",
    "datasets.logging.set_verbosity_error()\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_dataset, test_dataset = datasets.load_dataset(\n",
    "    \"imdb\", \n",
    "    ignore_verifications = True,\n",
    "    split = [\"train\", \"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.logging.set_verbosity_error()\n",
    "\n",
    "tokenize = lambda batch: tokenizer(batch[\"text\"], padding = \"max_length\", truncation = True)\n",
    "# test_ds = test_dataset.shuffle().select(range(10000))\n",
    "\n",
    "train_ds = train_dataset.shuffle().map(tokenize)\n",
    "test_ds = test_dataset.shuffle().map(tokenize)\n",
    "\n",
    "try:\n",
    "    train_ds = train_ds.rename_column(\"label\", \"labels\")\n",
    "    test_ds = test_ds.rename_column(\"label\", \"labels\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "columns = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "train_ds.set_format(\"torch\", columns = columns)\n",
    "test_ds.set_format(\"torch\", columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>(tensor(101), tensor(1045), tensor(2245), tens...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>(tensor(101), tensor(1045), tensor(2253), tens...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>(tensor(101), tensor(1045), tensor(4771), tens...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>(tensor(101), tensor(10386), tensor(19558), te...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>(tensor(101), tensor(1996), tensor(2364), tens...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      attention_mask  \\\n",
       "0  (tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "1  (tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "2  (tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "3  (tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "4  (tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "\n",
       "                                           input_ids  labels  \n",
       "0  (tensor(101), tensor(1045), tensor(2245), tens...       0  \n",
       "1  (tensor(101), tensor(1045), tensor(2253), tens...       0  \n",
       "2  (tensor(101), tensor(1045), tensor(4771), tens...       0  \n",
       "3  (tensor(101), tensor(10386), tensor(19558), te...       0  \n",
       "4  (tensor(101), tensor(1996), tensor(2364), tens...       0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas.DataFrame(train_ds[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the Dataset to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "s3_prefix = \"datasets/imdb-binary-classification\"\n",
    "training_input_path = f\"s3://{bucket}/{s3_prefix}/train\"\n",
    "test_input_path = f\"s3://{bucket}/{s3_prefix}/test\"\n",
    "\n",
    "train_ds.save_to_disk(training_input_path, fs = s3)\n",
    "test_ds.save_to_disk(test_input_path, fs = s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup an Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "job_name = \"imdb-huggingface\"\n",
    "\n",
    "metric_definitions = [\n",
    "    { \"Name\": \"loss\", \"Regex\": \"'loss': ([0-9]+(.|e\\-)[0-9]+),?\" },\n",
    "    { \"Name\": \"learning_rate\", \"Regex\": \"'learning_rate': ([0-9]+(.|e\\-)[0-9]+),?\" },\n",
    "    { \"Name\": \"eval_loss\", \"Regex\": \"'eval_loss': ([0-9]+(.|e\\-)[0-9]+),?\" },\n",
    "    { \"Name\": \"eval_accuracy\", \"Regex\": \"'eval_accuracy': ([0-9]+(.|e\\-)[0-9]+),?\" },\n",
    "    { \"Name\": \"eval_f1\", \"Regex\": \"'eval_f1': ([0-9]+(.|e\\-)[0-9]+),?\" },\n",
    "    { \"Name\": \"eval_precision\", \"Regex\": \"'eval_precision': ([0-9]+(.|e\\-)[0-9]+),?\" },\n",
    "    { \"Name\": \"eval_recall\", \"Regex\": \"'eval_recall': ([0-9]+(.|e\\-)[0-9]+),?\" },\n",
    "    { \"Name\": \"eval_runtime\", \"Regex\": \"'eval_runtime': ([0-9]+(.|e\\-)[0-9]+),?\" },\n",
    "    { \"Name\": \"eval_samples_per_second\", \"Regex\": \"'eval_samples_per_second': ([0-9]+(.|e\\-)[0-9]+),?\" },\n",
    "    { \"Name\": \"epoch\", \"Regex\": \"'epoch': ([0-9]+(.|e\\-)[0-9]+),?\" }\n",
    "]\n",
    "\n",
    "estimator = HuggingFace(\n",
    "    base_job_name = job_name,\n",
    "    role = role,\n",
    "    py_version = \"py36\",\n",
    "    pytorch_version = \"1.6.0\",\n",
    "    transformers_version = TRANSFORMERS_VERSION,\n",
    "    entry_point = \"trainer.py\",\n",
    "    instance_count = 1,\n",
    "    instance_type = \"ml.p3.16xlarge\",\n",
    "    source_dir = \"./scripts\",\n",
    "    enable_sagemaker_metrics = True,\n",
    "    metric_definitions = metric_definitions,\n",
    "    hyperparameters = {\n",
    "        \"epochs\": 3,\n",
    "        \"eval_batch_size\": 128,\n",
    "        \"model_name\": model_name,\n",
    "        \"train_batch_size\": 64\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Model using the Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-11 14:25:05 Starting - Starting the training job...\n",
      "2021-05-11 14:25:33 Starting - Launching requested ML instancesProfilerReport-1620743105: InProgress\n",
      "............\n",
      "2021-05-11 14:27:34 Starting - Preparing the instances for training.........\n",
      "2021-05-11 14:29:05 Downloading - Downloading input data\n",
      "2021-05-11 14:29:05 Training - Downloading the training image........"
     ]
    }
   ],
   "source": [
    "inputs = { \n",
    "    \"train\": training_input_path, \n",
    "    \"test\": test_input_path\n",
    "}\n",
    "estimator.fit(inputs, wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sagemaker import TrainingJobAnalytics\n",
    "# df = TrainingJobAnalytics(training_job_name = estimator.latest_training_job.name).dataframe()\n",
    "# display(df[[\"metric_name\", \"value\"]].groupby(\"metric_name\").max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Model for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "class SentimentAnalysis(Predictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super().__init__(\n",
    "            endpoint_name, \n",
    "            sagemaker_session = sagemaker_session, \n",
    "            serializer = JSONSerializer(), \n",
    "            deserializer = JSONDeserializer()\n",
    "        )\n",
    "\n",
    "name = name_from_base(job_name)\n",
    "\n",
    "model = PyTorchModel(\n",
    "    name = name,\n",
    "    role = role, \n",
    "    model_data = estimator.model_data,\n",
    "    source_dir = \"./scripts\",\n",
    "    entry_point = \"predictor.py\",\n",
    "    framework_version = \"1.6.0\",\n",
    "    py_version = \"py36\",\n",
    "    predictor_cls = SentimentAnalysis\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor = model.deploy(\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = \"ml.m5.large\",\n",
    "    endpoint_name = name,\n",
    "    wait = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Inferences Using a SageMaker Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "inputs = [\n",
    "    \"Willow is the greatest movie that ever lived.\",\n",
    "    \"The Notebook is ironically depressing.\",\n",
    "    \"It's annoying that I had to Google the capitalization of Back to the Future, but it is a gem of nostalgic wonder.\",\n",
    "    \"Yikes! Weird Science did not age well for 2021.\"\n",
    "]\n",
    "\n",
    "for it in inputs:\n",
    "    prediction = predictor.predict({\"text\": it})\n",
    "    print(f'    {prediction}: {it}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    predictor.delete_endpoint()\n",
    "    model.delete_model()\n",
    "except:\n",
    "    display(\"Already deleted\")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.6-cpu-py36-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
