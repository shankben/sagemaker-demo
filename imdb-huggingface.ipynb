{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Sentiment Classifier\n",
    "### Using Hugging Face with the SageMaker SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What We're Going To Do:\n",
    "\n",
    "#### Installation\n",
    "1. Install the SageMaker SDK and the Hugging Face libraries\n",
    "1. Start a SageMaker session, including the default IAM role and S3 bucket\n",
    "    \n",
    "#### Data Preparation\n",
    "1. Tokenization: Download and prepare our IMDB dataset for NLP model training\n",
    "1. Upload our tokenized and split dataset to S3\n",
    "\n",
    "#### Model Training\n",
    "1. Setup an Estimator\n",
    "1. Prepare the model for deployment\n",
    "\n",
    "#### Realtime Inference\n",
    "1. Deploy the model\n",
    "1. Make inferences with a Predictor\n",
    "\n",
    "#### Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install --upgrade \"sagemaker>=2.31.0\" \"transformers==4.4.2\" \"datasets[s3]==1.5.0\"\n",
    "!conda install -c conda-forge ipywidgets -y\n",
    "\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)\n",
    "\n",
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker role arn: arn:aws:iam::061635907654:role/service-role/AmazonSageMaker-ExecutionRole-20201221T131849\n",
      "SageMaker bucket: sagemaker-us-east-1-061635907654\n",
      "SageMaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "print(f\"SageMaker role arn: {role}\")\n",
    "print(f\"SageMaker bucket: {session.default_bucket()}\")\n",
    "print(f\"SageMaker session region: {session.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Split the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import pandas\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from sagemaker_demo_helper import SageMakerDemoHelper\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "helper = SageMakerDemoHelper.instance(bucket, role, model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_dataset, test_dataset = datasets.load_dataset(\n",
    "    \"imdb\", \n",
    "    ignore_verifications = True,\n",
    "    split = [\"train\", \"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-9849c7513e7c2228.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-2710b701f1525e3f.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57666bcbba054b3dac3ad7392b08a677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[101, 2096, 1996, 2434, 4673, 2544, 1010, 2007...</td>\n",
       "      <td>1</td>\n",
       "      <td>While the original 1932 version, with Preston ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[101, 18527, 1996, 2087, 5720, 1011, 2055, 213...</td>\n",
       "      <td>1</td>\n",
       "      <td>Ironically the most talked-about American film...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[101, 27137, 2003, 1996, 2208, 1010, 14636, 20...</td>\n",
       "      <td>0</td>\n",
       "      <td>Bingo is the game, bullshit is the name. Rarel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[101, 2045, 2024, 2061, 2116, 4436, 2004, 2000...</td>\n",
       "      <td>1</td>\n",
       "      <td>There are so many reasons as to why I rate the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[101, 1045, 2245, 3666, 6107, 6876, 2006, 5971...</td>\n",
       "      <td>0</td>\n",
       "      <td>I thought watching employment videos on corpor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      attention_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                           input_ids  labels  \\\n",
       "0  [101, 2096, 1996, 2434, 4673, 2544, 1010, 2007...       1   \n",
       "1  [101, 18527, 1996, 2087, 5720, 1011, 2055, 213...       1   \n",
       "2  [101, 27137, 2003, 1996, 2208, 1010, 14636, 20...       0   \n",
       "3  [101, 2045, 2024, 2061, 2116, 4436, 2004, 2000...       1   \n",
       "4  [101, 1045, 2245, 3666, 6107, 6876, 2006, 5971...       0   \n",
       "\n",
       "                                                text  \n",
       "0  While the original 1932 version, with Preston ...  \n",
       "1  Ironically the most talked-about American film...  \n",
       "2  Bingo is the game, bullshit is the name. Rarel...  \n",
       "3  There are so many reasons as to why I rate the...  \n",
       "4  I thought watching employment videos on corpor...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize = lambda batch: tokenizer(batch[\"text\"], padding = \"max_length\", truncation = True)\n",
    "# test_ds = test_dataset.shuffle().select(range(10000))\n",
    "\n",
    "train_ds = train_dataset.shuffle().map(tokenize)\n",
    "test_ds = test_dataset.shuffle().map(tokenize)\n",
    "\n",
    "try:\n",
    "    train_ds = train_ds.rename_column(\"label\", \"labels\")\n",
    "    test_ds = test_ds.rename_column(\"label\", \"labels\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "pandas.DataFrame(train_ds[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "train_ds.set_format(\"torch\", columns = columns)\n",
    "test_ds.set_format(\"torch\", columns = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading the Dataset to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "s3_prefix = \"datasets/imdb-binary-classification\"\n",
    "\n",
    "training_input_path = f\"s3://{bucket}/{s3_prefix}/train\"\n",
    "train_ds.save_to_disk(training_input_path, fs = s3)\n",
    "\n",
    "test_input_path = f\"s3://{bucket}/{s3_prefix}/test\"\n",
    "test_ds.save_to_disk(test_input_path, fs = s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup an Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sagemaker.huggingface.estimator.HuggingFace object at 0x7f9080198748>\n"
     ]
    }
   ],
   "source": [
    "estimator = helper.use_standard_training()\n",
    "\n",
    "inputs = { \n",
    "    \"train\": training_input_path, \n",
    "    \"test\": test_input_path\n",
    "}\n",
    "\n",
    "estimator.fit(\n",
    "    inputs,\n",
    "    wait = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Model for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'TrainingStartTime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b9c42da2b6e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msagemaker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainingJobAnalytics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainingJobAnalytics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_job_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metric_name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"metric_name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/analytics.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, training_job_name, metric_names, sagemaker_session, start_time, end_time, period)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainingJobAnalytics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/analytics.py\u001b[0m in \u001b[0;36mclear_cache\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainingJobAnalytics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_time_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_determine_timeinterval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_determine_timeinterval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/analytics.py\u001b[0m in \u001b[0;36m_determine_timeinterval\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \"\"\"\n\u001b[1;32m    346\u001b[0m         \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sage_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainingJobName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mu\"TrainingStartTime\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# datetime object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0;31m# Incrementing end time by 1 min since CloudWatch drops seconds before finding the logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;31m# This results in logs being searched in the time range in which the correct log line was\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'TrainingStartTime'"
     ]
    }
   ],
   "source": [
    "from sagemaker import TrainingJobAnalytics\n",
    "\n",
    "df = TrainingJobAnalytics(training_job_name = estimator.latest_training_job.name).dataframe()\n",
    "display(df[[\"metric_name\", \"value\"]].groupby(\"metric_name\").max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "class SentimentAnalysis(Predictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super().__init__(\n",
    "            endpoint_name, \n",
    "            sagemaker_session = sagemaker_session, \n",
    "            serializer = JSONSerializer(), \n",
    "            deserializer = JSONDeserializer()\n",
    "        )\n",
    "\n",
    "name = name_from_base(\"imdb-huggingface\")\n",
    "\n",
    "model = PyTorchModel(\n",
    "    name = name,\n",
    "    role = role, \n",
    "    model_data = estimator.model_data,\n",
    "    source_dir = \"./scripts\",\n",
    "    entry_point = \"torchserve-predictor.py\",\n",
    "    framework_version = \"1.6.0\",\n",
    "    py_version = \"py36\",\n",
    "    predictor_cls = SentimentAnalysis\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor = model.deploy(\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = \"ml.m5.large\",\n",
    "    endpoint_name = name,\n",
    "    wait = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Inferences\n",
    "## Using a SageMaker Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "inputs = [\n",
    "    \"Willow is the greatest movie that has ever lived.\",\n",
    "    \"The Notebook is ironically depressing.\",\n",
    "    \"My cat's breath smells like cat food.\"\n",
    "]\n",
    "\n",
    "prediction = predictor.predict({\"text\": it})\n",
    "print(f'    {prediction}: {it}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for i in range(len(models)):\n",
    "        predictors[i].delete_endpoint()\n",
    "        models[i].delete_model()\n",
    "except:\n",
    "    display(\"Already deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.6-cpu-py36-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
