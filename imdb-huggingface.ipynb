{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Sentiment Classifier\n",
    "### Using Hugging Face with the SageMaker SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What We're Going To Do:\n",
    "\n",
    "#### Installation\n",
    "1. Install the SageMaker SDK and the Hugging Face libraries\n",
    "1. Start a SageMaker session, including the default IAM role and S3 bucket\n",
    "    \n",
    "#### Data Preparation\n",
    "1. Tokenization: Download and prepare our IMDB dataset for NLP model training\n",
    "1. Upload our tokenized and split dataset to S3\n",
    "\n",
    "#### Model Training\n",
    "1. Setup an Estimator\n",
    "1. Prepare the model for deployment\n",
    "\n",
    "#### Realtime Inference\n",
    "1. Deploy the model\n",
    "1. Make inferences with a Predictor\n",
    "\n",
    "#### Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install --upgrade \"sagemaker>=2.31.0\" \"transformers==4.4.2\" \"datasets[s3]==1.5.0\"\n",
    "!conda install -c conda-forge ipywidgets -y\n",
    "\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)\n",
    "\n",
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker role arn: arn:aws:iam::061635907654:role/service-role/AmazonSageMaker-ExecutionRole-20201221T131849\n",
      "SageMaker bucket: sagemaker-us-east-1-061635907654\n",
      "SageMaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "print(f\"SageMaker role arn: {role}\")\n",
    "print(f\"SageMaker bucket: {session.default_bucket()}\")\n",
    "print(f\"SageMaker session region: {session.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Split the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pandas\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from sagemaker_demo_helper import SageMakerDemoHelper\n",
    "\n",
    "datasets.logging.set_verbosity_error()\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "helper = SageMakerDemoHelper.instance(bucket, role, model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_dataset, test_dataset = datasets.load_dataset(\n",
    "    \"imdb\", \n",
    "    ignore_verifications = True,\n",
    "    split = [\"train\", \"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-9849c7513e7c2228.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-2710b701f1525e3f.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c5f0dde49d4d07af675700f4075684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "datasets.logging.set_verbosity_info()\n",
    "\n",
    "tokenize = lambda batch: tokenizer(batch[\"text\"], padding = \"max_length\", truncation = True)\n",
    "# test_ds = test_dataset.shuffle().select(range(10000))\n",
    "\n",
    "train_ds = train_dataset.shuffle().map(tokenize)\n",
    "test_ds = test_dataset.shuffle().map(tokenize)\n",
    "\n",
    "try:\n",
    "    train_ds = train_ds.rename_column(\"label\", \"labels\")\n",
    "    test_ds = test_ds.rename_column(\"label\", \"labels\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "columns = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "train_ds.set_format(\"torch\", columns = columns)\n",
    "test_ds.set_format(\"torch\", columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 5696.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>(tensor(101), tensor(2096), tensor(1996), tens...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>(tensor(101), tensor(18527), tensor(1996), ten...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>(tensor(101), tensor(27137), tensor(2003), ten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>(tensor(101), tensor(2045), tensor(2024), tens...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>(tensor(101), tensor(1045), tensor(2245), tens...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      attention_mask  \\\n",
       "0  (tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "1  (tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "2  (tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "3  (tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "4  (tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "\n",
       "                                           input_ids  labels  \n",
       "0  (tensor(101), tensor(2096), tensor(1996), tens...       1  \n",
       "1  (tensor(101), tensor(18527), tensor(1996), ten...       1  \n",
       "2  (tensor(101), tensor(27137), tensor(2003), ten...       0  \n",
       "3  (tensor(101), tensor(2045), tensor(2024), tens...       1  \n",
       "4  (tensor(101), tensor(1045), tensor(2245), tens...       0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas.DataFrame(train_ds[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading the Dataset to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "s3_prefix = \"datasets/imdb-binary-classification\"\n",
    "training_input_path = f\"s3://{bucket}/{s3_prefix}/train\"\n",
    "test_input_path = f\"s3://{bucket}/{s3_prefix}/test\"\n",
    "\n",
    "train_ds.save_to_disk(training_input_path, fs = s3)\n",
    "test_ds.save_to_disk(test_input_path, fs = s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup an Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-07 16:44:07 Starting - Starting the training job...\n",
      "2021-05-07 16:44:34 Starting - Launching requested ML instancesProfilerReport-1620405846: InProgress\n",
      "............\n",
      "2021-05-07 16:46:35 Starting - Preparing the instances for training.........\n",
      "2021-05-07 16:48:08 Downloading - Downloading input data...\n",
      "2021-05-07 16:48:35 Training - Downloading the training image..............\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-05-07 16:50:54,872 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-05-07 16:50:54,951 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\n",
      "2021-05-07 16:50:56 Training - Training image download completed. Training in progress.\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2021-05-07 16:50:56,246 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2021-05-07 16:50:56,325 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2021-05-07 16:50:59,350 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2021-05-07 16:50:59,790 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.1.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: transformers==4.4.2 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (4.4.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 3)) (20.9)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 3)) (2.25.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 3)) (2021.3.17)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 3)) (3.0.12)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 3)) (0.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 3)) (4.49.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 3)) (3.5.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 3)) (0.10.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 3)) (0.0.43)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 2)) (2.8.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 2)) (2021.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas->-r requirements.txt (line 2)) (1.15.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.4.2->-r requirements.txt (line 3)) (3.7.4.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.4.2->-r requirements.txt (line 3)) (3.4.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.4.2->-r requirements.txt (line 3)) (2.4.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.4.2->-r requirements.txt (line 3)) (2020.12.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.4.2->-r requirements.txt (line 3)) (2.10)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.4.2->-r requirements.txt (line 3)) (3.0.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.4.2->-r requirements.txt (line 3)) (1.25.11)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.4.2->-r requirements.txt (line 3)) (7.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.4.2->-r requirements.txt (line 3)) (1.0.1)\u001b[0m\n",
      "\u001b[34m2021-05-07 16:51:01,176 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-05-07 16:51:01,570 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35m2021-05-07 16:51:02,238 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2021-05-07 16:51:02,239 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[35m2021-05-07 16:51:02,240 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-05-07 16:51:02,240 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.2.78.34\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers==4.4.2 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (4.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 3)) (4.49.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 3)) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 3)) (3.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 3)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 3)) (0.10.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 3)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 3)) (2021.3.17)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 3)) (0.0.43)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 3)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 2)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 2)) (2021.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas->-r requirements.txt (line 2)) (1.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.4.2->-r requirements.txt (line 3)) (3.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.4.2->-r requirements.txt (line 3)) (3.7.4.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.4.2->-r requirements.txt (line 3)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.4.2->-r requirements.txt (line 3)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.4.2->-r requirements.txt (line 3)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.4.2->-r requirements.txt (line 3)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.4.2->-r requirements.txt (line 3)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.4.2->-r requirements.txt (line 3)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.4.2->-r requirements.txt (line 3)) (7.1.2)\u001b[0m\n",
      "\u001b[35m2021-05-07 16:51:03,242 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-05-07 16:51:03,242 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.2.78.34\u001b[0m\n",
      "\u001b[34m2021-05-07 16:51:03,997 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2021-05-07 16:51:03,998 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2021-05-07 16:51:04,001 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2021-05-07 16:51:04,002 sagemaker-training-toolkit INFO     Cannot connect to host algo-2\u001b[0m\n",
      "\u001b[34m2021-05-07 16:51:04,002 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.2.82.101\u001b[0m\n",
      "\u001b[35m2021-05-07 16:51:04,251 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[35m2021-05-07 16:51:04,322 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2021-05-07 16:51:04,323 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-05-07 16:51:04,323 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2021-05-07 16:51:04,323 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[35m2021-05-07 16:51:04,328 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m2021-05-07 16:51:05,011 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[34m2021-05-07 16:51:05,082 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2021-05-07 16:51:05,082 sagemaker-training-toolkit INFO     Can connect to host algo-2\u001b[0m\n",
      "\u001b[34m2021-05-07 16:51:05,082 sagemaker-training-toolkit INFO     Worker algo-2 available for communication\u001b[0m\n",
      "\u001b[34m2021-05-07 16:51:05,082 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1', 'algo-2'] Hosts: ['algo-1:2', 'algo-2:2'] process_per_hosts: 2 num_processes: 4\u001b[0m\n",
      "\u001b[34m2021-05-07 16:51:05,084 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2021-05-07 16:51:05,163 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 2,\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": true,\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"\",\n",
      "        \"sagemaker_mpi_enabled\": true,\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"sagemaker_distributed_dataparallel_custom_mpi_options\": \"\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"eval_batch_size\": 128,\n",
      "        \"train_batch_size\": 64,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"output_dir\": \"/opt/ml/checkpoints\",\n",
      "        \"epochs\": 3\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"imdb-huggingface-2021-05-07-16-44-06-777\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-061635907654/imdb-huggingface-2021-05-07-16-44-06-777/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":3,\"eval_batch_size\":128,\"model_name\":\"distilbert-base-uncased\",\"output_dir\":\"/opt/ml/checkpoints\",\"train_batch_size\":64}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":2}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-061635907654/imdb-huggingface-2021-05-07-16-44-06-777/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":2},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"epochs\":3,\"eval_batch_size\":128,\"model_name\":\"distilbert-base-uncased\",\"output_dir\":\"/opt/ml/checkpoints\",\"train_batch_size\":64},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"imdb-huggingface-2021-05-07-16-44-06-777\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-061635907654/imdb-huggingface-2021-05-07-16-44-06-777/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"3\",\"--eval_batch_size\",\"128\",\"--model_name\",\"distilbert-base-uncased\",\"--output_dir\",\"/opt/ml/checkpoints\",\"--train_batch_size\",\"64\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_BATCH_SIZE=128\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/checkpoints\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=3\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:2,algo-2:2 -np 4 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TEST -x SM_CHANNEL_TRAIN -x SM_HP_EVAL_BATCH_SIZE -x SM_HP_TRAIN_BATCH_SIZE -x SM_HP_MODEL_NAME -x SM_HP_OUTPUT_DIR -x SM_HP_EPOCHS -x PYTHONPATH /opt/conda/bin/python3.6 -m mpi4py train.py --epochs 3 --eval_batch_size 128 --model_name distilbert-base-uncased --output_dir /opt/ml/checkpoints --train_batch_size 64\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35m2021-05-07 16:51:06,333 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=55, name='orted', status='disk-sleep', started='16:51:06')]\u001b[0m\n",
      "\u001b[35m2021-05-07 16:51:06,333 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=55, name='orted', status='disk-sleep', started='16:51:06')]\u001b[0m\n",
      "\u001b[35m2021-05-07 16:51:06,334 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=55, name='orted', status='running', started='16:51:06')]\u001b[0m\n",
      "\u001b[34m Data for JOB [41160,1] offset 0 Total slots allocated 4\n",
      "\n",
      " ========================   JOB MAP   ========================\n",
      "\n",
      " Data for node: algo-1#011Num slots: 2#011Max slots: 0#011Num procs: 2\n",
      " #011Process OMPI jobid: [41160,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [41160,1] App: 0 Process rank: 1 Bound: N/A\n",
      "\n",
      " Data for node: algo-2#011Num slots: 2#011Max slots: 0#011Num procs: 2\n",
      " #011Process OMPI jobid: [41160,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [41160,1] App: 0 Process rank: 3 Bound: N/A\n",
      "\n",
      " =============================================================\n",
      " Data for JOB [41160,1] offset 0 Total slots allocated 4\n",
      "\n",
      " ========================   JOB MAP   ========================\n",
      "\n",
      " Data for node: algo-1#011Num slots: 2#011Max slots: 0#011Num procs: 2\n",
      " #011Process OMPI jobid: [41160,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [41160,1] App: 0 Process rank: 1 Bound: N/A\n",
      "\n",
      " Data for node: algo-2#011Num slots: 2#011Max slots: 0#011Num procs: 2\n",
      " #011Process OMPI jobid: [41160,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [41160,1] App: 0 Process rank: 3 Bound: N/A\n",
      "\n",
      " =============================================================\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:2021-05-07 16:51:10,641 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:2021-05-07 16:51:10,641 - __main__ - INFO -  loaded test_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:2021-05-07 16:51:10,641 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:2021-05-07 16:51:10,641 - __main__ - INFO -  loaded test_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:2021-05-07 16:51:10,777 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:2021-05-07 16:51:10,777 - __main__ - INFO -  loaded test_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:2021-05-07 16:51:10,777 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:2021-05-07 16:51:10,777 - __main__ - INFO -  loaded test_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:2021-05-07 16:51:10,803 - filelock - INFO - Lock 139988652921296 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:2021-05-07 16:51:10,824 - filelock - INFO - Lock 139988652921296 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:2021-05-07 16:51:10,827 - filelock - INFO - Lock 140297698061112 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:2021-05-07 16:51:10,846 - filelock - INFO - Lock 139988652874720 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:2021-05-07 16:51:10,847 - filelock - INFO - Lock 140297698061112 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:2021-05-07 16:51:10,854 - filelock - INFO - Lock 139758764100352 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:2021-05-07 16:51:10,854 - filelock - INFO - Lock 139758764100352 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:2021-05-07 16:51:10,868 - filelock - INFO - Lock 140297698061112 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:2021-05-07 16:51:10,878 - filelock - INFO - Lock 139975816969016 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:2021-05-07 16:51:10,879 - filelock - INFO - Lock 139975816969016 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:2021-05-07 16:51:16,009 - filelock - INFO - Lock 139988652874720 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:2021-05-07 16:51:16,047 - filelock - INFO - Lock 139751797538712 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:2021-05-07 16:51:16,048 - filelock - INFO - Lock 139751797538712 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:2021-05-07 16:51:16,368 - filelock - INFO - Lock 140297698061112 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:2021-05-07 16:51:16,373 - filelock - INFO - Lock 139968850760536 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:2021-05-07 16:51:16,374 - filelock - INFO - Lock 139968850760536 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO Bootstrap : Using [0]eth0:10.2.82.101<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO Bootstrap : Using [0]eth0:10.2.78.34<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO NET/Socket : Using [0]eth0:10.2.82.101<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO NET/Socket : Using [0]eth0:10.2.78.34<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:NCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:NCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO Bootstrap : Using [0]eth0:10.2.82.101<0>\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO NET/Socket : Using [0]eth0:10.2.82.101<0>\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO Bootstrap : Using [0]eth0:10.2.78.34<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO NET/Socket : Using [0]eth0:10.2.78.34<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO Channel 00/04 :    0   1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO Channel 01/04 :    0   1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO Channel 02/04 :    0   1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO Channel 03/04 :    0   1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO Trees [0] -1/-1/-1->1->0|0->1->-1/-1/-1 [1] -1/-1/-1->1->0|0->1->-1/-1/-1 [2] -1/-1/-1->1->0|0->1->-1/-1/-1 [3] -1/-1/-1->1->0|0->1->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1|-1->0->1/-1/-1 [1] 1/-1/-1->0->-1|-1->0->1/-1/-1 [2] 1/-1/-1->0->-1|-1->0->1/-1/-1 [3] 1/-1/-1->0->-1|-1->0->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO Channel 00 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO Channel 00 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO Channel 01 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO Channel 01 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO Channel 02 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO Channel 02 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO Channel 03 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO Channel 03 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO comm 0x561acf683220 rank 1 nranks 2 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO comm 0x55a6378d4140 rank 0 nranks 2 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO Channel 00/04 :    0   1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO Channel 01/04 :    0   1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO Channel 02/04 :    0   1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO Channel 03/04 :    0   1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO Trees [0] -1/-1/-1->1->0|0->1->-1/-1/-1 [1] -1/-1/-1->1->0|0->1->-1/-1/-1 [2] -1/-1/-1->1->0|0->1->-1/-1/-1 [3] -1/-1/-1->1->0|0->1->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1|-1->0->1/-1/-1 [1] 1/-1/-1->0->-1|-1->0->1/-1/-1 [2] 1/-1/-1->0->-1|-1->0->1/-1/-1 [3] 1/-1/-1->0->-1|-1->0->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO Channel 00 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO Channel 00 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO Channel 01 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO Channel 01 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO Channel 02 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO Channel 02 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO Channel 03 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO Channel 03 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO comm 0x55ecee45ffc0 rank 1 nranks 2 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO comm 0x558c7408ec30 rank 0 nranks 2 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO Trees [0] 2/-1/-1->1->0|0->1->2/-1/-1 [1] -1/-1/-1->1->0|0->1->-1/-1/-1 [2] 2/-1/-1->1->0|0->1->2/-1/-1 [3] -1/-1/-1->1->0|0->1->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO Channel 00/04 :    0   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO Channel 01/04 :    0   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO Channel 02/04 :    0   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO Channel 03/04 :    0   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1|-1->0->1/-1/-1 [1] 1/-1/-1->0->3|3->0->1/-1/-1 [2] 1/-1/-1->0->-1|-1->0->1/-1/-1 [3] 1/-1/-1->0->3|3->0->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO Trees [0] -1/-1/-1->3->2|2->3->-1/-1/-1 [1] 0/-1/-1->3->2|2->3->0/-1/-1 [2] -1/-1/-1->3->2|2->3->-1/-1/-1 [3] 0/-1/-1->3->2|2->3->0/-1/-1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO Trees [0] 3/-1/-1->2->1|1->2->3/-1/-1 [1] 3/-1/-1->2->-1|-1->2->3/-1/-1 [2] 3/-1/-1->2->1|1->2->3/-1/-1 [3] 3/-1/-1->2->-1|-1->2->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO Channel 00 : 1[180] -> 2[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO Channel 00 : 3[180] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO Channel 00 : 1[180] -> 2[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO Channel 00 : 3[180] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO Channel 00 : 2[170] -> 3[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO Channel 00 : 3[180] -> 2[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO Channel 00 : 2[170] -> 1[180] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO Channel 01 : 3[180] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO Channel 00 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO Channel 00 : 2[170] -> 1[180] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO Channel 00 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO Channel 01 : 3[180] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO Channel 01 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO Channel 01 : 1[180] -> 2[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO Channel 01 : 2[170] -> 3[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO Channel 01 : 0[170] -> 3[180] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO Channel 01 : 3[180] -> 2[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO Channel 01 : 1[180] -> 2[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO Channel 01 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO Channel 02 : 1[180] -> 2[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO Channel 02 : 2[170] -> 3[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO Channel 01 : 0[170] -> 3[180] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO Channel 02 : 1[180] -> 2[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO Channel 02 : 3[180] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO Channel 02 : 3[180] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO Channel 02 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO Channel 02 : 2[170] -> 1[180] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO Channel 02 : 3[180] -> 2[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO Channel 02 : 2[170] -> 1[180] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO Channel 02 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO Channel 03 : 3[180] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO Channel 03 : 3[180] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO Channel 03 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO Channel 03 : 1[180] -> 2[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO Channel 03 : 1[180] -> 2[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO Channel 03 : 2[170] -> 3[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO Channel 03 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:53:53 [1] NCCL INFO comm 0x55ecf110fd70 rank 1 nranks 4 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO Channel 03 : 0[170] -> 3[180] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO Channel 03 : 0[170] -> 3[180] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO Channel 03 : 3[180] -> 2[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:59 [0] NCCL INFO comm 0x55a63a583ff0 rank 2 nranks 4 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-2:60:60 [1] NCCL INFO comm 0x561ad23330d0 rank 3 nranks 4 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:52 [0] NCCL INFO comm 0x558c76d3eae0 rank 0 nranks 4 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Running smdistributed.dataparallel v1.0.0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:368 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:23.714 algo-1:53 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:23.714 algo-1:52 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:23.727 algo-2:59 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:23.727 algo-2:60 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:23.892 algo-1:53 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:23.892 algo-1:52 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:23.893 algo-1:53 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:23.893 algo-1:52 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:23.893 algo-1:53 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:23.893 algo-1:52 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:23.917 algo-2:59 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:23.917 algo-2:60 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:23.917 algo-2:59 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:23.917 algo-2:60 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:23.918 algo-2:59 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:23.918 algo-2:60 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:23.992 algo-1:53 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:23.993 algo-1:52 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.011 algo-2:60 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.011 algo-2:59 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.263 algo-1:52 INFO hook.py:550] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.263 algo-1:52 INFO hook.py:550] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.263 algo-1:52 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.263 algo-1:52 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.264 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.264 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.264 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.264 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.264 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.264 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.264 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.264 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.264 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.264 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.264 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.264 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.265 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.265 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.265 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.265 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.265 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.265 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.265 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.265 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.265 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.265 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.265 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.265 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.265 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.265 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.266 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.266 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.266 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.266 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.266 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.266 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.266 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.266 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.266 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.266 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.266 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.266 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.266 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.267 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.267 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.267 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.267 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.267 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.267 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.267 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.267 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.267 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.267 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.267 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.267 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.267 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.267 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.268 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.268 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.268 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.268 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.268 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.268 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.268 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.268 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.268 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.268 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.268 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.268 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.269 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.269 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.269 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.269 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.269 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.269 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.269 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.269 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.269 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.269 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.269 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.269 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.269 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.268 algo-2:59 INFO hook.py:550] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.268 algo-2:59 INFO hook.py:550] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.268 algo-2:59 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.269 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.268 algo-2:60 INFO hook.py:550] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896[1,2]<stdout>:[2021-05-07 16:51:24.268 algo-2:59 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.270 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.268 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.268 algo-2:60 INFO hook.py:550] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.268 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.268 algo-2:60 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.270 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.268 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.268 algo-2:60 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.268 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.270 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.268 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.268 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.270 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.268 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.269 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.270 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.269 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.269 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.270 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.269 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.269 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768[1,0]<stdout>:[2021-05-07 16:51:24.270 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.269 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.269 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.270 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.269 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.269 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.270 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.269 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.269 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.270 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.269 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.269 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.269 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.270 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.269 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.269 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.269 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.270 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.269 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.270 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.269 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.269 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.271 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.269 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.269 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.271 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.269 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.269 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.269 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.271 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.269 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.271 algo-1:52 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.269 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.271 algo-1:52 INFO hook.py:550] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.270 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.270 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.270 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.271 algo-1:52 INFO hook.py:550] name:module.pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.270 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.270 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.271 algo-1:52 INFO hook.py:550] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.270 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.270 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.271 algo-1:52 INFO hook.py:550] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.270 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.270 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.271 algo-1:52 INFO hook.py:552] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.270 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.270 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.270 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.270 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.271 algo-1:52 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.270 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.270 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.270 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.270 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.270 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.270 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.272 algo-1:52 INFO state_store.py:95] Checkpoints not updated. There are no checkpoint files created yet, to be updated\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.270 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.270 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.270 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.270 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.270 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.270 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.270 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.270 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.270 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.271 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.271 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.271 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.271 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.271 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.271 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.271 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.271 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.271 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.271 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.271 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.271 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.271 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.271 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.271 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.271 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.271 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.271 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.271 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.271 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.271 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.271 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.271 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.271 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.271 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.271 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.271 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.272 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.272 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.272 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.272 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.272 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.272 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.272 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.273 algo-1:53 INFO hook.py:550] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896[1,3]<stdout>:[2021-05-07 16:51:24.272 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.272 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.272 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.272 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.273 algo-1:53 INFO hook.py:550] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.272 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.272 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.272 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.273 algo-1:53 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.272 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.272 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.272 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.272 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.274 algo-1:53 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.272 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.272 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.272 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.272 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.274 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.272 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:24.274 algo-1:52 INFO hook.py:476] Hook is writing from the hook with pid: 52\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.274 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.272 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.272 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.272 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.274 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.272 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.272 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.274 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.273 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.273 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.274 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.273 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.273 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.274 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.273 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.274 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.273 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.273 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.273 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.274 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.273 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.274 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.273 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.273 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.273 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.274 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.273 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.274 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.273 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.273 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.273 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.274 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.273 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824[1,3]<stdout>:[2021-05-07 16:51:24.273 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.273 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.275 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.273 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.275 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.273 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.273 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.273 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.275 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.273 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.275 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.273 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.273 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.275 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.273 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.273 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.274 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.275 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.274 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.274 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.274 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.275 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824[1,3]<stdout>:[2021-05-07 16:51:24.274 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.274 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.274 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.274 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768[1,1]<stdout>:[2021-05-07 16:51:24.275 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.274 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.274 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.274 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.275 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824[1,2]<stdout>:[2021-05-07 16:51:24.274 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.274 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.274 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.274 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.275 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.274 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.274 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.274 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.276 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.274 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.274 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.274 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.274 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.276 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.274 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.274 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.274 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.274 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.276 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.274 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.274 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.275 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.276 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.275 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.275 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.275 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.275 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.276 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.275 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.275 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.275 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.276 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.275 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.275 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.275 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.275 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.275 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.276 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.275 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.275 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.275 algo-2:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.275 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.276 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.275 algo-2:59 INFO hook.py:550] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.275 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.275 algo-2:59 INFO hook.py:550] name:module.pre_classifier.bias count_params:768[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.277 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.275 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.275 algo-2:59 INFO hook.py:550] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.275 algo-2:60 INFO hook.py:550] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.277 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.275 algo-2:59 INFO hook.py:550] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.275 algo-2:60 INFO hook.py:550] name:module.pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.275 algo-2:59 INFO hook.py:552] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.275 algo-2:60 INFO hook.py:550] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.277 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.276 algo-2:60 INFO hook.py:550] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.276 algo-2:59 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.277 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.276 algo-2:60 INFO hook.py:552] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.277 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.276 algo-2:60 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.277 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.276 algo-2:59 INFO state_store.py:95] Checkpoints not updated. There are no checkpoint files created yet, to be updated\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.277 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.276 algo-2:60 INFO state_store.py:95] Checkpoints not updated. There are no checkpoint files created yet, to be updated\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.277 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.277 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.277 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.278 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.278 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.278 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.278 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.278 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:24.276 algo-2:60 WARNING hook.py:482] Unsupported Distributed Training Strategy Detected.                             Sagemaker-Debugger will only write from one process.                             The process with pid: 60 will not be writing any data. \u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.278 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.278 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.278 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.278 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.278 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.278 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.278 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.278 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.278 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.278 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.279 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.279 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.279 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.279 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.279 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.279 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.279 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.279 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.279 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.279 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:24.278 algo-2:59 INFO hook.py:476] Hook is writing from the hook with pid: 59\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.279 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.279 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.279 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.279 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.280 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.280 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.280 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.280 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.280 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.280 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.280 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.280 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.280 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.280 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.280 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.280 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.280 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.280 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.281 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.281 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.281 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.281 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.281 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.281 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.281 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.281 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.281 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.281 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.281 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.281 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.281 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.281 algo-1:53 INFO hook.py:550] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.282 algo-1:53 INFO hook.py:550] name:module.pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.282 algo-1:53 INFO hook.py:550] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.282 algo-1:53 INFO hook.py:550] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.282 algo-1:53 INFO hook.py:552] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.282 algo-1:53 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.282 algo-1:53 INFO state_store.py:95] Checkpoints not updated. There are no checkpoint files created yet, to be updated\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:24.282 algo-1:53 WARNING hook.py:482] Unsupported Distributed Training Strategy Detected.                             Sagemaker-Debugger will only write from one process.                             The process with pid: 53 will not be writing any data. \u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:25.287 algo-1:52 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:25.287 algo-1:53 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:25.288 algo-1:53 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:25.288 algo-1:52 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:25.295 algo-2:60 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:25.295 algo-2:59 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:25.295 algo-2:60 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:25.295 algo-2:59 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:25.297 algo-1:53 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:25.297 algo-1:53 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:DistributedDataParallel SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:25.299 algo-1:52 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:25.299 algo-1:52 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:DistributedDataParallel SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:25.305 algo-2:60 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:25.305 algo-2:60 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:DistributedDataParallel SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:25.307 algo-2:59 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:25.307 algo-2:59 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:DistributedDataParallel SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:26.059 algo-2:60 INFO state_store.py:95] Checkpoints not updated. There are no checkpoint files created yet, to be updated\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-05-07 16:51:26.060 algo-2:59 INFO state_store.py:95] Checkpoints not updated. There are no checkpoint files created yet, to be updated\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-05-07 16:51:26.060 algo-2:60 WARNING hook.py:638] Unsupported Distributed Training Strategy Detected.                 Sagemaker-Debugger will only write from one process.                 The process with pid: 60 will not be writing any data. \u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-05-07 16:51:26.066 algo-1:52 INFO state_store.py:95] Checkpoints not updated. There are no checkpoint files created yet, to be updated\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:26.076 algo-1:53 INFO state_store.py:95] Checkpoints not updated. There are no checkpoint files created yet, to be updated\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-05-07 16:51:26.077 algo-1:53 WARNING hook.py:638] Unsupported Distributed Training Strategy Detected.                 Sagemaker-Debugger will only write from one process.                 The process with pid: 53 will not be writing any data. \u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-2:59:356 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:52:360 [0] NCCL INFO Launch mode Parallel\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "estimator = helper.use_spot_distributed()\n",
    "\n",
    "inputs = { \n",
    "    \"train\": training_input_path, \n",
    "    \"test\": test_input_path\n",
    "}\n",
    "\n",
    "estimator.fit(inputs, wait = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Model for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import TrainingJobAnalytics\n",
    "\n",
    "df = TrainingJobAnalytics(training_job_name = estimator.latest_training_job.name).dataframe()\n",
    "display(df[[\"metric_name\", \"value\"]].groupby(\"metric_name\").max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "class SentimentAnalysis(Predictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super().__init__(\n",
    "            endpoint_name, \n",
    "            sagemaker_session = sagemaker_session, \n",
    "            serializer = JSONSerializer(), \n",
    "            deserializer = JSONDeserializer()\n",
    "        )\n",
    "\n",
    "name = name_from_base(\"imdb-huggingface\")\n",
    "\n",
    "model = PyTorchModel(\n",
    "    name = name,\n",
    "    role = role, \n",
    "    model_data = estimator.model_data,\n",
    "    source_dir = \"./scripts\",\n",
    "    entry_point = \"torchserve-predictor.py\",\n",
    "    framework_version = \"1.6.0\",\n",
    "    py_version = \"py36\",\n",
    "    predictor_cls = SentimentAnalysis\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "predictor = model.deploy(\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = \"ml.m5.large\",\n",
    "    endpoint_name = name,\n",
    "    wait = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Inferences\n",
    "## Using a SageMaker Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    POSITIVE: Willow is the greatest movie that ever lived.\n",
      "    NEGATIVE: The Notebook is ironically depressing.\n",
      "    POSITIVE: It's annoying that I had to Google the capitalization of Back to the Future, but it is a gem of nostalgic wonder.\n",
      "    NEGATIVE: Yikes! Weird Science did not age well for 2021.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "inputs = [\n",
    "    \"Willow is the greatest movie that ever lived.\",\n",
    "    \"The Notebook is ironically depressing.\",\n",
    "    \"It's annoying that I had to Google the capitalization of Back to the Future, but it is a gem of nostalgic wonder.\",\n",
    "    \"Yikes! Weird Science did not age well for 2021.\"\n",
    "]\n",
    "\n",
    "for it in inputs:\n",
    "    prediction = predictor.predict({\"text\": it})\n",
    "    print(f'    {prediction}: {it}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    predictor.delete_endpoint()\n",
    "    model.delete_model()\n",
    "except:\n",
    "    display(\"Already deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.6-cpu-py36-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
