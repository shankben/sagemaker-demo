{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface Sagemaker SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install --upgrade \"sagemaker>=2.31.0\" \"transformers==4.4.2\" \"datasets[s3]==1.5.0\"\n",
    "!conda install -c conda-forge ipywidgets -y\n",
    "\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)\n",
    "\n",
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name AmazonSageMaker-ExecutionRole-20201221T131849 to get Role path.\n",
      "Assuming role was created in SageMaker AWS console, as the name contains `AmazonSageMaker-ExecutionRole`. Defaulting to Role ARN with service-role in path. If this Role ARN is incorrect, please add IAM read permissions to your role or supply the Role Arn directly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker role arn: arn:aws:iam::061635907654:role/service-role/AmazonSageMaker-ExecutionRole-20201221T131849\n",
      "SageMaker bucket: sagemaker-us-east-1-061635907654\n",
      "SageMaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket = sagemaker_session_bucket)\n",
    "\n",
    "print(f\"SageMaker role arn: {role}\")\n",
    "print(f\"SageMaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"SageMaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-50165e5a8112834d.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3e4cd7a55d47a8925b639e288361d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d3c3974e224496b5d83cad52616c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 37.9 s, sys: 422 ms, total: 38.3 s\n",
      "Wall time: 39.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_dataset, test_dataset = datasets.load_dataset(\n",
    "    'imdb',\n",
    "    ignore_verifications = True,\n",
    "    split = ['train', 'test']\n",
    ")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding = 'max_length', truncation = True)\n",
    "\n",
    "test_ds = test_dataset.shuffle().select(range(10000))\n",
    "\n",
    "# tokenize dataset\n",
    "train_ds = train_dataset.map(tokenize)\n",
    "test_ds = test_ds.map(tokenize)\n",
    "\n",
    "# set format for pytorch\n",
    "train_ds = train_ds.rename_column(\"label\", \"labels\")\n",
    "train_ds.set_format('torch', columns = ['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "test_ds = test_ds.rename_column(\"label\", \"labels\")\n",
    "test_ds.set_format('torch', columns = ['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data to `sagemaker_session_bucket`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "s3_prefix = \"datasets/imdb-binary-classification\"\n",
    "\n",
    "training_input_path = f\"s3://{sess.default_bucket()}/{s3_prefix}/train\"\n",
    "train_ds.save_to_disk(training_input_path, fs = s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f\"s3://{sess.default_bucket()}/{s3_prefix}/test\"\n",
    "test_ds.save_to_disk(test_input_path, fs = s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "job_name = \"imdb-huggingface\"\n",
    "\n",
    "params = {\n",
    "    \"base_job_name\": job_name,\n",
    "    \"entry_point\": \"train.py\",\n",
    "    \"source_dir\": \"./scripts\",\n",
    "    \"instance_type\": \"ml.p3.16xlarge\",\n",
    "    \"instance_count\": 1,\n",
    "    \"role\": role,\n",
    "    \"transformers_version\": \"4.4.2\",\n",
    "    \"pytorch_version\": \"1.6.0\",\n",
    "    \"py_version\": \"py36\"\n",
    "}\n",
    "\n",
    "spot_params = {\n",
    "    \"checkpoint_s3_uri\": f\"s3://{sess.default_bucket()}/{job_name}/checkpoints\",\n",
    "    \"use_spot_instances\": True,\n",
    "    \"max_wait\": 3600,\n",
    "    \"max_run\": 660    \n",
    "}\n",
    "\n",
    "distributed_params = {\n",
    "    \"instance_count\": 2,\n",
    "    \"distribution\": {\n",
    "#         \"mpi\": {\n",
    "#             \"enabled\": True,\n",
    "#             \"processes_per_host\" : 1\n",
    "#         },\n",
    "        \"smdistributed\": {\n",
    "            \"dataparallel\": {\n",
    "                \"enabled\": True\n",
    "            }\n",
    "        }\n",
    "#         \"modelparallel\": {\n",
    "#             \"enabled\": True,\n",
    "#             \"parameters\": {\n",
    "#                 \"partitions\": 2\n",
    "#             }\n",
    "#         }\n",
    "    }\n",
    "}\n",
    "\n",
    "hyperparams = {\n",
    "    \"epochs\": 6,\n",
    "    \"train_batch_size\": 64,\n",
    "    \"eval_batch_size\": 128,\n",
    "    \"model_name\": model_name \n",
    "}\n",
    "\n",
    "def use_standard_training():\n",
    "    return HuggingFace(\n",
    "        **params,\n",
    "        hyperparameters = hyperparams\n",
    "    )\n",
    "\n",
    "def use_spot():\n",
    "    return HuggingFace(\n",
    "        **params,\n",
    "        **spot_params,\n",
    "        hyperparameters = {\n",
    "            **hyperparams,\n",
    "            \"output_dir\": \"/opt/ml/checkpoints\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "def use_spot_distributed():\n",
    "    return HuggingFace(**{\n",
    "        **params,\n",
    "        **spot_params, \n",
    "        **distributed_params,\n",
    "        hyperparameters = {\n",
    "            **hyperparams,\n",
    "            \"output_dir\": \"/opt/ml/checkpoints\"\n",
    "        }\n",
    "    })\n",
    "\n",
    "\n",
    "def use_distributed():\n",
    "    return HuggingFace(**{\n",
    "        **params,\n",
    "        **distributed_params\n",
    "    })\n",
    "\n",
    "estimator = use_spot_distributed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: imdb-huggingface-2021-04-28-21-15-53-925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-28 21:15:54 Starting - Starting the training job...\n",
      "2021-04-28 21:16:17 Starting - Launching requested ML instancesProfilerReport-1619644554: InProgress\n",
      "............\n",
      "2021-04-28 21:18:17 Starting - Preparing the instances for training.........\n",
      "2021-04-28 21:19:41 Downloading - Downloading input data...\n",
      "2021-04-28 21:20:24 Training - Downloading the training image...............\n",
      "2021-04-28 21:22:54 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-04-28 21:22:50,059 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-04-28 21:22:50,137 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2021-04-28 21:22:55,243 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2021-04-28 21:22:55,321 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2021-04-28 21:22:58,341 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\u001b[0m\n",
      "\u001b[35m2021-04-28 21:22:58,341 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2021-04-28 21:22:58,892 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2021-04-28 21:22:58,892 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[35m2021-04-28 21:22:58,894 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-04-28 21:22:58,895 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.2.127.61\u001b[0m\n",
      "\u001b[35m2021-04-28 21:22:59,897 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-04-28 21:22:59,897 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.2.127.61\u001b[0m\n",
      "\u001b[35m2021-04-28 21:23:00,899 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-04-28 21:23:00,899 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.2.127.61\u001b[0m\n",
      "\u001b[35m2021-04-28 21:23:01,901 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-04-28 21:23:01,901 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.2.127.61\u001b[0m\n",
      "\u001b[35m2021-04-28 21:23:02,903 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-04-28 21:23:02,903 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.2.127.61\u001b[0m\n",
      "\u001b[34m2021-04-28 21:23:02,764 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\u001b[0m\n",
      "\u001b[34m2021-04-28 21:23:02,764 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-04-28 21:23:03,321 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2021-04-28 21:23:03,321 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2021-04-28 21:23:03,324 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2021-04-28 21:23:03,326 sagemaker-training-toolkit INFO     Cannot connect to host algo-2 at port 22\u001b[0m\n",
      "\u001b[34m2021-04-28 21:23:03,326 sagemaker-training-toolkit ERROR    Connection failed\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/sagemaker_training/smdataparallel.py\", line 306, in _can_connect\n",
      "    client.connect(host, port=port)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/paramiko/client.py\", line 368, in connect\n",
      "    raise NoValidConnectionsError(errors)\u001b[0m\n",
      "\u001b[34mparamiko.ssh_exception.NoValidConnectionsError: [Errno None] Unable to connect to port 22 on 10.2.127.244\u001b[0m\n",
      "\u001b[34m2021-04-28 21:23:03,328 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[35m2021-04-28 21:23:03,913 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[35m2021-04-28 21:23:03,987 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2021-04-28 21:23:03,987 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-04-28 21:23:03,987 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2021-04-28 21:23:03,987 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[35m2021-04-28 21:23:03,992 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m2021-04-28 21:23:04,337 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[34m2021-04-28 21:23:04,410 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2021-04-28 21:23:04,410 sagemaker-training-toolkit INFO     Can connect to host algo-2 at port 22\u001b[0m\n",
      "\u001b[34m2021-04-28 21:23:04,411 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[34m2021-04-28 21:23:04,411 sagemaker-training-toolkit INFO     Worker algo-2 available for communication\u001b[0m\n",
      "\u001b[34m2021-04-28 21:23:04,411 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2021-04-28 21:23:04,411 sagemaker-training-toolkit INFO     Host: ['algo-1', 'algo-2']\u001b[0m\n",
      "\u001b[34m2021-04-28 21:23:04,411 sagemaker-training-toolkit INFO     instance type: ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34m2021-04-28 21:23:04,488 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": true,\n",
      "        \"sagemaker_distributed_dataparallel_custom_mpi_options\": \"\",\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"train_batch_size\": 64,\n",
      "        \"model_name\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
      "        \"epochs\": 6,\n",
      "        \"eval_batch_size\": 128\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"imdb-huggingface-2021-04-28-21-15-53-925\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-061635907654/imdb-huggingface-2021-04-28-21-15-53-925/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":6,\"eval_batch_size\":128,\"model_name\":\"distilbert-base-uncased-finetuned-sst-2-english\",\"train_batch_size\":64}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-061635907654/imdb-huggingface-2021-04-28-21-15-53-925/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"epochs\":6,\"eval_batch_size\":128,\"model_name\":\"distilbert-base-uncased-finetuned-sst-2-english\",\"train_batch_size\":64},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"imdb-huggingface-2021-04-28-21-15-53-925\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-061635907654/imdb-huggingface-2021-04-28-21-15-53-925/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"6\",\"--eval_batch_size\",\"128\",\"--model_name\",\"distilbert-base-uncased-finetuned-sst-2-english\",\"--train_batch_size\",\"64\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased-finetuned-sst-2-english\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=6\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_BATCH_SIZE=128\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:8,algo-2:8 -np 16 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 2 -x NCCL_SOCKET_IFNAME=eth0 -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_HOMOGENEOUS=1 -x FI_PROVIDER=sockets -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -x SMDATAPARALLEL_SERVER_ADDR=algo-1 -x SMDATAPARALLEL_SERVER_PORT=7592 -x SAGEMAKER_INSTANCE_TYPE=ml.p3.16xlarge smddprun /opt/conda/bin/python3.6 -m mpi4py train.py --epochs 6 --eval_batch_size 128 --model_name distilbert-base-uncased-finetuned-sst-2-english --train_batch_size 64\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35m2021-04-28 21:23:05,999 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=49, name='orted', status='sleeping', started='21:23:04')]\u001b[0m\n",
      "\u001b[35m2021-04-28 21:23:06,000 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=49, name='orted', status='sleeping', started='21:23:04')]\u001b[0m\n",
      "\u001b[35m2021-04-28 21:23:06,000 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=49, name='orted', status='sleeping', started='21:23:04')]\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:2021-04-28 21:23:10,988 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:2021-04-28 21:23:10,989 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:2021-04-28 21:23:10,988 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:2021-04-28 21:23:10,988 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:2021-04-28 21:23:10,988 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:2021-04-28 21:23:10,988 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:2021-04-28 21:23:10,988 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:2021-04-28 21:23:10,988 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:2021-04-28 21:23:10,988 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:2021-04-28 21:23:10,989 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:2021-04-28 21:23:10,988 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:2021-04-28 21:23:10,988 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:2021-04-28 21:23:10,988 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:2021-04-28 21:23:10,989 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:2021-04-28 21:23:10,988 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:2021-04-28 21:23:10,989 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:2021-04-28 21:23:11,104 - filelock - INFO - Lock 139772424157672 acquired on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:2021-04-28 21:23:11,125 - filelock - INFO - Lock 139772424157672 released on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:2021-04-28 21:23:11,149 - filelock - INFO - Lock 139765749048264 acquired on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:2021-04-28 21:23:11,154 - filelock - INFO - Lock 140164033286888 acquired on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:2021-04-28 21:23:11,154 - filelock - INFO - Lock 140164033286888 released on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:2021-04-28 21:23:11,154 - filelock - INFO - Lock 139735876198584 acquired on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:2021-04-28 21:23:11,155 - filelock - INFO - Lock 139735876198584 released on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:2021-04-28 21:23:11,204 - filelock - INFO - Lock 139790133033448 acquired on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:2021-04-28 21:23:11,204 - filelock - INFO - Lock 139790133033448 released on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:2021-04-28 21:23:11,205 - filelock - INFO - Lock 140121722024624 acquired on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:2021-04-28 21:23:11,205 - filelock - INFO - Lock 140121722024624 released on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:2021-04-28 21:23:11,254 - filelock - INFO - Lock 139925873178664 acquired on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:2021-04-28 21:23:11,255 - filelock - INFO - Lock 139925873178664 released on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:2021-04-28 21:23:11,255 - filelock - INFO - Lock 140305964791848 acquired on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:2021-04-28 21:23:11,255 - filelock - INFO - Lock 140305964791848 released on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:2021-04-28 21:23:11,305 - filelock - INFO - Lock 139773775180520 acquired on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:2021-04-28 21:23:11,305 - filelock - INFO - Lock 139773775180520 released on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:2021-04-28 21:23:11,355 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:2021-04-28 21:23:11,354 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:2021-04-28 21:23:11,355 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:2021-04-28 21:23:11,354 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:2021-04-28 21:23:11,355 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:2021-04-28 21:23:11,354 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:2021-04-28 21:23:11,355 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:2021-04-28 21:23:11,354 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:2021-04-28 21:23:11,355 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:2021-04-28 21:23:11,354 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:2021-04-28 21:23:11,355 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:2021-04-28 21:23:11,354 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:2021-04-28 21:23:11,355 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:2021-04-28 21:23:11,355 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:2021-04-28 21:23:11,355 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:2021-04-28 21:23:11,355 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:2021-04-28 21:23:11,461 - filelock - INFO - Lock 140676697336576 acquired on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:2021-04-28 21:23:11,484 - filelock - INFO - Lock 140676697336576 released on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:2021-04-28 21:23:11,506 - filelock - INFO - Lock 140669705966312 acquired on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:2021-04-28 21:23:11,511 - filelock - INFO - Lock 140291808492288 acquired on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:2021-04-28 21:23:11,511 - filelock - INFO - Lock 140291808492288 released on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:2021-04-28 21:23:11,512 - filelock - INFO - Lock 140263920163584 acquired on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:2021-04-28 21:23:11,512 - filelock - INFO - Lock 140263920163584 released on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:2021-04-28 21:23:11,513 - filelock - INFO - Lock 139694685614712 acquired on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:2021-04-28 21:23:11,513 - filelock - INFO - Lock 139694685614712 released on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:2021-04-28 21:23:11,513 - filelock - INFO - Lock 139842030465656 acquired on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:2021-04-28 21:23:11,514 - filelock - INFO - Lock 139842030465656 released on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:2021-04-28 21:23:11,562 - filelock - INFO - Lock 140603158747512 acquired on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:2021-04-28 21:23:11,562 - filelock - INFO - Lock 140603158747512 released on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:2021-04-28 21:23:11,612 - filelock - INFO - Lock 139959443337848 acquired on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:2021-04-28 21:23:11,612 - filelock - INFO - Lock 139959443337848 released on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:2021-04-28 21:23:11,662 - filelock - INFO - Lock 140328286503544 acquired on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:2021-04-28 21:23:11,663 - filelock - INFO - Lock 140328286503544 released on /root/.cache/huggingface/transformers/4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b.lock\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:2021-04-28 21:23:16,142 - filelock - INFO - Lock 139765749048264 released on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:2021-04-28 21:23:16,142 - filelock - INFO - Lock 139783458526152 acquired on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:2021-04-28 21:23:16,143 - filelock - INFO - Lock 139783458526152 released on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:2021-04-28 21:23:16,143 - filelock - INFO - Lock 139728970023880 acquired on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:2021-04-28 21:23:16,144 - filelock - INFO - Lock 139728970023880 released on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:2021-04-28 21:23:16,144 - filelock - INFO - Lock 140298945949424 acquired on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:2021-04-28 21:23:16,145 - filelock - INFO - Lock 140298945949424 released on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:2021-04-28 21:23:16,147 - filelock - INFO - Lock 139773775180688 acquired on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:2021-04-28 21:23:16,148 - filelock - INFO - Lock 139773775180688 released on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:2021-04-28 21:23:16,191 - filelock - INFO - Lock 140164033287056 acquired on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:2021-04-28 21:23:16,192 - filelock - INFO - Lock 140164033287056 released on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:2021-04-28 21:23:16,193 - filelock - INFO - Lock 140121722024792 acquired on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:2021-04-28 21:23:16,193 - filelock - INFO - Lock 140121722024792 released on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:2021-04-28 21:23:16,243 - filelock - INFO - Lock 139919261094520 acquired on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:2021-04-28 21:23:16,244 - filelock - INFO - Lock 139919261094520 released on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:2021-04-28 21:23:16,868 - filelock - INFO - Lock 140669705966312 released on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:2021-04-28 21:23:16,916 - filelock - INFO - Lock 140596391428560 acquired on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:2021-04-28 21:23:16,916 - filelock - INFO - Lock 140596391428560 released on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:2021-04-28 21:23:16,966 - filelock - INFO - Lock 140285032403632 acquired on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:2021-04-28 21:23:16,966 - filelock - INFO - Lock 140285032403632 released on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:2021-04-28 21:23:17,016 - filelock - INFO - Lock 140321207734904 acquired on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:2021-04-28 21:23:17,017 - filelock - INFO - Lock 140321207734904 released on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:2021-04-28 21:23:17,066 - filelock - INFO - Lock 140260756243120 acquired on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:2021-04-28 21:23:17,067 - filelock - INFO - Lock 140260756243120 released on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:2021-04-28 21:23:17,117 - filelock - INFO - Lock 139952726098496 acquired on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:2021-04-28 21:23:17,117 - filelock - INFO - Lock 139952726098496 released on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:2021-04-28 21:23:17,167 - filelock - INFO - Lock 139691554976376 acquired on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:2021-04-28 21:23:17,168 - filelock - INFO - Lock 139691554976376 released on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:2021-04-28 21:23:17,217 - filelock - INFO - Lock 139834978218672 acquired on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:2021-04-28 21:23:17,218 - filelock - INFO - Lock 139834978218672 released on /root/.cache/huggingface/transformers/8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c.lock\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:NCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:NCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Running smdistributed.dataparallel v1.0.0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.178 algo-1:49 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.178 algo-1:57 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.178 algo-1:60 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.178 algo-1:51 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.178 algo-1:61 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.178 algo-1:59 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.178 algo-1:55 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.178 algo-1:53 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.195 algo-2:56 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.195 algo-2:66 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.195 algo-2:60 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.195 algo-2:58 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.195 algo-2:64 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.195 algo-2:68 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.195 algo-2:62 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.195 algo-2:67 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.376 algo-1:49 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.376 algo-1:57 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.376 algo-1:60 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.376 algo-1:51 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.376 algo-1:59 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.376 algo-1:55 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.376 algo-1:49 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.376 algo-1:60 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.376 algo-1:51 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.376 algo-1:59 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.376 algo-1:55 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.376 algo-1:57 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.377 algo-1:59 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.377 algo-1:55 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.377 algo-1:49 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.377 algo-1:60 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.377 algo-1:51 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.377 algo-1:61 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.377 algo-1:57 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.377 algo-1:61 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.378 algo-1:61 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.378 algo-1:53 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.379 algo-1:53 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.379 algo-1:53 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.397 algo-2:56 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.397 algo-2:66 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.397 algo-2:58 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.397 algo-2:64 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.397 algo-2:68 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.397 algo-2:62 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.397 algo-2:67 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.398 algo-2:60 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.398 algo-2:62 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.398 algo-2:66 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.398 algo-2:64 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.398 algo-2:67 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.398 algo-2:58 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.398 algo-2:56 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.398 algo-2:68 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.398 algo-2:60 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.398 algo-2:64 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.[1,11]<stdout>:[2021-04-28 21:23:27.398 algo-2:62 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.398 algo-2:66 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.398 algo-2:58 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.398 algo-2:68 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.399 algo-2:60 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.399 algo-2:67 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.399 algo-2:56 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.478 algo-1:59 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.478 algo-1:59 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.478 algo-1:55 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.478 algo-1:51 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.478 algo-1:60 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.478 algo-1:53 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.478 algo-1:57 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.478 algo-1:60 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.478 algo-1:55 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.478 algo-1:51 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.478 algo-1:61 INFO hook.py:253] Saving to /opt/ml/output/tensors[1,2]<stdout>:[2021-04-28 21:23:27.479 algo-1:53 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.479 algo-1:57 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.479 algo-1:49 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.479 algo-1:61 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.479 algo-1:49 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.505 algo-2:68 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.505 algo-2:62 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.505 algo-2:64 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.505 algo-2:66 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.505 algo-2:68 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.505 algo-2:62 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.505 algo-2:64 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.505 algo-2:66 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.505 algo-2:56 INFO hook.py:253] Saving to /opt/ml/output/tensors[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.505 algo-2:58 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.505 algo-2:60 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.505 algo-2:56 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.505 algo-2:58 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.505 algo-2:67 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.505 algo-2:60 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.506 algo-2:67 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.776 algo-1:49 INFO hook.py:550] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.776 algo-1:49 INFO hook.py:550] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.776 algo-1:49 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.776 algo-1:49 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.776 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.776 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.776 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.776 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.776 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.776 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.776 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.776 algo-1:53 INFO hook.py:550] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.777 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.777 algo-1:53 INFO hook.py:550] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.777 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.777 algo-1:53 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.777 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.777 algo-1:53 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.777 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.777 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.777 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.777 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.777 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.777 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.777 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.777 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.777 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.777 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.777 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.777 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.777 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.777 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.777 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.777 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.777 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.777 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.778 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.778 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.778 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.778 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.778 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.778 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.778 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.778 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.778 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.778 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.778 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.778 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.778 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.778 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.778 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.778 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.778 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.778 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.778 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.778 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.778 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.778 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.778 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.778 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.778 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.778 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.778 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.779 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.779 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.779 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.779 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.779 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.779 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.779 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.779 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.779 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.779 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.779 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.779 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.779 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.779 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.779 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.779 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.779 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.779 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.779 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.779 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.779 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.779 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.779 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.779 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.779 algo-1:60 INFO hook.py:550] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.779 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.779 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.779 algo-1:60 INFO hook.py:550] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.779 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.779 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.780 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.780 algo-1:60 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.780 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.780 algo-1:60 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.780 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.780 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.780 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.780 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.780 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.780 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.780 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.780 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.780 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.780 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.780 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.780 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.780 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.780 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.780 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.780 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.780 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.780 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.780 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.780 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.780 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.780 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.780 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.780 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.780 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.780 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.780 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.780 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.780 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.780 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.780 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768[1,5]<stdout>:[2021-04-28 21:23:27.780 algo-1:59 INFO hook.py:550] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.780 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.780 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.780 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.780 algo-1:59 INFO hook.py:550] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216[1,0]<stdout>:[2021-04-28 21:23:27.780 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.780 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.781 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.781 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.781 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.781 algo-1:59 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.781 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.781 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.781 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.781 algo-1:59 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.781 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.781 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.781 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.781 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.781 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.781 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.781 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768[1,6]<stdout>:[2021-04-28 21:23:27.781 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.781 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.781 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.781 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.781 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.781 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.781 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.781 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.781 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.781 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.781 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.781 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.781 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.781 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.781 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.781 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824[1,2]<stdout>:[2021-04-28 21:23:27.781 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.781 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.781 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.781 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.781 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.781 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.781 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.781 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.781 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.781 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.781 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.781 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.781 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.781 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.781 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.781 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.781 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.781 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.781 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.782 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.782 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.782 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.782 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.782 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.782 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.782 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.782 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.782 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.782 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.782 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.782 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.782 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.782 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.782 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.782 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.782 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.782 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.782 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.782 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.782 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.782 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.782 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.782 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.782 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.782 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.782 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.782 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.782 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.782 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.782 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.782 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.782 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.782 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.782 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.782 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.782 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.782 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.782 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.782 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.782 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.782 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.782 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.782 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.782 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.782 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.782 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.782 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.782 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.782 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.782 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.782 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.782 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.782 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.782 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.782 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.783 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.783 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.783 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.783 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.783 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.783 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.783 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.783 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.783 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.783 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.783 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296[1,5]<stdout>:[2021-04-28 21:23:27.783 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.783 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.783 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.783 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072[1,0]<stdout>:[2021-04-28 21:23:27.783 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.783 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.783 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.783 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.783 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.783 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.783 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.783 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.783 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.783 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.783 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.783 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.783 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.783 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.783 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.783 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.783 algo-1:49 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.783 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.783 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824[1,5]<stdout>:[2021-04-28 21:23:27.783 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.783 algo-1:49 INFO hook.py:550] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.783 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.783 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.783 algo-1:49 INFO hook.py:550] name:module.pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.783 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.783 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.783 algo-1:49 INFO hook.py:550] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.783 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.783 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.783 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.783 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.783 algo-1:49 INFO hook.py:550] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.783 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.783 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.783 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.783 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.783 algo-1:49 INFO hook.py:552] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.783 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.783 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.783 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.784 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.784 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.784 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:27.784 algo-1:49 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.784 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.784 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.784 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.784 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.784 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.784 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.784 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.784 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.784 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.784 algo-1:51 INFO hook.py:550] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.784 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.784 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.784 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.784 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.784 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.784 algo-1:53 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.784 algo-1:51 INFO hook.py:550] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.784 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.784 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.784 algo-1:53 INFO hook.py:550] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.784 algo-1:51 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.weight count_params:768[1,5]<stdout>:[2021-04-28 21:23:27.784 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.784 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.784 algo-1:53 INFO hook.py:550] name:module.pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.784 algo-1:51 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.784 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768[1,6]<stdout>:[2021-04-28 21:23:27.784 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.784 algo-1:53 INFO hook.py:550] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.784 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.784 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.784 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.784 algo-1:53 INFO hook.py:550] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.784 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.784 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.784 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.784 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.784 algo-1:53 INFO hook.py:552] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.784 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.784 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.784 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.784 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.784 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.784 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:27.784 algo-1:53 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.784 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.784 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.785 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.785 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.785 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.785 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.785 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.785 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.785 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.785 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.785 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.785 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.785 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.785 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.785 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.785 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.785 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.785 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.785 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.785 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.785 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.785 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.785 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.785 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.785 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.785 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.785 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.785 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.785 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.785 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.785 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.785 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.785 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.785 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.785 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.785 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.785 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.785 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.785 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.785 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.785 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.785 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.786 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.786 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.786 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.786 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.786 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.786 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.786 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.786 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.786 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.786 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.786 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.786 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.786 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.786 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.786 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.786 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.786 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.786 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.786 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.786 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.786 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.786 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.786 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.786 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.786 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.786 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.786 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.786 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.786 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.786 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.786 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.786 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.786 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.786 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.786 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.786 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.786 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.786 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.786 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.786 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.786 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.786 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.787 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.787 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.787 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.787 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.787 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.787 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.787 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.787 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.787 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.787 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.787 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.787 algo-1:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.787 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.787 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.787 algo-1:60 INFO hook.py:550] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.787 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.787 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.787 algo-1:60 INFO hook.py:550] name:module.pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.787 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.787 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.787 algo-1:60 INFO hook.py:550] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.787 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.787 algo-1:60 INFO hook.py:550] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.787 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.787 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.787 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.787 algo-1:60 INFO hook.py:552] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.787 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.787 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.787 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.787 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:27.787 algo-1:60 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.787 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.787 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.787 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.787 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.787 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.787 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.788 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.788 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.788 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.788 algo-1:59 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.788 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.788 algo-1:59 INFO hook.py:550] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.788 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.788 algo-1:59 INFO hook.py:550] name:module.pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.788 algo-1:55 INFO hook.py:550] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.788 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.788 algo-1:59 INFO hook.py:550] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.788 algo-1:55 INFO hook.py:550] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.788 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.788 algo-1:59 INFO hook.py:550] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.788 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.788 algo-1:59 INFO hook.py:552] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.788 algo-1:55 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.788 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.788 algo-1:55 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.788 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:27.788 algo-1:59 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.788 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.788 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.788 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.788 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.788 algo-1:61 INFO hook.py:550] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.788 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.788 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.788 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.788 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.788 algo-1:61 INFO hook.py:550] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.788 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.788 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.789 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.789 algo-1:61 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.789 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.789 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.789 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.789 algo-1:61 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.789 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.789 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.789 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.789 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.789 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.789 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.789 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.789 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.789 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.789 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.789 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.789 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.789 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.789 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.789 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.789 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.789 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.789 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.789 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768[1,1]<stdout>:[2021-04-28 21:23:27.789 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.789 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.789 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.789 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.789 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.789 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.789 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.789 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.789 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.789 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.789 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.789 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.789 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.789 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.790 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.790 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.790 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.790 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.790 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.790 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.790 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.790 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.790 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.790 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.790 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.790 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.790 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.790 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.790 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.790 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.790 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.790 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.790 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.790 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.790 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.790 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.790 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.790 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.790 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.790 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.790 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.790 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.790 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.790 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.790 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.790 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.790 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.790 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.790 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.790 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.790 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.790 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.791 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.791 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.791 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.791 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.791 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.791 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.791 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.791 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.791 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.791 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.791 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.791 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.791 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.791 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.791 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.791 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.791 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.791 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.791 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.791 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.791 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.791 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.791 algo-1:51 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.791 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.791 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.791 algo-1:51 INFO hook.py:550] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.791 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.791 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.791 algo-1:51 INFO hook.py:550] name:module.pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.791 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.791 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.791 algo-1:51 INFO hook.py:550] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.791 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.791 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.791 algo-1:51 INFO hook.py:550] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.791 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.791 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.791 algo-1:51 INFO hook.py:552] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.791 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.792 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.792 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:27.792 algo-1:51 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.792 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.792 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.792 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.792 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.792 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.792 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.792 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.792 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.792 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.792 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.792 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.792 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.792 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.792 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.792 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.792 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.792 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.792 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.792 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.792 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.792 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.792 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.792 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.792 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.793 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.793 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.793 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.793 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.793 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.793 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.793 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.793 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.793 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.793 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.793 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.793 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.793 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.793 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.793 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.793 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.793 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.793 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.793 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.793 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.793 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.793 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.793 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.793 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.793 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.793 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.794 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296[1,3]<stdout>:[2021-04-28 21:23:27.794 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.794 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.794 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.794 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.794 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.794 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.794 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.794 algo-1:57 INFO hook.py:550] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.794 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.794 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.794 algo-1:57 INFO hook.py:550] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.794 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.794 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.794 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.794 algo-1:57 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.794 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.794 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.794 algo-1:57 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.794 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.794 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.794 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.794 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.794 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.794 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.794 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.794 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.794 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.794 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.794 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.794 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768[1,4]<stdout>:[2021-04-28 21:23:27.794 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.795 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.795 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.795 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.795 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.795 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.795 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.795 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.795 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.795 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.795 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.795 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.795 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.795 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.795 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.795 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.795 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.795 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.795 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.795 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.795 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.795 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.795 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.795 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.795 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.795 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.795 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.795 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.795 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.795 algo-1:55 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.795 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.795 algo-1:55 INFO hook.py:550] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.795 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.795 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.795 algo-1:55 INFO hook.py:550] name:module.pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.795 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.795 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.795 algo-1:55 INFO hook.py:550] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.796 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.796 algo-1:55 INFO hook.py:550] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.796 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.796 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.796 algo-1:55 INFO hook.py:552] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.796 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.796 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.796 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:27.796 algo-1:55 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.796 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.796 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.796 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.796 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.796 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.796 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.796 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.796 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.796 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.796 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.796 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.796 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.796 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.796 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.796 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.796 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.796 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.796 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.796 algo-1:61 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.797 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.797 algo-1:61 INFO hook.py:550] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.797 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.797 algo-1:61 INFO hook.py:550] name:module.pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.797 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.797 algo-1:61 INFO hook.py:550] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.797 algo-1:61 INFO hook.py:550] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.797 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.797 algo-1:61 INFO hook.py:552] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.797 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.797 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:27.797 algo-1:61 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.797 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.797 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.797 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.797 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.797 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.797 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.798 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.798 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.798 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.798 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.798 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.798 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.798 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.798 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.798 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.798 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.798 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.798 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.798 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.799 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.799 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.799 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.799 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.799 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.799 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.799 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.799 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.799 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.799 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.799 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.799 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.799 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.799 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.800 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.800 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.800 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.800 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.800 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.800 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.800 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.800 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.800 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.800 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.800 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.800 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.800 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.801 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.801 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.801 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.801 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.801 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.801 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.801 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.801 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.801 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.801 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.801 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.801 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.801 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.802 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.802 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.802 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.802 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.802 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.802 algo-1:57 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.802 algo-1:57 INFO hook.py:550] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.802 algo-1:57 INFO hook.py:550] name:module.pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.802 algo-1:57 INFO hook.py:550] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.802 algo-1:57 INFO hook.py:550] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.802 algo-1:57 INFO hook.py:552] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:27.802 algo-1:57 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.810 algo-2:62 INFO hook.py:550] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.810 algo-2:62 INFO hook.py:550] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.810 algo-2:62 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.810 algo-2:62 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.810 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.810 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.811 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.811 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.811 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.811 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.811 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.811 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.811 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.811 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.811 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.811 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.811 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.811 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.811 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.811 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.812 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.812 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.812 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.812 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.812 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.812 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.812 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.812 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.812 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.812 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.812 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.812 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.812 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.812 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.813 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.813 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.813 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.813 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.813 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.813 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.813 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.813 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.813 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.813 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.813 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.813 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.813 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.813 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.814 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.814 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.814 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768[1,11]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.814 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.814 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.814 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.814 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.814 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.814 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.814 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.814 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.814 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.815 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.815 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.815 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.815 algo-2:67 INFO hook.py:550] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.815 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.815 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.815 algo-2:67 INFO hook.py:550] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.815 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.815 algo-2:67 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.815 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.815 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768[1,14]<stdout>:[2021-04-28 21:23:27.815 algo-2:67 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.815 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.815 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.815 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.815 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.815 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.815 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.815 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.815 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.815 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.815 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.815 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.815 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.816 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.816 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.816 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.816 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.816 algo-2:68 INFO hook.py:550] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.816 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.816 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.816 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.816 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.816 algo-2:68 INFO hook.py:550] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.816 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.816 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.816 algo-2:68 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.816 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.816 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.816 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.816 algo-2:68 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.816 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.816 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.816 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.816 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.816 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.816 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.816 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.816 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.816 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.816 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.816 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.816 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.816 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.816 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.816 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.816 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.816 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.816 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.816 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.816 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.817 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.817 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.817 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.817 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.817 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.817 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.817 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.817 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.817 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.817 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.817 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.817 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.817 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.817 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.817 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.817 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.817 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.817 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.817 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.817 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.817 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.817 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.817 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.817 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.817 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.817 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.817 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.817 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.817 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.817 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.817 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.817 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.817 algo-2:62 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.817 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.817 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.817 algo-2:62 INFO hook.py:550] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.817 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.817 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.817 algo-2:62 INFO hook.py:550] name:module.pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.817 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.818 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.818 algo-2:62 INFO hook.py:550] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.818 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.818 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.818 algo-2:62 INFO hook.py:550] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.818 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.818 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.818 algo-2:62 INFO hook.py:552] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.818 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.818 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.818 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:27.818 algo-2:62 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.818 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.818 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.818 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.818 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.818 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.818 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.818 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.818 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.818 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.818 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.818 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.818 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.818 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.818 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.818 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.818 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.818 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.819 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.819 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.819 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.819 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.819 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.819 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.819 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.819 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.819 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.819 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.819 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.819 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.819 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.819 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.819 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.819 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.819 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.819 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.819 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.819 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.819 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.819 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.819 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.819 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.819 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.819 algo-2:66 INFO hook.py:550] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.819 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.819 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.820 algo-2:66 INFO hook.py:550] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.820 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.820 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.820 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.820 algo-2:66 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.820 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.820 algo-2:66 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.820 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.820 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.820 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.820 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.820 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.820 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.820 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.820 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.820 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.820 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.820 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.820 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.820 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.820 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.820 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.820 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.820 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.820 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.820 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.820 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.820 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.820 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.820 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.820 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.820 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.820 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.820 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.820 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.820 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.820 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.820 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.820 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.820 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.821 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.821 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.821 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.821 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.821 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.821 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.821 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.821 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.821 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.821 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.821 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.821 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.821 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.821 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.821 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.821 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.821 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.821 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.821 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.821 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.821 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.821 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.821 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.821 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.821 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.821 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.821 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.821 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.821 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.821 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.821 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.821 algo-2:60 INFO hook.py:550] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.821 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.821 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.821 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.821 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.821 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.821 algo-2:60 INFO hook.py:550] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.821 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.821 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.821 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.821 algo-2:60 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.822 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.822 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.822 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.822 algo-2:60 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.822 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.822 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.822 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.822 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.822 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.822 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.822 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.822 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.822 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.822 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.822 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.822 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.822 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768[1,10]<stdout>:[2021-04-28 21:23:27.822 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.822 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.822 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.822 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.822 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.822 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.822 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.822 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.822 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.822 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.822 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.822 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.822 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.822 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.822 algo-2:67 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.822 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.822 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.822 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.822 algo-2:67 INFO hook.py:550] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.822 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.822 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.822 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.822 algo-2:67 INFO hook.py:550] name:module.pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.822 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.822 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.822 algo-2:67 INFO hook.py:550] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.822 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.822 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.822 algo-2:67 INFO hook.py:550] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.822 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.822 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.822 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.822 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.822 algo-2:67 INFO hook.py:552] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.822 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.823 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.823 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.823 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.823 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.823 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:27.823 algo-2:67 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.823 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.823 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.823 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.823 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.823 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.823 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.823 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.823 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.823 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.823 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.823 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.823 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.823 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.823 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.823 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.823 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.823 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.823 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.823 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.823 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.823 algo-2:58 INFO hook.py:550] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.823 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.823 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.823 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.823 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.823 algo-2:58 INFO hook.py:550] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.823 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.823 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.823 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.823 algo-2:58 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.823 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.823 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.823 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.823 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.823 algo-2:58 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.823 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.823 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.823 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.823 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.823 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.823 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.824 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.824 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.824 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.824 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.824 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.824 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.824 algo-2:68 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.824 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.824 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.824 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.824 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.824 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.824 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.824 algo-2:68 INFO hook.py:550] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.824 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.824 algo-2:68 INFO hook.py:550] name:module.pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.824 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.824 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.824 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.824 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.824 algo-2:68 INFO hook.py:550] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.824 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.824 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.824 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.824 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.824 algo-2:68 INFO hook.py:550] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.824 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.824 algo-2:68 INFO hook.py:552] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.824 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.824 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.824 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.824 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.824 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.824 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.824 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.824 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:27.824 algo-2:68 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.824 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.824 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.824 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.824 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.824 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.824 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.824 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.824 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.824 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.824 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.824 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.824 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.825 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.825 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.825 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.825 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.825 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.825 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.825 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.825 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.825 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.825 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.825 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.825 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.825 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.825 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.825 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.825 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.825 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.825 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.825 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.825 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.825 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.825 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.825 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.825 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.825 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.825 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.825 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.825 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.825 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.825 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.825 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.825 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.825 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.825 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.825 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.825 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.825 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.825 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.825 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.825 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.825 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.825 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.825 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.826 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.826 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.826 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.826 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.826 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.826 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.826 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.826 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.826 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.826 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.826 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.826 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.826 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.826 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.826 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.826 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.826 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.826 algo-2:56 INFO hook.py:550] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.826 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.826 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.826 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.826 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.826 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.826 algo-2:56 INFO hook.py:550] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.826 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.826 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.826 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.826 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.826 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.826 algo-2:56 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.826 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.826 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.826 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.826 algo-2:56 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.826 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.826 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.826 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.826 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.826 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.826 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.826 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.826 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.826 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.826 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.826 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.826 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.826 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.827 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.827 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.827 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.827 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.827 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.827 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.827 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.827 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.827 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.827 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.827 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.827 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.827 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.827 algo-2:66 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.827 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.827 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.827 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.827 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.827 algo-2:66 INFO hook.py:550] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.827 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.827 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.827 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.827 algo-2:66 INFO hook.py:550] name:module.pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.827 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.827 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.827 algo-2:66 INFO hook.py:550] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.827 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.827 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.827 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.827 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.827 algo-2:66 INFO hook.py:550] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.827 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.827 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.827 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.827 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.827 algo-2:66 INFO hook.py:552] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.827 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.827 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.827 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.827 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.827 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.827 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:27.827 algo-2:66 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.827 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.827 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.827 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.827 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.827 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.827 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.827 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.827 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.827 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.828 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.828 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.828 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.828 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.828 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.828 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.828 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.828 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.828 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.828 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.828 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.828 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.828 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.828 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.828 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.828 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.828 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.828 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.828 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.828 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.828 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.828 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.828 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.828 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.828 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.828 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.828 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.828 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.828 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.828 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.828 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.828 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.828 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.828 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.828 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.828 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.828 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.828 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.828 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.828 algo-2:60 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.828 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.829 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.829 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.829 algo-2:60 INFO hook.py:550] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.829 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.829 algo-2:60 INFO hook.py:550] name:module.pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.829 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.829 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.829 algo-2:60 INFO hook.py:550] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.829 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.829 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.829 algo-2:60 INFO hook.py:550] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.829 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.829 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.829 algo-2:60 INFO hook.py:552] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.829 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.829 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.829 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:27.829 algo-2:60 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.829 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.829 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.829 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.829 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.829 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.829 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.829 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.829 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.829 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.829 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.829 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.829 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.829 algo-2:64 INFO hook.py:550] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.829 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.829 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.830 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.830 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.830 algo-2:64 INFO hook.py:550] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.830 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.830 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.830 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.830 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.830 algo-2:64 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.830 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.830 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.830 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.830 algo-2:64 INFO hook.py:550] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.830 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.830 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.830 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.830 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.830 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.830 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.830 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.830 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.830 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.830 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.830 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.830 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.830 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.830 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.830 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.830 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.830 algo-2:58 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.830 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768[1,8]<stdout>:[2021-04-28 21:23:27.830 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.830 algo-2:58 INFO hook.py:550] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.830 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.830 algo-2:58 INFO hook.py:550] name:module.pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.831 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.831 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.831 algo-2:58 INFO hook.py:550] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.831 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.831 algo-2:58 INFO hook.py:550] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.831 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.831 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.831 algo-2:58 INFO hook.py:552] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.831 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.831 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:27.831 algo-2:58 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.831 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.831 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.831 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.831 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.831 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.831 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.831 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.831 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.831 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.831 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296[1,8]<stdout>:[2021-04-28 21:23:27.831 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.831 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.831 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.831 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.832 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296[1,8]<stdout>:[2021-04-28 21:23:27.832 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.832 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.832 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.832 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.832 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.832 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.832 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.832 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.832 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.832 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.832 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.832 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.832 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.832 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.832 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.832 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.832 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.832 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.832 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.832 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.833 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.833 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824[1,8]<stdout>:[2021-04-28 21:23:27.833 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.833 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.833 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.833 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.833 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.833 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.833 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.833 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.833 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.833 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.833 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.833 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.833 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.833 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.833 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.833 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.833 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.833 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.833 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.833 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.834 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.834 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.834 algo-2:56 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.834 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768[1,12]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.834 algo-2:56 INFO hook.py:550] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.834 algo-2:56 INFO hook.py:550] name:module.pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.834 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.834 algo-2:56 INFO hook.py:550] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.834 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.834 algo-2:56 INFO hook.py:550] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.834 algo-2:56 INFO hook.py:552] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.834 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:27.834 algo-2:56 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.834 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.834 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.834 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.835 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.835 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.835 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.835 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.835 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.835 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.835 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.835 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.836 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.836 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.836 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.836 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.836 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.836 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.836 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.836 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.837 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.837 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.837 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.837 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.837 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.837 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.837 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.838 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.838 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296[1,12]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.838 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.838 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.838 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.838 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.838 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.838 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.839 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.839 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.839 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.839 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.839 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.839 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.839 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.839 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.840 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.840 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.840 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.840 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.840 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.840 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.840 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.840 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.841 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.841 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.841 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.841 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.841 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.841 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.841 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.841 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.842 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.842 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.842 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.842 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.842 algo-2:64 INFO hook.py:550] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.842 algo-2:64 INFO hook.py:550] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.842 algo-2:64 INFO hook.py:550] name:module.pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.842 algo-2:64 INFO hook.py:550] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.843 algo-2:64 INFO hook.py:550] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.843 algo-2:64 INFO hook.py:552] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:27.843 algo-2:64 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:29.089 algo-1:49 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:29.089 algo-1:49 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:29.089 algo-1:60 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:29.089 algo-1:61 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:29.089 algo-1:60 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:29.090 algo-1:61 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:29.090 algo-1:51 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:29.090 algo-1:55 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:29.090 algo-1:53 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:29.090 algo-1:51 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:29.090 algo-1:55 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:29.090 algo-1:59 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:29.090 algo-1:53 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:29.090 algo-1:59 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:29.090 algo-1:57 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:29.090 algo-1:57 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:29.098 algo-1:51 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:29.098 algo-1:60 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:29.098 algo-1:61 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:29.098 algo-1:59 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:29.098 algo-1:53 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:29.098 algo-1:55 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:29.098 algo-1:57 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-04-28 21:23:29.098 algo-1:55 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:DistributedDataParallel SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-04-28 21:23:29.098 algo-1:51 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:DistributedDataParallel SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-04-28 21:23:29.098 algo-1:59 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:DistributedDataParallel SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-04-28 21:23:29.099 algo-1:53 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:DistributedDataParallel SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-04-28 21:23:29.099 algo-1:60 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:DistributedDataParallel SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-04-28 21:23:29.099 algo-1:61 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:DistributedDataParallel SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-04-28 21:23:29.099 algo-1:57 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:DistributedDataParallel SequenceClassifierOutput[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:29.100 algo-1:49 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-04-28 21:23:29.100 algo-1:49 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:DistributedDataParallel SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:29.142 algo-2:68 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:29.142 algo-2:68 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:29.145 algo-2:64 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:29.145 algo-2:64 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:29.145 algo-2:67 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput[1,14]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:29.145 algo-2:56 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:29.145 algo-2:67 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:29.145 algo-2:56 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:29.145 algo-2:66 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:29.145 algo-2:58 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput[1,9]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:29.145 algo-2:66 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:29.145 algo-2:60 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:29.145 algo-2:62 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:29.146 algo-2:58 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:29.146 algo-2:60 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:29.146 algo-2:62 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module.distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:29.153 algo-2:56 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:29.153 algo-2:58 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:29.153 algo-2:64 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:29.153 algo-2:60 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:29.153 algo-2:66 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:29.153 algo-2:67 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:29.153 algo-2:68 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:29.153 algo-2:62 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-04-28 21:23:29.153 algo-2:56 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:DistributedDataParallel SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-04-28 21:23:29.153 algo-2:58 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:DistributedDataParallel SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-04-28 21:23:29.153 algo-2:64 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:DistributedDataParallel SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-04-28 21:23:29.153 algo-2:66 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:DistributedDataParallel SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-04-28 21:23:29.153 algo-2:67 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:DistributedDataParallel SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-04-28 21:23:29.153 algo-2:62 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:DistributedDataParallel SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-04-28 21:23:29.153 algo-2:68 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:DistributedDataParallel SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-04-28 21:23:29.153 algo-2:60 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:DistributedDataParallel SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:{'eval_loss': 0.29170313477516174, 'eval_accuracy': 0.8979, 'eval_f1': 0.8994583948793697, 'eval_precision': 0.8878304821150855, 'eval_recall': 0.9113949311514667, 'eval_runtime': 3.487, 'eval_samples_per_second': 2867.779, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:{'eval_loss': 0.29170313477516174, 'eval_accuracy': 0.8979, 'eval_f1': 0.8994583948793697, 'eval_precision': 0.8878304821150855, 'eval_recall': 0.9113949311514667, 'eval_runtime': 3.4854, 'eval_samples_per_second': 2869.141, 'epoch': 1.0}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "estimator.fit({\n",
    "    \"train\": training_input_path, \n",
    "    \"test\": test_input_path\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "from sagemaker.s3 import S3Downloader\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "_, model_key = sagemaker.s3.parse_s3_url(estimator.model_data)\n",
    "job_name, *rest = model_key.split(\"/\")\n",
    "local_path = f\"./models/{job_name}\"\n",
    "\n",
    "S3Downloader.download(\n",
    "    s3_uri = estimator.model_data,\n",
    "    local_path = local_path,\n",
    "    sagemaker_session = sess\n",
    ")\n",
    "\n",
    "tarfile.open(f'{local_path}/model.tar.gz', mode = 'r:gz').extractall(path = local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from pprint import pprint\n",
    "\n",
    "models = [\n",
    "    AutoModelForSequenceClassification.from_pretrained(model_name),\n",
    "    AutoModelForSequenceClassification.from_pretrained(local_path)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'Negative',\n",
      "  'score': 0.9998512268066406,\n",
      "  'sequence': \"Good. The best movie I've ever seen in my life.\"},\n",
      " {'label': 'Negative',\n",
      "  'score': 0.99973464012146,\n",
      "  'sequence': 'Bad. The worst thing ever.'}]\n",
      "\n",
      "[{'label': 'Negative',\n",
      "  'score': 0.9996551275253296,\n",
      "  'sequence': \"Good. The best movie I've ever seen in my life.\"},\n",
      " {'label': 'Negative',\n",
      "  'score': 0.9996955394744873,\n",
      "  'sequence': 'Bad. The worst thing ever.'}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seqs = [\n",
    "    \"Good. The best movie I've ever seen in my life.\",\n",
    "    \"Bad. The worst thing ever.\"\n",
    "]\n",
    "\n",
    "for it in models:\n",
    "    classifier = pipeline(\n",
    "        \"sentiment-analysis\", \n",
    "        model = it, \n",
    "        tokenizer = tokenizer\n",
    "    )\n",
    "    result = classifier(seqs)\n",
    "    \n",
    "    for i in range(len(result)):\n",
    "        res = result[i]\n",
    "        res[\"label\"] = \"Positive\" if res[\"label\"] == \"LABEL_1\" else \"Negative\"\n",
    "        res[\"sequence\"] = seqs[i]\n",
    "        \n",
    "    pprint(result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.6-gpu-py36-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
